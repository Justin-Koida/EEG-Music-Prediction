{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "60c83d17",
   "metadata": {},
   "source": [
    "The best scalogram model is in a file caled scalogram_based_best.\n",
    "\n",
    "Important to note that I tweaked a lot of parameters. Things that I tweaked are:\n",
    "\n",
    "max_seconds: how many seconds of the song are used in creating the scalogram\n",
    "\n",
    "k: top k most impactful channels in the powerbase model that are then used to create the scalograms (mostly just use k = 8)\n",
    "\n",
    "target_fs: messes with the downsampling. Initially started low so it was faster. I tested increasing it and it does improve model performance sometimes. Curious to see what happens if no downsampling occurs (tried this waited for hours but run took to long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "7653fcc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import scipy.io\n",
    "import h5py\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.io import loadmat\n",
    "from scipy.signal import welch\n",
    "import seaborn as sns\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import LeaveOneGroupOut\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, precision_score, recall_score, precision_recall_fscore_support, confusion_matrix, ConfusionMatrixDisplay\n",
    "from sklearn.svm import SVC\n",
    "from scipy.signal import decimate\n",
    "from skimage.transform import resize\n",
    "import pywt\n",
    "from scipy.io import loadmat\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f2d87036",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HDF5 keys: ['behavioralRatings']\n"
     ]
    }
   ],
   "source": [
    "path = \"../data/behavioralRatings.mat\"\n",
    "with h5py.File(path, \"r\") as f:\n",
    "    print(\"HDF5 keys:\", list(f.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "18f73c28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type: <class 'h5py._hl.dataset.Dataset'>\n",
      "Shape: (2, 10, 20)\n",
      "Dtype: float64\n"
     ]
    }
   ],
   "source": [
    "with h5py.File(path, \"r\") as f:\n",
    "    br = f[\"behavioralRatings\"]\n",
    "    print(\"Type:\", type(br))\n",
    "    print(\"Shape:\", br.shape)\n",
    "    print(\"Dtype:\", br.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "e001791f",
   "metadata": {},
   "outputs": [],
   "source": [
    "with h5py.File(path, \"r\") as f:\n",
    "    br = f[\"behavioralRatings\"][:]   # turn into NumPy array, shape (2, 10, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "0b6c7619",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20, 10, 2)\n"
     ]
    }
   ],
   "source": [
    "# (2, 10, 20) to (20, 10, 2) in order to turn into same structure as matlab \n",
    "ratings = np.transpose(br, (2, 1, 0))\n",
    "\n",
    "print(ratings.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "7e1e78bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "enjoyment = ratings[:, :, 1] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "ec31390f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subject</th>\n",
       "      <th>song_1</th>\n",
       "      <th>song_2</th>\n",
       "      <th>song_3</th>\n",
       "      <th>song_4</th>\n",
       "      <th>song_5</th>\n",
       "      <th>song_6</th>\n",
       "      <th>song_7</th>\n",
       "      <th>song_8</th>\n",
       "      <th>song_9</th>\n",
       "      <th>song_10</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>8.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>8.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>8.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>8.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>7.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>6.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>6.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>5.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>7.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>11</td>\n",
       "      <td>7.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>12</td>\n",
       "      <td>9.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>13</td>\n",
       "      <td>6.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>14</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>15</td>\n",
       "      <td>7.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>16</td>\n",
       "      <td>6.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>17</td>\n",
       "      <td>5.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>18</td>\n",
       "      <td>9.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>19</td>\n",
       "      <td>7.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>20</td>\n",
       "      <td>8.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    subject  song_1  song_2  song_3  song_4  song_5  song_6  song_7  song_8  \\\n",
       "0         1     8.0     8.0     5.0     5.0     9.0     7.0     6.0     7.0   \n",
       "1         2     8.0     8.0     7.0     5.0     3.0     7.0     5.0     5.0   \n",
       "2         3     8.0     7.0     8.0     6.0     8.0     7.0     7.0     8.0   \n",
       "3         4     8.0     7.0     2.0     7.0     9.0     6.0     7.0     7.0   \n",
       "4         5     6.0     8.0     6.0     5.0     7.0     7.0     8.0     7.0   \n",
       "5         6     4.0     5.0     4.0     5.0     5.0     5.0     4.0     4.0   \n",
       "6         7     6.0     2.0     2.0     5.0     3.0     7.0     3.0     5.0   \n",
       "7         8     5.0     8.0     3.0     7.0     5.0     6.0     6.0     6.0   \n",
       "8         9     7.0     5.0     6.0     5.0     6.0     5.0     4.0     6.0   \n",
       "9        10     5.0     4.0     2.0     7.0     5.0     6.0     4.0     7.0   \n",
       "10       11     7.0     5.0     2.0     4.0     7.0     9.0     7.0     8.0   \n",
       "11       12     9.0     6.0     6.0     8.0     6.0     7.0     6.0     8.0   \n",
       "12       13     6.0     7.0     5.0     9.0     7.0     7.0     6.0     8.0   \n",
       "13       14     3.0     5.0     5.0     6.0     5.0     6.0     4.0     6.0   \n",
       "14       15     7.0     4.0     4.0     3.0     5.0     5.0     3.0     6.0   \n",
       "15       16     6.0     5.0     4.0     6.0     5.0     5.0     7.0     6.0   \n",
       "16       17     5.0     6.0     4.0     5.0     7.0     7.0     4.0     6.0   \n",
       "17       18     9.0     7.0     8.0     9.0     9.0     9.0     9.0     9.0   \n",
       "18       19     7.0     6.0     6.0     8.0     5.0     8.0     5.0     7.0   \n",
       "19       20     8.0     8.0     2.0     6.0     8.0     9.0     4.0     6.0   \n",
       "\n",
       "    song_9  song_10  \n",
       "0      5.0      5.0  \n",
       "1      4.0      4.0  \n",
       "2      7.0      6.0  \n",
       "3      5.0      7.0  \n",
       "4      7.0      3.0  \n",
       "5      5.0      1.0  \n",
       "6      3.0      2.0  \n",
       "7      7.0      4.0  \n",
       "8      3.0      2.0  \n",
       "9      3.0      1.0  \n",
       "10     6.0      1.0  \n",
       "11     5.0      6.0  \n",
       "12     3.0      4.0  \n",
       "13     3.0      2.0  \n",
       "14     3.0      2.0  \n",
       "15     5.0      2.0  \n",
       "16     5.0      1.0  \n",
       "17     9.0      3.0  \n",
       "18     8.0      4.0  \n",
       "19     8.0      1.0  "
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "participants = np.arange(1, 21)\n",
    "songs = np.arange(1, 11)\n",
    "\n",
    "like_df = pd.DataFrame(enjoyment,\n",
    "                      index=participants,\n",
    "                      columns=[f\"song_{s}\" for s in songs]).rename_axis(\"subject\").reset_index()\n",
    "\n",
    "like_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "02b3fed0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basically mapping freq to names/categories\n",
    "BANDS = {\n",
    "    \"delta\": (1, 4),\n",
    "    \"theta\": (4, 8),\n",
    "    \"alpha\": (8, 12),\n",
    "    \"beta\": (12, 30),\n",
    "    \"gamma_low\": (30, 45),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "7b7c62ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_bandpower(f, psd, fmin, fmax):\n",
    "    idx = (f >= fmin) & (f <= fmax)\n",
    "    return np.trapezoid(psd[idx], f[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "49f5c7db",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "88139fdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_X = []\n",
    "all_y = []\n",
    "participant_ids = []\n",
    "song_ids = []\n",
    "\n",
    "# they had weird mappings in the study, these numbers correspond to file names\n",
    "# looping through each song\n",
    "for song in range(21, 31):\n",
    "    # loads in the file\n",
    "    load_mat_file = loadmat(f\"../data/cleaned_eeg/song{song}_Imputed.mat\")\n",
    "    # gets the current song\n",
    "    data = load_mat_file[f\"data{song}\"]\n",
    "\n",
    "    fs = float(load_mat_file[\"fs\"].squeeze())\n",
    "\n",
    "    channels, time, participants = data.shape\n",
    "\n",
    "    # converting werid mappings to start at 1\n",
    "    song_idx = song - 20\n",
    "    song_label = like_df[f\"song_{song_idx}\"].to_numpy()\n",
    "\n",
    "    # loop through each participant\n",
    "    for participant in range(participants):\n",
    "        # get eeg data for given participant for the given song\n",
    "        eeg_participant = data[:, :, participant]\n",
    "\n",
    "        # this chunk just gets the band power for each participant for given song\n",
    "        feature_list = []\n",
    "        for channel in range(channels):\n",
    "            f, psd = welch(eeg_participant[channel, :], fs = fs, nperseg = 2 * int(fs))\n",
    "            for fmin, fmax in BANDS.values():\n",
    "                band_power = calculate_bandpower(f, psd, fmin, fmax)\n",
    "                feature_list.append(band_power)\n",
    "\n",
    "        feature_vec = np.log10(np.array(feature_list) + .000000000000000000001)\n",
    "        all_X.append(feature_vec)\n",
    "\n",
    "        # basically liked if score above 6, no like below 6\n",
    "        all_y.append(1 if song_label[participant] >= 6 else 0)\n",
    "        participant_ids.append(participant)\n",
    "        song_ids.append(song_idx)\n",
    "\n",
    "# Convert to usable format\n",
    "X_all = np.vstack(all_X)\n",
    "y_all = np.array(all_y)\n",
    "\n",
    "participant_ids = np.array(participant_ids)\n",
    "song_ids = np.array(song_ids)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "0c0292ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>615</th>\n",
       "      <th>616</th>\n",
       "      <th>617</th>\n",
       "      <th>618</th>\n",
       "      <th>619</th>\n",
       "      <th>620</th>\n",
       "      <th>621</th>\n",
       "      <th>622</th>\n",
       "      <th>623</th>\n",
       "      <th>624</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.735372</td>\n",
       "      <td>0.517957</td>\n",
       "      <td>0.360125</td>\n",
       "      <td>0.591212</td>\n",
       "      <td>0.054921</td>\n",
       "      <td>0.675716</td>\n",
       "      <td>0.469317</td>\n",
       "      <td>0.341129</td>\n",
       "      <td>0.873114</td>\n",
       "      <td>0.770013</td>\n",
       "      <td>...</td>\n",
       "      <td>0.533776</td>\n",
       "      <td>0.590708</td>\n",
       "      <td>0.351696</td>\n",
       "      <td>0.670366</td>\n",
       "      <td>-0.092422</td>\n",
       "      <td>0.621949</td>\n",
       "      <td>0.751444</td>\n",
       "      <td>0.606980</td>\n",
       "      <td>0.855188</td>\n",
       "      <td>-0.121695</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.832643</td>\n",
       "      <td>0.344199</td>\n",
       "      <td>0.790840</td>\n",
       "      <td>0.473331</td>\n",
       "      <td>-0.143020</td>\n",
       "      <td>0.902948</td>\n",
       "      <td>0.359715</td>\n",
       "      <td>0.869040</td>\n",
       "      <td>0.561634</td>\n",
       "      <td>-0.022386</td>\n",
       "      <td>...</td>\n",
       "      <td>0.833698</td>\n",
       "      <td>0.342209</td>\n",
       "      <td>0.798735</td>\n",
       "      <td>0.474534</td>\n",
       "      <td>-0.358329</td>\n",
       "      <td>0.764603</td>\n",
       "      <td>0.306948</td>\n",
       "      <td>0.732067</td>\n",
       "      <td>0.456729</td>\n",
       "      <td>-0.532869</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.892297</td>\n",
       "      <td>0.557208</td>\n",
       "      <td>0.713414</td>\n",
       "      <td>0.376822</td>\n",
       "      <td>-0.115197</td>\n",
       "      <td>1.509475</td>\n",
       "      <td>0.808840</td>\n",
       "      <td>0.828644</td>\n",
       "      <td>0.805651</td>\n",
       "      <td>0.519312</td>\n",
       "      <td>...</td>\n",
       "      <td>0.756495</td>\n",
       "      <td>0.444941</td>\n",
       "      <td>0.606594</td>\n",
       "      <td>0.282850</td>\n",
       "      <td>-0.382810</td>\n",
       "      <td>0.581887</td>\n",
       "      <td>0.701730</td>\n",
       "      <td>0.814113</td>\n",
       "      <td>0.309131</td>\n",
       "      <td>-0.595847</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.111488</td>\n",
       "      <td>0.715326</td>\n",
       "      <td>0.444180</td>\n",
       "      <td>0.517769</td>\n",
       "      <td>0.115481</td>\n",
       "      <td>1.115695</td>\n",
       "      <td>0.938606</td>\n",
       "      <td>0.541302</td>\n",
       "      <td>0.665672</td>\n",
       "      <td>0.223781</td>\n",
       "      <td>...</td>\n",
       "      <td>0.917731</td>\n",
       "      <td>0.762848</td>\n",
       "      <td>0.297982</td>\n",
       "      <td>0.375505</td>\n",
       "      <td>-0.152520</td>\n",
       "      <td>0.981974</td>\n",
       "      <td>0.834027</td>\n",
       "      <td>0.653438</td>\n",
       "      <td>0.462594</td>\n",
       "      <td>-0.248389</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.067522</td>\n",
       "      <td>0.624089</td>\n",
       "      <td>0.637543</td>\n",
       "      <td>0.703967</td>\n",
       "      <td>0.176480</td>\n",
       "      <td>1.415276</td>\n",
       "      <td>0.881465</td>\n",
       "      <td>0.884471</td>\n",
       "      <td>0.988769</td>\n",
       "      <td>0.549320</td>\n",
       "      <td>...</td>\n",
       "      <td>1.078836</td>\n",
       "      <td>0.685927</td>\n",
       "      <td>0.544265</td>\n",
       "      <td>0.818167</td>\n",
       "      <td>0.302669</td>\n",
       "      <td>0.959196</td>\n",
       "      <td>0.666225</td>\n",
       "      <td>0.569900</td>\n",
       "      <td>0.610659</td>\n",
       "      <td>-0.207420</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>195</th>\n",
       "      <td>0.961990</td>\n",
       "      <td>0.511375</td>\n",
       "      <td>0.068491</td>\n",
       "      <td>0.297462</td>\n",
       "      <td>-0.104591</td>\n",
       "      <td>0.810637</td>\n",
       "      <td>0.372254</td>\n",
       "      <td>-0.041543</td>\n",
       "      <td>0.127531</td>\n",
       "      <td>-0.254268</td>\n",
       "      <td>...</td>\n",
       "      <td>0.722118</td>\n",
       "      <td>0.373444</td>\n",
       "      <td>-0.052891</td>\n",
       "      <td>0.131516</td>\n",
       "      <td>-0.337811</td>\n",
       "      <td>0.684117</td>\n",
       "      <td>0.551660</td>\n",
       "      <td>0.033654</td>\n",
       "      <td>0.125793</td>\n",
       "      <td>-0.488601</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196</th>\n",
       "      <td>1.055436</td>\n",
       "      <td>0.749483</td>\n",
       "      <td>0.975975</td>\n",
       "      <td>0.437121</td>\n",
       "      <td>-0.041992</td>\n",
       "      <td>0.936351</td>\n",
       "      <td>0.777895</td>\n",
       "      <td>1.091959</td>\n",
       "      <td>0.559783</td>\n",
       "      <td>0.052457</td>\n",
       "      <td>...</td>\n",
       "      <td>0.798728</td>\n",
       "      <td>0.835067</td>\n",
       "      <td>1.071288</td>\n",
       "      <td>0.579723</td>\n",
       "      <td>-0.062425</td>\n",
       "      <td>0.890615</td>\n",
       "      <td>1.048405</td>\n",
       "      <td>1.262889</td>\n",
       "      <td>0.672654</td>\n",
       "      <td>-0.202523</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>197</th>\n",
       "      <td>1.110303</td>\n",
       "      <td>0.601051</td>\n",
       "      <td>0.342901</td>\n",
       "      <td>0.500978</td>\n",
       "      <td>0.296940</td>\n",
       "      <td>0.942765</td>\n",
       "      <td>0.436964</td>\n",
       "      <td>0.244673</td>\n",
       "      <td>0.824408</td>\n",
       "      <td>0.611259</td>\n",
       "      <td>...</td>\n",
       "      <td>0.642207</td>\n",
       "      <td>0.274625</td>\n",
       "      <td>0.051561</td>\n",
       "      <td>0.318407</td>\n",
       "      <td>0.073765</td>\n",
       "      <td>0.691856</td>\n",
       "      <td>0.321364</td>\n",
       "      <td>0.111347</td>\n",
       "      <td>0.213866</td>\n",
       "      <td>-0.034468</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>198</th>\n",
       "      <td>1.091442</td>\n",
       "      <td>0.828457</td>\n",
       "      <td>0.891922</td>\n",
       "      <td>0.848703</td>\n",
       "      <td>0.423952</td>\n",
       "      <td>1.111520</td>\n",
       "      <td>0.771587</td>\n",
       "      <td>0.886824</td>\n",
       "      <td>0.882306</td>\n",
       "      <td>0.221922</td>\n",
       "      <td>...</td>\n",
       "      <td>0.641523</td>\n",
       "      <td>0.536660</td>\n",
       "      <td>0.789501</td>\n",
       "      <td>0.678763</td>\n",
       "      <td>-0.214273</td>\n",
       "      <td>0.651204</td>\n",
       "      <td>0.613554</td>\n",
       "      <td>0.759610</td>\n",
       "      <td>0.493334</td>\n",
       "      <td>-0.325397</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199</th>\n",
       "      <td>0.825969</td>\n",
       "      <td>0.299535</td>\n",
       "      <td>0.037611</td>\n",
       "      <td>0.547068</td>\n",
       "      <td>0.294606</td>\n",
       "      <td>0.739853</td>\n",
       "      <td>0.210879</td>\n",
       "      <td>-0.039012</td>\n",
       "      <td>0.473949</td>\n",
       "      <td>0.218795</td>\n",
       "      <td>...</td>\n",
       "      <td>0.625190</td>\n",
       "      <td>0.034581</td>\n",
       "      <td>-0.185647</td>\n",
       "      <td>0.266156</td>\n",
       "      <td>0.016930</td>\n",
       "      <td>0.660981</td>\n",
       "      <td>0.365677</td>\n",
       "      <td>-0.040350</td>\n",
       "      <td>0.229787</td>\n",
       "      <td>-0.271086</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>200 rows Ã— 625 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          0         1         2         3         4         5         6    \\\n",
       "0    0.735372  0.517957  0.360125  0.591212  0.054921  0.675716  0.469317   \n",
       "1    0.832643  0.344199  0.790840  0.473331 -0.143020  0.902948  0.359715   \n",
       "2    0.892297  0.557208  0.713414  0.376822 -0.115197  1.509475  0.808840   \n",
       "3    1.111488  0.715326  0.444180  0.517769  0.115481  1.115695  0.938606   \n",
       "4    1.067522  0.624089  0.637543  0.703967  0.176480  1.415276  0.881465   \n",
       "..        ...       ...       ...       ...       ...       ...       ...   \n",
       "195  0.961990  0.511375  0.068491  0.297462 -0.104591  0.810637  0.372254   \n",
       "196  1.055436  0.749483  0.975975  0.437121 -0.041992  0.936351  0.777895   \n",
       "197  1.110303  0.601051  0.342901  0.500978  0.296940  0.942765  0.436964   \n",
       "198  1.091442  0.828457  0.891922  0.848703  0.423952  1.111520  0.771587   \n",
       "199  0.825969  0.299535  0.037611  0.547068  0.294606  0.739853  0.210879   \n",
       "\n",
       "          7         8         9    ...       615       616       617  \\\n",
       "0    0.341129  0.873114  0.770013  ...  0.533776  0.590708  0.351696   \n",
       "1    0.869040  0.561634 -0.022386  ...  0.833698  0.342209  0.798735   \n",
       "2    0.828644  0.805651  0.519312  ...  0.756495  0.444941  0.606594   \n",
       "3    0.541302  0.665672  0.223781  ...  0.917731  0.762848  0.297982   \n",
       "4    0.884471  0.988769  0.549320  ...  1.078836  0.685927  0.544265   \n",
       "..        ...       ...       ...  ...       ...       ...       ...   \n",
       "195 -0.041543  0.127531 -0.254268  ...  0.722118  0.373444 -0.052891   \n",
       "196  1.091959  0.559783  0.052457  ...  0.798728  0.835067  1.071288   \n",
       "197  0.244673  0.824408  0.611259  ...  0.642207  0.274625  0.051561   \n",
       "198  0.886824  0.882306  0.221922  ...  0.641523  0.536660  0.789501   \n",
       "199 -0.039012  0.473949  0.218795  ...  0.625190  0.034581 -0.185647   \n",
       "\n",
       "          618       619       620       621       622       623       624  \n",
       "0    0.670366 -0.092422  0.621949  0.751444  0.606980  0.855188 -0.121695  \n",
       "1    0.474534 -0.358329  0.764603  0.306948  0.732067  0.456729 -0.532869  \n",
       "2    0.282850 -0.382810  0.581887  0.701730  0.814113  0.309131 -0.595847  \n",
       "3    0.375505 -0.152520  0.981974  0.834027  0.653438  0.462594 -0.248389  \n",
       "4    0.818167  0.302669  0.959196  0.666225  0.569900  0.610659 -0.207420  \n",
       "..        ...       ...       ...       ...       ...       ...       ...  \n",
       "195  0.131516 -0.337811  0.684117  0.551660  0.033654  0.125793 -0.488601  \n",
       "196  0.579723 -0.062425  0.890615  1.048405  1.262889  0.672654 -0.202523  \n",
       "197  0.318407  0.073765  0.691856  0.321364  0.111347  0.213866 -0.034468  \n",
       "198  0.678763 -0.214273  0.651204  0.613554  0.759610  0.493334 -0.325397  \n",
       "199  0.266156  0.016930  0.660981  0.365677 -0.040350  0.229787 -0.271086  \n",
       "\n",
       "[200 rows x 625 columns]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(X_all)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5755eac",
   "metadata": {},
   "source": [
    "# Best Power-Based"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12794510",
   "metadata": {},
   "source": [
    "The best power-based model is created below from previous model creation. I extract feature importance in order to find which channels are the most important. I take the top 8 and use these channels to create scalograms. 125 channels is way to many channels for scalograms that already take a while to make"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "6a3a4887",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LeaveOneGroupOut()"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaler = StandardScaler()\n",
    "logo = LeaveOneGroupOut()\n",
    "X_scaled = scaler.fit_transform(X_all)\n",
    "logo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "2b391cfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "logistic_reg_initial = LogisticRegression(max_iter = 2000, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "504f04f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metrics from aggregation across folds\n",
      "Acurracy across all folds:  0.625\n",
      "Precision for Class 0:  0.4594841269841269\n",
      "Precision for Class 1:  0.5030158730158729\n",
      "Recall for Class 0:  0.545654761904762\n",
      "Recall for Class 1:  0.5473214285714285\n",
      "------------------------------------\n",
      "Computing Precision Recall whole data set (not per fold)\n",
      "Precision for class 0:  0.5816326530612245\n",
      "Precision for class 1:  0.6666666666666666\n",
      "Recall for class 0:  0.6263736263736264\n",
      "Recall for class 1:  0.6238532110091743\n"
     ]
    }
   ],
   "source": [
    "accuracies = []\n",
    "precisions = []\n",
    "recalls = []\n",
    "# conf_matrix_folds = []\n",
    "\n",
    "# collecting across all folds bc my prec recall kinda weird with some classes having none per fold\n",
    "all_folds_y_true = []\n",
    "all_folds_y_pred = []\n",
    "\n",
    "for train_idx, test_idx in logo.split(X_scaled, y_all, participant_ids):\n",
    "    X_train, X_test = X_scaled[train_idx], X_scaled[test_idx]\n",
    "    y_train, y_test = y_all[train_idx], y_all[test_idx]\n",
    "\n",
    "    logistic_reg_initial.fit(X_train, y_train)\n",
    "    y_pred = logistic_reg_initial.predict(X_test)\n",
    "    y_prob = logistic_reg_initial.predict_proba(X_test)[:, 1]\n",
    "\n",
    "    accuracies.append(accuracy_score(y_test, y_pred))\n",
    "\n",
    "    all_folds_y_true.extend(y_test)\n",
    "    all_folds_y_pred.extend(y_pred)\n",
    "\n",
    "    # cm = confusion_matrix(y_test, y_pred, labels = [0,1])\n",
    "    # conf_matrix_folds.append(cm)\n",
    "    \n",
    "    # precisions.append(precision_score(y_test, y_pred, pos_label=1, zero_division = 0))\n",
    "    # recalls.append(recall_score(y_test, y_pred, pos_label = 1, zero_division=0))\n",
    "\n",
    "    prec, rec, f1, support = precision_recall_fscore_support(\n",
    "        y_test,\n",
    "        y_pred,\n",
    "        labels = [0,1],\n",
    "        average = None,\n",
    "        zero_division = 0\n",
    "    )\n",
    "\n",
    "    precisions.append(prec)\n",
    "    recalls.append(rec)\n",
    "\n",
    "all_folds_y_true = np.array(all_folds_y_true)\n",
    "all_folds_y_pred = np.array(all_folds_y_pred)\n",
    "\n",
    "prec, rec, f1, support = precision_recall_fscore_support(\n",
    "    all_folds_y_true,\n",
    "    all_folds_y_pred,\n",
    "    labels = [0,1],\n",
    "    average = None,\n",
    "    zero_division = 0\n",
    ")\n",
    "\n",
    "# print(\"Precision across all folds: \", np.mean(precisions))\n",
    "# print(\"Recall across all folds: \", np.mean(recalls))\n",
    "\n",
    "precisions = np.array(precisions)\n",
    "recalls = np.array(recalls)\n",
    "\n",
    "mean_prec_class0, mean_prec_class1 = precisions.mean(axis = 0)\n",
    "mean_recall_class0, mean_recall_class1 = recalls.mean(axis = 0)\n",
    "\n",
    "print(\"Metrics from aggregation across folds\")\n",
    "print(\"Acurracy across all folds: \", np.mean(accuracies))\n",
    "\n",
    "print(\"Precision for Class 0: \", mean_prec_class0)\n",
    "print(\"Precision for Class 1: \", mean_prec_class1)\n",
    "\n",
    "print(\"Recall for Class 0: \", mean_recall_class0)\n",
    "print(\"Recall for Class 1: \", mean_recall_class1)\n",
    "\n",
    "print(\"------------------------------------\")\n",
    "print(\"Computing Precision Recall whole data set (not per fold)\")\n",
    "\n",
    "print(\"Precision for class 0: \", prec[0])\n",
    "print(\"Precision for class 1: \", prec[1])\n",
    "\n",
    "print(\"Recall for class 0: \", rec[0])\n",
    "print(\"Recall for class 1: \", rec[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "c57b58e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([4.34633263e-01, 2.22551533e-01, 2.22805628e-02, 1.35156094e-01,\n",
       "       3.00718109e-01, 5.36088260e-02, 1.73541434e-02, 7.29167800e-02,\n",
       "       3.06152166e-01, 4.15234751e-01, 1.85450063e-01, 1.87011204e-01,\n",
       "       6.25979904e-02, 2.39198690e-01, 4.83568962e-02, 2.78234207e-01,\n",
       "       1.20114324e-01, 7.83794065e-02, 2.08202274e-01, 4.08479323e-03,\n",
       "       1.30846588e-01, 3.75851433e-01, 1.76498510e-01, 8.25339997e-02,\n",
       "       1.65004131e-02, 4.76257757e-02, 1.50921468e-01, 9.17140204e-02,\n",
       "       1.43550429e-01, 1.17179159e-02, 3.77665564e-01, 1.35971500e-01,\n",
       "       2.35240232e-01, 5.58896755e-02, 1.20828953e-01, 3.34144597e-01,\n",
       "       2.89114756e-01, 2.94297367e-01, 2.26119731e-01, 1.50485434e-02,\n",
       "       2.25696326e-01, 1.64935916e-01, 2.12470648e-02, 4.20849793e-02,\n",
       "       1.13333780e-01, 4.17311240e-01, 1.09164607e-01, 6.91407038e-02,\n",
       "       2.72699535e-01, 2.75212255e-01, 7.78335972e-02, 4.62013634e-02,\n",
       "       4.19069552e-02, 2.70163968e-03, 1.65455554e-01, 6.11228640e-02,\n",
       "       2.22898206e-01, 4.07964126e-02, 2.68425617e-02, 1.26604441e-01,\n",
       "       2.07985414e-01, 2.78233040e-01, 1.40415796e-02, 1.17168509e-01,\n",
       "       1.21346269e-01, 1.09655187e-01, 2.20735784e-02, 9.72343127e-02,\n",
       "       5.17657547e-02, 1.38936793e-01, 1.41395431e-01, 1.77168933e-01,\n",
       "       6.13322599e-02, 4.82920263e-01, 4.34528386e-01, 3.49098332e-02,\n",
       "       3.08732023e-02, 1.73913427e-03, 1.58753057e-01, 1.03020769e-01,\n",
       "       4.65113133e-01, 2.51902760e-01, 2.80768064e-01, 3.32549074e-01,\n",
       "       2.67548967e-01, 1.99826841e-01, 1.31535480e-01, 5.81166449e-02,\n",
       "       1.51073831e-02, 7.75536832e-02, 3.99677802e-01, 1.72131745e-01,\n",
       "       3.16927176e-02, 1.13451271e-01, 1.37814136e-01, 1.67349468e-01,\n",
       "       2.91347377e-01, 5.48613555e-02, 9.37706218e-02, 1.51648320e-02,\n",
       "       3.01684295e-01, 1.19734713e-01, 9.12360147e-03, 1.14978881e-01,\n",
       "       1.27066853e-01, 1.46576315e-01, 2.67470355e-01, 1.18078766e-01,\n",
       "       1.98654092e-01, 1.20910508e-01, 2.54779287e-01, 2.60359137e-01,\n",
       "       1.16992150e-01, 2.47938711e-01, 3.47192518e-01, 2.65399010e-01,\n",
       "       6.73520686e-02, 1.63182518e-02, 1.45798271e-01, 1.29336507e-01,\n",
       "       8.39567641e-03, 1.85237674e-02, 9.67742686e-02, 1.55972236e-01,\n",
       "       2.39289848e-01, 1.01525098e+00, 2.54336363e-01, 7.91044492e-02,\n",
       "       2.33073355e-01, 2.18742618e-01, 7.30191723e-02, 1.62455087e-01,\n",
       "       8.74968703e-02, 1.84708152e-01, 1.16500470e-01, 1.27174186e-01,\n",
       "       8.21514255e-02, 7.09325273e-02, 7.79151473e-02, 1.07428646e-01,\n",
       "       3.47742910e-01, 3.71497993e-01, 7.54582374e-02, 1.35922240e-01,\n",
       "       5.03762828e-02, 1.28395990e-01, 1.16113267e-01, 3.62576358e-01,\n",
       "       8.77796517e-02, 2.63586487e-01, 3.44295704e-03, 1.43999451e-01,\n",
       "       3.34812841e-01, 1.10758294e-01, 9.05064117e-02, 4.32133094e-01,\n",
       "       1.69404992e-02, 1.34150735e-01, 1.26069397e-02, 6.91703691e-02,\n",
       "       1.79702225e-01, 3.29640118e-02, 1.53576364e-01, 1.50150021e-01,\n",
       "       4.48826169e-01, 2.92053532e-02, 2.13122690e-01, 4.81640552e-02,\n",
       "       1.99608942e-01, 3.93278044e-01, 2.93132314e-02, 4.35017392e-02,\n",
       "       9.63791251e-02, 1.69514239e-01, 1.52939765e-01, 2.58428683e-01,\n",
       "       3.07152921e-01, 5.95500867e-02, 4.38717187e-01, 6.48404259e-01,\n",
       "       1.24868203e-01, 3.04082111e-02, 1.43821738e-01, 1.62614927e-01,\n",
       "       2.55741570e-01, 3.79951418e-01, 8.85582492e-02, 2.98106862e-01,\n",
       "       1.95886851e-01, 2.45609525e-01, 1.69187031e-01, 1.22186537e-02,\n",
       "       2.05226238e-01, 3.72222929e-02, 1.05251470e-01, 1.04646150e-01,\n",
       "       3.38687437e-01, 5.95784421e-02, 1.43241500e-01, 8.47822624e-02,\n",
       "       1.21119276e-01, 7.54681276e-02, 2.14510852e-01, 5.27562452e-01,\n",
       "       5.30896680e-01, 2.39570182e-01, 9.25865128e-02, 6.38655580e-02,\n",
       "       2.54193166e-02, 2.14413131e-01, 2.55090188e-01, 9.57639124e-02,\n",
       "       8.32749516e-02, 3.18655675e-01, 1.13743139e-01, 3.87488850e-01,\n",
       "       5.44216913e-02, 2.14653349e-01, 4.00109838e-01, 2.06147872e-01,\n",
       "       2.86035510e-01, 1.94237212e-01, 2.08329835e-01, 2.91851739e-01,\n",
       "       4.33732763e-02, 3.57666923e-01, 3.46804504e-01, 1.33025191e-01,\n",
       "       9.36493840e-02, 2.04178910e-01, 1.02356459e-01, 6.74264786e-03,\n",
       "       2.32993711e-01, 1.23670335e-01, 9.04151159e-02, 1.59800738e-03,\n",
       "       6.93094073e-02, 1.57577651e-01, 1.75414816e-01, 3.06133276e-01,\n",
       "       7.04522548e-02, 2.26576030e-01, 1.85666773e-01, 1.67776298e-01,\n",
       "       1.37109066e-02, 3.30843065e-02, 2.71209777e-01, 1.65964722e-01,\n",
       "       4.88979954e-02, 1.14848504e-01, 1.42482721e-02, 3.45006145e-02,\n",
       "       2.10139105e-01, 1.11449435e-01, 5.82154764e-02, 4.81100640e-01,\n",
       "       2.23856214e-02, 1.95287404e-01, 1.06058914e-01, 2.70808070e-01,\n",
       "       4.87295328e-01, 2.54015605e-02, 1.83761783e-01, 7.47999343e-02,\n",
       "       2.75666180e-01, 7.05071861e-02, 1.43795947e-01, 6.11371466e-02,\n",
       "       2.47458015e-01, 1.17258413e-01, 6.59737169e-01, 8.61418691e-02,\n",
       "       4.24858347e-01, 2.99922858e-01, 3.17503662e-02, 3.17072696e-01,\n",
       "       3.30586261e-01, 5.80570583e-02, 9.20564595e-02, 9.82629099e-02,\n",
       "       6.97226321e-02, 1.04140460e-01, 3.54809579e-01, 4.32364598e-01,\n",
       "       3.21504301e-01, 1.53907513e-01, 2.04903655e-02, 1.68478164e-01,\n",
       "       1.98700526e-01, 2.23531983e-01, 2.48629657e-03, 1.68340126e-01,\n",
       "       7.64371526e-03, 1.99708032e-01, 2.28833153e-01, 1.96250399e-01,\n",
       "       1.06904415e-02, 1.57846301e-01, 1.98767551e-01, 1.95419023e-01,\n",
       "       1.25461598e-01, 1.82238163e-01, 8.19363715e-02, 3.34021746e-01,\n",
       "       1.69740836e-02, 1.99359544e-01, 2.44245130e-01, 3.75817081e-02,\n",
       "       3.44122576e-01, 1.34518080e-01, 5.33600433e-01, 2.10593489e-01,\n",
       "       2.73692558e-01, 1.56227662e-01, 1.85998222e-02, 1.30586154e-01,\n",
       "       4.54934332e-02, 8.23241611e-02, 1.23434615e-01, 5.97387740e-01,\n",
       "       2.60865911e-01, 1.92329690e-02, 7.67909733e-02, 1.73965616e-01,\n",
       "       2.76043331e-01, 1.70380062e-01, 6.33914377e-03, 1.10818887e-01,\n",
       "       3.40090454e-02, 3.02390517e-02, 1.34756543e-01, 4.88778769e-02,\n",
       "       8.23351800e-02, 2.84637991e-01, 5.93779336e-02, 1.82338742e-01,\n",
       "       3.60427340e-02, 2.19921439e-01, 8.84152433e-02, 2.33352241e-01,\n",
       "       2.34364111e-01, 4.26522984e-02, 1.49458406e-01, 1.08060886e-01,\n",
       "       3.20999530e-01, 2.75747737e-01, 5.88603598e-02, 1.14822879e-01,\n",
       "       2.43860615e-01, 8.38960270e-02, 2.00528134e-01, 1.04701741e-01,\n",
       "       1.34810187e-01, 1.07600173e-01, 3.52587158e-01, 1.02928236e-03,\n",
       "       1.16309590e-01, 6.73939266e-02, 9.53877586e-02, 3.09535564e-01,\n",
       "       5.07343438e-02, 7.94022525e-03, 2.70124440e-01, 1.81329254e-01,\n",
       "       9.50011508e-02, 2.48186684e-01, 1.70410666e-02, 3.81922923e-02,\n",
       "       6.57778317e-02, 8.62840773e-02, 5.62667914e-02, 1.22051429e-01,\n",
       "       1.89637169e-01, 2.88901585e-01, 2.06890298e-01, 2.89078819e-01,\n",
       "       3.38711934e-01, 2.24941244e-01, 2.86797131e-01, 1.01552418e-01,\n",
       "       1.04216840e-01, 9.45781704e-02, 3.82606892e-02, 8.61681540e-02,\n",
       "       1.97259158e-01, 1.47474838e-01, 1.39249086e-01, 2.26411702e-02,\n",
       "       8.01961616e-02, 1.30114633e-01, 2.13665012e-01, 4.71961932e-03,\n",
       "       7.90907292e-02, 2.65111047e-01, 1.87009109e-01, 5.75902070e-02,\n",
       "       2.87056767e-01, 2.01814095e-01, 1.18224513e-01, 1.72847344e-01,\n",
       "       2.68812866e-01, 3.59526696e-02, 4.33673159e-02, 2.48633375e-01,\n",
       "       4.26839650e-02, 2.67634360e-01, 1.80356790e-02, 2.20103745e-01,\n",
       "       6.19308313e-04, 3.29173467e-02, 9.52974161e-02, 5.32177100e-02,\n",
       "       8.80137706e-02, 2.22486129e-01, 3.37521598e-01, 1.52682777e-01,\n",
       "       1.54581223e-01, 1.88016316e-02, 1.51719636e-01, 1.45217493e-01,\n",
       "       1.24896628e-01, 3.53507524e-02, 4.29181483e-02, 1.19424976e-02,\n",
       "       2.91108504e-01, 5.28128264e-02, 3.99037821e-02, 1.55159614e-02,\n",
       "       1.15548715e-01, 3.06047088e-02, 5.56662905e-02, 2.08719902e-01,\n",
       "       1.05975741e-01, 1.00415987e-01, 2.59113522e-01, 5.29209656e-01,\n",
       "       1.99886079e-01, 1.20832555e-02, 3.27186808e-01, 1.73547756e-01,\n",
       "       3.22295125e-01, 4.21451320e-02, 2.93549208e-01, 7.24251281e-02,\n",
       "       1.91562773e-01, 9.80041685e-02, 1.07988806e-01, 2.11648212e-01,\n",
       "       3.23901788e-02, 4.24062104e-01, 2.90776417e-01, 1.00628098e-01,\n",
       "       1.50140729e-01, 1.64851050e-01, 1.06496302e-01, 4.83171962e-01,\n",
       "       1.82134531e-01, 5.37750137e-02, 2.42623300e-01, 2.90368539e-01,\n",
       "       1.62396186e-01, 2.70366425e-01, 1.67635629e-01, 4.92375003e-01,\n",
       "       2.36465181e-01, 3.78798591e-01, 9.77326720e-02, 2.14457735e-01,\n",
       "       3.72132094e-03, 3.25047522e-01, 2.89067002e-01, 4.56461840e-02,\n",
       "       2.94962985e-01, 1.89882806e-01, 1.55886133e-01, 1.97246835e-01,\n",
       "       1.61572481e-01, 2.17265295e-01, 2.32978942e-02, 5.78132813e-01,\n",
       "       2.82176834e-01, 2.58815066e-01, 1.04248824e-01, 2.98843630e-01,\n",
       "       3.27564594e-01, 2.14657037e-01, 2.35568384e-01, 2.98030274e-01,\n",
       "       3.98890576e-01, 1.22425732e-01, 2.24418322e-01, 8.58048116e-02,\n",
       "       2.10222639e-01, 6.67340760e-02, 4.27406336e-01, 2.80849310e-01,\n",
       "       5.87974298e-02, 1.85542947e-01, 3.40974417e-01, 2.80684831e-01,\n",
       "       1.25985397e-02, 2.74536957e-01, 1.94190309e-01, 1.49595379e-01,\n",
       "       2.72152626e-02, 8.28402523e-02, 6.67315374e-02, 2.27347959e-02,\n",
       "       3.35812914e-03, 2.93288874e-01, 2.93094878e-02, 1.95172483e-02,\n",
       "       2.60559416e-01, 1.21581355e-01, 5.61834830e-01, 1.45938759e-01,\n",
       "       2.21249479e-01, 5.18164725e-02, 1.18067956e-01, 2.33849583e-01,\n",
       "       7.14664298e-02, 6.83750731e-02, 2.59343955e-02, 9.05776730e-02,\n",
       "       1.87510005e-01, 2.42659255e-01, 1.25301736e-03, 1.90769657e-02,\n",
       "       4.53269672e-02, 3.56439384e-01, 4.66454848e-02, 1.66121404e-01,\n",
       "       1.82232598e-01, 2.46158412e-02, 2.48241622e-01, 1.75992951e-01,\n",
       "       2.16249405e-01, 2.81026295e-02, 4.94435338e-02, 3.60183111e-01,\n",
       "       1.73870518e-01, 9.79282370e-03, 8.62288080e-02, 1.24690096e-01,\n",
       "       3.87662119e-01, 2.67729714e-02, 2.86815718e-02, 9.97789434e-02,\n",
       "       2.48280938e-01, 2.64966699e-01, 3.48753702e-01, 9.62612048e-02,\n",
       "       2.51204906e-01, 1.32942650e-02, 1.77937197e-02, 2.40512969e-01,\n",
       "       1.09660695e-01, 1.84995428e-01, 8.00864641e-03, 7.97818936e-03,\n",
       "       2.64309315e-01, 2.00623746e-01, 3.41846968e-01, 1.49010680e-01,\n",
       "       5.04227352e-01, 1.39623587e-01, 6.35275990e-01, 4.31727960e-01,\n",
       "       2.42725183e-02, 2.31178802e-02, 2.79137540e-02, 7.90890821e-02,\n",
       "       4.54053975e-01, 3.19374802e-01, 1.33953786e-01, 1.08468862e-01,\n",
       "       7.68113328e-02, 2.40332802e-01, 2.32735218e-01, 2.21141251e-01,\n",
       "       1.59988330e-01, 2.17866436e-02, 2.25770747e-01, 2.17158432e-01,\n",
       "       8.49303975e-01, 4.24030491e-01, 1.78829844e-01, 2.75683490e-01,\n",
       "       2.85712728e-02, 3.33577861e-01, 1.83703336e-01, 3.54211994e-01,\n",
       "       2.67606015e-01, 6.18293857e-02, 1.36667393e-01, 6.99108203e-01,\n",
       "       2.31690325e-02, 1.82066154e-01, 1.63386537e-01, 1.58418072e-01,\n",
       "       5.88473237e-02, 4.60335137e-01, 2.20078822e-01, 3.97298575e-01,\n",
       "       1.51628460e-01, 8.36998160e-01, 5.02458960e-02, 2.17372773e-01,\n",
       "       1.49246389e-01, 4.43791892e-01, 9.19164991e-02, 2.60849719e-02,\n",
       "       1.34665668e-01, 6.66876601e-02, 8.33720731e-02, 6.53020557e-02,\n",
       "       1.56353815e-01, 1.53684189e-01, 1.15651071e-01, 1.28016504e-01,\n",
       "       1.78693722e-02, 8.27791556e-03, 2.78689003e-01, 2.38120258e-02,\n",
       "       4.87778595e-02])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coefs = logistic_reg_initial.coef_.ravel()\n",
    "feat_importance = np.abs(coefs)\n",
    "feat_importance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7874c490",
   "metadata": {},
   "source": [
    "# Define CNN models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "09eb039f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import TensorDataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "365bd7b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScalogramCNN(nn.Module):\n",
    "    def __init__(self, in_channels=1):\n",
    "        super().__init__()\n",
    "        # 64x64 -> 32x32\n",
    "        self.conv1 = nn.Conv2d(in_channels, 16, kernel_size=3, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(16)\n",
    "        self.pool1 = nn.MaxPool2d(2)\n",
    "\n",
    "        # 32x32 -> 16x16\n",
    "        self.conv2 = nn.Conv2d(16, 32, kernel_size=3, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(32)\n",
    "        self.pool2 = nn.MaxPool2d(2)\n",
    "\n",
    "        # 16x16 -> 8x8\n",
    "        self.conv3 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
    "        self.bn3 = nn.BatchNorm2d(64)\n",
    "        self.pool3 = nn.MaxPool2d(2)\n",
    "\n",
    "        # Global average pool where it go from 8x8 -> 1x1 (works for any input size)\n",
    "        self.global_pool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "\n",
    "        # Small classifier head\n",
    "        self.dropout = nn.Dropout(p=0.5)\n",
    "        self.fc_out = nn.Linear(64, 1)   # turned into binary logits\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x -> (batch, in_channels, H, W)\n",
    "        x = self.pool1(F.relu(self.bn1(self.conv1(x))))\n",
    "        x = self.pool2(F.relu(self.bn2(self.conv2(x))))\n",
    "        x = self.pool3(F.relu(self.bn3(self.conv3(x))))\n",
    "\n",
    "        x = self.global_pool(x) # (batch, 64, 1, 1)\n",
    "        x = x.view(x.size(0), -1) # (batch, 64)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc_out(x) # (batch, 1) logits\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "5a7d0d60",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed=42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "9a3bbe4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_channels = 125\n",
    "n_bands = 5\n",
    "\n",
    "channel_importance = feat_importance.reshape(n_channels, n_bands).sum(axis=1)\n",
    "# channel_importance[i] = total importance for channel i across all 5 bands\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "2c5f9e72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 16  25  35  40  54 112 116 121]\n"
     ]
    }
   ],
   "source": [
    "k = 8 # k kinda arbitrary here just wanted to limit bc computation time\n",
    "top_channel_indices = np.argsort(channel_importance)[-k:]\n",
    "top_channel_indices = np.sort(top_channel_indices)\n",
    "print(top_channel_indices)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "8dd70f0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "logo = LeaveOneGroupOut()\n",
    "\n",
    "# just set it to these as default; not really gonna tune these\n",
    "num_epochs = 15\n",
    "batch_size = 16\n",
    "lr = 1e-3\n",
    "\n",
    "all_acc = []\n",
    "all_prec = []\n",
    "all_rec = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "b0cabcd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_scalogram(\n",
    "    signal,\n",
    "    fs,\n",
    "    fmin=1.0,\n",
    "    fmax=45.0,\n",
    "    num_freqs=32, # gonna experiment with later\n",
    "    out_size=(64, 64),\n",
    "    target_fs=32.0, # for downsampling\n",
    "):\n",
    "\n",
    "    # downsampling here in order to save time\n",
    "    decim_factor = int(max(1, round(fs / target_fs)))\n",
    "    if decim_factor > 1:\n",
    "        signal_ds = decimate(signal, decim_factor, zero_phase=True)\n",
    "        fs_eff = fs / decim_factor\n",
    "    else:\n",
    "        signal_ds = signal\n",
    "        fs_eff = fs\n",
    "\n",
    "    # using morl wavelet here\n",
    "    freqs = np.linspace(fmin, fmax, num_freqs)\n",
    "    wavelet = 'morl'\n",
    "    central_freq = pywt.central_frequency(wavelet)\n",
    "    scales = central_freq * fs_eff / freqs\n",
    "\n",
    "    coef, _ = pywt.cwt(signal_ds, scales, wavelet, sampling_period=1.0/fs_eff)\n",
    "    power = np.abs(coef) ** 2  # (num_freqs, time_ds)\n",
    "\n",
    "    # apply log transformation then normalize\n",
    "    power = np.log10(power + 1e-12)\n",
    "    power = (power - power.mean()) / (power.std() + 1e-6)\n",
    "\n",
    "    # wanna have it 64,64\n",
    "    img = resize(power, out_size, mode=\"reflect\", anti_aliasing=True)\n",
    "\n",
    "    return img\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "8df685b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_cnn shape: (200, 8, 64, 64)\n"
     ]
    }
   ],
   "source": [
    "k = len(top_channel_indices)\n",
    "H, W = 64, 64\n",
    "\n",
    "X_cnn = []\n",
    "y_cnn = []\n",
    "participant_ids_cnn = []\n",
    "song_ids_cnn = []\n",
    "\n",
    "for song in range(21, 31):\n",
    "    load_mat_file = loadmat(f\"../data/cleaned_eeg/song{song}_Imputed.mat\")\n",
    "    data = load_mat_file[f\"data{song}\"] # (channels=125, time, participants)\n",
    "    fs = float(load_mat_file[\"fs\"].squeeze())\n",
    "\n",
    "    channels, time, n_participants = data.shape\n",
    "\n",
    "    song_idx = song - 20\n",
    "    song_label = like_df[f\"song_{song_idx}\"].to_numpy()  # should be length 20\n",
    "\n",
    "    for participant in range(n_participants):\n",
    "        eeg_participant_full = data[:, :, participant]   # (125, time)\n",
    "\n",
    "        # limit to first 60 seconds cause more computationally efficient\n",
    "        max_seconds = 60\n",
    "        max_samples = min(time, int(max_seconds * fs))\n",
    "        eeg_participant = eeg_participant_full[:, :max_samples]  # (125, max_samples)\n",
    "\n",
    "        # limit to top k channels\n",
    "        eeg_top = eeg_participant[top_channel_indices, :]\n",
    "\n",
    "        scalograms = []\n",
    "        for ch_idx in range(k):\n",
    "            sig = eeg_top[ch_idx, :]\n",
    "            img = compute_scalogram(\n",
    "                sig,\n",
    "                fs,\n",
    "                fmin=1.0,\n",
    "                fmax=45.0,\n",
    "                num_freqs=32,\n",
    "                out_size=(H, W),\n",
    "                target_fs=32.0,\n",
    "            )\n",
    "            scalograms.append(img)\n",
    "\n",
    "        # stack into (k, H, W) tensor for this sample\n",
    "        scalograms = np.stack(scalograms, axis=0)  # this is (k, H, W)\n",
    "        X_cnn.append(scalograms)\n",
    "\n",
    "        #define Like is enjoyment >= 6\n",
    "        label = 1 if song_label[participant] >= 6 else 0\n",
    "        y_cnn.append(label)\n",
    "        participant_ids_cnn.append(participant)\n",
    "        song_ids_cnn.append(song_idx)\n",
    "\n",
    "# convert to arrays\n",
    "X_cnn = np.stack(X_cnn, axis=0)  # this (N, k, H, W)\n",
    "y_cnn = np.array(y_cnn)\n",
    "participant_ids_cnn = np.array(participant_ids_cnn)\n",
    "song_ids_cnn = np.array(song_ids_cnn)\n",
    "\n",
    "print(\"X_cnn shape:\", X_cnn.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "df8c49f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_cnn: (200, 8, 64, 64) y_cnn: (200,) groups: (200,)\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "X_cnn = np.array(X_cnn) # (N, k, H, W)\n",
    "y_cnn = np.array(y_cnn).astype(np.float32)\n",
    "groups = np.array(participant_ids_cnn)\n",
    "\n",
    "N, in_channels, H, W = X_cnn.shape\n",
    "print(\"X_cnn:\", X_cnn.shape, \"y_cnn:\", y_cnn.shape, \"groups:\", groups.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "0c84689b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Fold 1: leaving out participant 0\n",
      "\n",
      "Fold 2: leaving out participant 1\n",
      "\n",
      "Fold 3: leaving out participant 2\n",
      "\n",
      "Fold 4: leaving out participant 3\n",
      "\n",
      "Fold 5: leaving out participant 4\n",
      "\n",
      "Fold 6: leaving out participant 5\n",
      "\n",
      "Fold 7: leaving out participant 6\n",
      "\n",
      "Fold 8: leaving out participant 7\n",
      "\n",
      "Fold 9: leaving out participant 8\n",
      "\n",
      "Fold 10: leaving out participant 9\n",
      "\n",
      "Fold 11: leaving out participant 10\n",
      "\n",
      "Fold 12: leaving out participant 11\n",
      "\n",
      "Fold 13: leaving out participant 12\n",
      "\n",
      "Fold 14: leaving out participant 13\n",
      "\n",
      "Fold 15: leaving out participant 14\n",
      "\n",
      "Fold 16: leaving out participant 15\n",
      "\n",
      "Fold 17: leaving out participant 16\n",
      "\n",
      "Fold 18: leaving out participant 17\n",
      "\n",
      "Fold 19: leaving out participant 18\n",
      "\n",
      "Fold 20: leaving out participant 19\n"
     ]
    }
   ],
   "source": [
    "all_acc = []\n",
    "fold_precisions = [] # fold precisions\n",
    "fold_recalls = [] # fold recalls\n",
    "# used to compute not per fold metrics\n",
    "all_folds_y_true = []\n",
    "all_folds_y_pred = []\n",
    "\n",
    "for fold_idx, (train_idx, test_idx) in enumerate(logo.split(X_cnn, y_cnn, groups)):\n",
    "    print(f\"\\nFold {fold_idx+1}: leaving out participant {groups[test_idx][0]}\")\n",
    "\n",
    "    X_train = X_cnn[train_idx] # (n_train, k, H, W)\n",
    "    y_train = y_cnn[train_idx] # (n_train,)\n",
    "    X_test = X_cnn[test_idx] # (n_test, k, H, W)\n",
    "    y_test = y_cnn[test_idx] # (n_test,)\n",
    "\n",
    "    # convert to tensors cause u need to use tensors\n",
    "    X_train_t = torch.from_numpy(X_train).float()\n",
    "    y_train_t = torch.from_numpy(y_train).unsqueeze(1)\n",
    "    X_test_t = torch.from_numpy(X_test).float()\n",
    "    y_test_t = torch.from_numpy(y_test).unsqueeze(1)\n",
    "\n",
    "    train_ds = TensorDataset(X_train_t, y_train_t)\n",
    "    train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    # model each fold\n",
    "    model = ScalogramCNN(in_channels=in_channels).to(device)\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=1e-4)\n",
    "\n",
    "    # actual training loop\n",
    "    model.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        epoch_loss = 0.0\n",
    "        for xb, yb in train_loader:\n",
    "            xb = xb.to(device)\n",
    "            yb = yb.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            logits = model(xb)\n",
    "            loss = criterion(logits, yb)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            epoch_loss += loss.item() * xb.size(0)\n",
    "\n",
    "        epoch_loss /= len(train_ds)\n",
    "\n",
    "    # Actutually evaluating the model\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        logits_test = model(X_test_t.to(device))\n",
    "        probs_test = torch.sigmoid(logits_test).cpu().numpy().ravel()\n",
    "        y_pred = (probs_test >= 0.5).astype(int)\n",
    "\n",
    "    y_true = y_test.astype(int)\n",
    "\n",
    "    # store accuracy for each fold\n",
    "    acc = accuracy_score(y_true, y_pred)\n",
    "    all_acc.append(acc)\n",
    "\n",
    "    # accumulate all folds predictions for \"global\" metrics\n",
    "    all_folds_y_true.extend(y_true)\n",
    "    all_folds_y_pred.extend(y_pred)\n",
    "\n",
    "    # computing precision recall for each fold (class 0 and class 1)\n",
    "    prec, rec, f1, support = precision_recall_fscore_support(\n",
    "        y_true,\n",
    "        y_pred,\n",
    "        labels=[0, 1],\n",
    "        average=None,\n",
    "        zero_division=0\n",
    "    )\n",
    "    fold_precisions.append(prec)\n",
    "    fold_recalls.append(rec)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9846860c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metrics from aggregation across folds\n",
      "Accuracy across all folds: 0.52\n",
      "Mean precision for class 0 across folds: 0.4067063492063491\n",
      "Mean precision for class 1 across folds: 0.48166666666666674\n",
      "Mean recall for class 0 across folds: 0.46708333333333335\n",
      "Mean recall for class 1 across folds: 0.5134920634920636\n",
      "------------------------------------\n",
      "Metrics computed on all folds combined (global)\n",
      "Global precision for class 0: 0.4742268041237113\n",
      "Global precision for class 1: 0.5631067961165048\n",
      "Global recall for class 0: 0.5054945054945055\n",
      "Global recall for class 1: 0.5321100917431193\n"
     ]
    }
   ],
   "source": [
    "fold_precisions = np.array(fold_precisions)\n",
    "fold_recalls = np.array(fold_recalls)\n",
    "\n",
    "mean_prec_class0, mean_prec_class1 = fold_precisions.mean(axis=0)\n",
    "mean_recall_class0, mean_recall_class1 = fold_recalls.mean(axis=0)\n",
    "\n",
    "all_folds_y_true = np.array(all_folds_y_true)\n",
    "all_folds_y_pred = np.array(all_folds_y_pred)\n",
    "\n",
    "# \"global\" metrics over all predictions\n",
    "prec_global, rec_global, f1_global, support_global = precision_recall_fscore_support(\n",
    "    all_folds_y_true,\n",
    "    all_folds_y_pred,\n",
    "    labels=[0, 1],\n",
    "    average=None,\n",
    "    zero_division=0\n",
    ")\n",
    "\n",
    "print(\"Metrics from aggregation across folds\")\n",
    "print(\"Accuracy across all folds:\", np.mean(all_acc))\n",
    "\n",
    "print(\"Mean precision for class 0 across folds:\", mean_prec_class0)\n",
    "print(\"Mean precision for class 1 across folds:\", mean_prec_class1)\n",
    "\n",
    "print(\"Mean recall for class 0 across folds:\", mean_recall_class0)\n",
    "print(\"Mean recall for class 1 across folds:\", mean_recall_class1)\n",
    "\n",
    "# Global metrics just be the ypred and ground truth for all preds combined together. It makes a bit more sense to use here bc test size just so low from LOGO.\n",
    "print(\"------------------------------------\")\n",
    "print(\"Metrics computed on all folds combined (global)\")\n",
    "\n",
    "print(\"Global precision for class 0:\", prec_global[0])\n",
    "print(\"Global precision for class 1:\", prec_global[1])\n",
    "\n",
    "print(\"Global recall for class 0:\", rec_global[0])\n",
    "print(\"Global recall for class 1:\", rec_global[1])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
