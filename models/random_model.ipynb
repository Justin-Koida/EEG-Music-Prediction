{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5d7d3763",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import scipy.io\n",
    "import h5py\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.io import loadmat\n",
    "from scipy.signal import welch\n",
    "import seaborn as sns\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import LeaveOneGroupOut\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, precision_score, recall_score, precision_recall_fscore_support, confusion_matrix, ConfusionMatrixDisplay\n",
    "from sklearn.svm import SVC\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d24cbe25",
   "metadata": {},
   "source": [
    "# Creating the Like DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c6d40cd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HDF5 keys: ['behavioralRatings']\n"
     ]
    }
   ],
   "source": [
    "path = \"../data/behavioralRatings.mat\"\n",
    "with h5py.File(path, \"r\") as f:\n",
    "    print(\"HDF5 keys:\", list(f.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "db10a8bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type: <class 'h5py._hl.dataset.Dataset'>\n",
      "Shape: (2, 10, 20)\n",
      "Dtype: float64\n"
     ]
    }
   ],
   "source": [
    "with h5py.File(path, \"r\") as f:\n",
    "    br = f[\"behavioralRatings\"]\n",
    "    print(\"Type:\", type(br))\n",
    "    print(\"Shape:\", br.shape)\n",
    "    print(\"Dtype:\", br.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "740f42ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "with h5py.File(path, \"r\") as f:\n",
    "    br = f[\"behavioralRatings\"][:]   # turn into NumPy array, shape (2, 10, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d7bb3b9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20, 10, 2)\n"
     ]
    }
   ],
   "source": [
    "# (2, 10, 20) to (20, 10, 2) in order to turn into same structure as matlab \n",
    "ratings = np.transpose(br, (2, 1, 0))\n",
    "\n",
    "print(ratings.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9e46e242",
   "metadata": {},
   "outputs": [],
   "source": [
    "enjoyment = ratings[:, :, 1] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a68f3616",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subject</th>\n",
       "      <th>song_1</th>\n",
       "      <th>song_2</th>\n",
       "      <th>song_3</th>\n",
       "      <th>song_4</th>\n",
       "      <th>song_5</th>\n",
       "      <th>song_6</th>\n",
       "      <th>song_7</th>\n",
       "      <th>song_8</th>\n",
       "      <th>song_9</th>\n",
       "      <th>song_10</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>8.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>8.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>8.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>8.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>7.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>6.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>6.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>5.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>7.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>11</td>\n",
       "      <td>7.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>12</td>\n",
       "      <td>9.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>13</td>\n",
       "      <td>6.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>14</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>15</td>\n",
       "      <td>7.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>16</td>\n",
       "      <td>6.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>17</td>\n",
       "      <td>5.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>18</td>\n",
       "      <td>9.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>19</td>\n",
       "      <td>7.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>20</td>\n",
       "      <td>8.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    subject  song_1  song_2  song_3  song_4  song_5  song_6  song_7  song_8  \\\n",
       "0         1     8.0     8.0     5.0     5.0     9.0     7.0     6.0     7.0   \n",
       "1         2     8.0     8.0     7.0     5.0     3.0     7.0     5.0     5.0   \n",
       "2         3     8.0     7.0     8.0     6.0     8.0     7.0     7.0     8.0   \n",
       "3         4     8.0     7.0     2.0     7.0     9.0     6.0     7.0     7.0   \n",
       "4         5     6.0     8.0     6.0     5.0     7.0     7.0     8.0     7.0   \n",
       "5         6     4.0     5.0     4.0     5.0     5.0     5.0     4.0     4.0   \n",
       "6         7     6.0     2.0     2.0     5.0     3.0     7.0     3.0     5.0   \n",
       "7         8     5.0     8.0     3.0     7.0     5.0     6.0     6.0     6.0   \n",
       "8         9     7.0     5.0     6.0     5.0     6.0     5.0     4.0     6.0   \n",
       "9        10     5.0     4.0     2.0     7.0     5.0     6.0     4.0     7.0   \n",
       "10       11     7.0     5.0     2.0     4.0     7.0     9.0     7.0     8.0   \n",
       "11       12     9.0     6.0     6.0     8.0     6.0     7.0     6.0     8.0   \n",
       "12       13     6.0     7.0     5.0     9.0     7.0     7.0     6.0     8.0   \n",
       "13       14     3.0     5.0     5.0     6.0     5.0     6.0     4.0     6.0   \n",
       "14       15     7.0     4.0     4.0     3.0     5.0     5.0     3.0     6.0   \n",
       "15       16     6.0     5.0     4.0     6.0     5.0     5.0     7.0     6.0   \n",
       "16       17     5.0     6.0     4.0     5.0     7.0     7.0     4.0     6.0   \n",
       "17       18     9.0     7.0     8.0     9.0     9.0     9.0     9.0     9.0   \n",
       "18       19     7.0     6.0     6.0     8.0     5.0     8.0     5.0     7.0   \n",
       "19       20     8.0     8.0     2.0     6.0     8.0     9.0     4.0     6.0   \n",
       "\n",
       "    song_9  song_10  \n",
       "0      5.0      5.0  \n",
       "1      4.0      4.0  \n",
       "2      7.0      6.0  \n",
       "3      5.0      7.0  \n",
       "4      7.0      3.0  \n",
       "5      5.0      1.0  \n",
       "6      3.0      2.0  \n",
       "7      7.0      4.0  \n",
       "8      3.0      2.0  \n",
       "9      3.0      1.0  \n",
       "10     6.0      1.0  \n",
       "11     5.0      6.0  \n",
       "12     3.0      4.0  \n",
       "13     3.0      2.0  \n",
       "14     3.0      2.0  \n",
       "15     5.0      2.0  \n",
       "16     5.0      1.0  \n",
       "17     9.0      3.0  \n",
       "18     8.0      4.0  \n",
       "19     8.0      1.0  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "participants = np.arange(1, 21)\n",
    "songs = np.arange(1, 11)\n",
    "\n",
    "like_df = pd.DataFrame(enjoyment,\n",
    "                      index=participants,\n",
    "                      columns=[f\"song_{s}\" for s in songs]).rename_axis(\"subject\").reset_index()\n",
    "\n",
    "like_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81aad60a",
   "metadata": {},
   "source": [
    "# Feature Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "606b22b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basically mapping freq to names/categories\n",
    "BANDS = {\n",
    "    \"delta\": (1, 4),\n",
    "    \"theta\": (4, 8),\n",
    "    \"alpha\": (8, 12),\n",
    "    \"beta\": (12, 30),\n",
    "    \"gamma_low\": (30, 45),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "755b0198",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_bandpower(f, psd, fmin, fmax):\n",
    "    idx = (f >= fmin) & (f <= fmax)\n",
    "    return np.trapezoid(psd[idx], f[idx])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c6fd705",
   "metadata": {},
   "source": [
    "# Loading in Cleaned EEG data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b2f5172e",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f228f882",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_X = []\n",
    "all_y = []\n",
    "participant_ids = []\n",
    "song_ids = []\n",
    "\n",
    "# they had weird mappings in the study, these numbers correspond to file names\n",
    "# looping through each song\n",
    "for song in range(21, 31):\n",
    "    # loads in the file\n",
    "    load_mat_file = loadmat(f\"../data/cleaned_eeg/song{song}_Imputed.mat\")\n",
    "    # gets the current song\n",
    "    data = load_mat_file[f\"data{song}\"]\n",
    "\n",
    "    fs = float(load_mat_file[\"fs\"].squeeze())\n",
    "\n",
    "    channels, time, participants = data.shape\n",
    "\n",
    "    # converting werid mappings to start at 1\n",
    "    song_idx = song - 20\n",
    "    song_label = like_df[f\"song_{song_idx}\"].to_numpy()\n",
    "\n",
    "    # loop through each participant\n",
    "    for participant in range(participants):\n",
    "        # get eeg data for given participant for the given song\n",
    "        eeg_participant = data[:, :, participant]\n",
    "\n",
    "        # this chunk just gets the band power for each participant for given song\n",
    "        feature_list = []\n",
    "        for channel in range(channels):\n",
    "            f, psd = welch(eeg_participant[channel, :], fs = fs, nperseg = 2 * int(fs))\n",
    "            for fmin, fmax in BANDS.values():\n",
    "                band_power = calculate_bandpower(f, psd, fmin, fmax)\n",
    "                feature_list.append(band_power)\n",
    "\n",
    "        feature_vec = np.log10(np.array(feature_list) + .000000000000000000001)\n",
    "        all_X.append(feature_vec)\n",
    "\n",
    "        # basically liked if score above 6, no like below 6\n",
    "        all_y.append(1 if song_label[participant] >= 6 else 0)\n",
    "        participant_ids.append(participant)\n",
    "        song_ids.append(song_idx)\n",
    "\n",
    "# Convert to usable format\n",
    "X_all = np.vstack(all_X)\n",
    "y_all = np.array(all_y)\n",
    "\n",
    "participant_ids = np.array(participant_ids)\n",
    "song_ids = np.array(song_ids)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "28c285c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LeaveOneGroupOut()"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaler = StandardScaler()\n",
    "logo = LeaveOneGroupOut()\n",
    "X_scaled = scaler.fit_transform(X_all)\n",
    "logo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "adb43aa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "logistic_reg_initial = LogisticRegression(max_iter = 2000, random_state = 42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4e56c10",
   "metadata": {},
   "source": [
    "## Metrics for Logistic Reg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "24becbb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metrics from aggregation across folds\n",
      "Acurracy across all folds:  0.625\n",
      "Precision for Class 0:  0.4594841269841269\n",
      "Precision for Class 1:  0.5030158730158729\n",
      "Recall for Class 0:  0.545654761904762\n",
      "Recall for Class 1:  0.5473214285714285\n",
      "------------------------------------\n",
      "Computing Precision Recall whole data set (not per fold)\n",
      "Precision for class 0:  0.5816326530612245\n",
      "Precision for class 1:  0.6666666666666666\n",
      "Recall for class 0:  0.6263736263736264\n",
      "Recall for class 1:  0.6238532110091743\n"
     ]
    }
   ],
   "source": [
    "accuracies = []\n",
    "precisions = []\n",
    "recalls = []\n",
    "# conf_matrix_folds = []\n",
    "\n",
    "# collecting across all folds bc my prec recall kinda weird with some classes having none per fold\n",
    "all_folds_y_true = []\n",
    "all_folds_y_pred = []\n",
    "\n",
    "for train_idx, test_idx in logo.split(X_scaled, y_all, participant_ids):\n",
    "    X_train, X_test = X_scaled[train_idx], X_scaled[test_idx]\n",
    "    y_train, y_test = y_all[train_idx], y_all[test_idx]\n",
    "\n",
    "    logistic_reg_initial.fit(X_train, y_train)\n",
    "    y_pred = logistic_reg_initial.predict(X_test)\n",
    "    y_prob = logistic_reg_initial.predict_proba(X_test)[:, 1]\n",
    "\n",
    "    accuracies.append(accuracy_score(y_test, y_pred))\n",
    "\n",
    "    all_folds_y_true.extend(y_test)\n",
    "    all_folds_y_pred.extend(y_pred)\n",
    "\n",
    "    # cm = confusion_matrix(y_test, y_pred, labels = [0,1])\n",
    "    # conf_matrix_folds.append(cm)\n",
    "    \n",
    "    # precisions.append(precision_score(y_test, y_pred, pos_label=1, zero_division = 0))\n",
    "    # recalls.append(recall_score(y_test, y_pred, pos_label = 1, zero_division=0))\n",
    "\n",
    "    prec, rec, f1, support = precision_recall_fscore_support(\n",
    "        y_test,\n",
    "        y_pred,\n",
    "        labels = [0,1],\n",
    "        average = None,\n",
    "        zero_division = 0\n",
    "    )\n",
    "\n",
    "    precisions.append(prec)\n",
    "    recalls.append(rec)\n",
    "\n",
    "all_folds_y_true = np.array(all_folds_y_true)\n",
    "all_folds_y_pred = np.array(all_folds_y_pred)\n",
    "\n",
    "prec, rec, f1, support = precision_recall_fscore_support(\n",
    "    all_folds_y_true,\n",
    "    all_folds_y_pred,\n",
    "    labels = [0,1],\n",
    "    average = None,\n",
    "    zero_division = 0\n",
    ")\n",
    "\n",
    "# print(\"Precision across all folds: \", np.mean(precisions))\n",
    "# print(\"Recall across all folds: \", np.mean(recalls))\n",
    "\n",
    "precisions = np.array(precisions)\n",
    "recalls = np.array(recalls)\n",
    "\n",
    "mean_prec_class0, mean_prec_class1 = precisions.mean(axis = 0)\n",
    "mean_recall_class0, mean_recall_class1 = recalls.mean(axis = 0)\n",
    "\n",
    "print(\"Metrics from aggregation across folds\")\n",
    "print(\"Acurracy across all folds: \", np.mean(accuracies))\n",
    "\n",
    "print(\"Precision for Class 0: \", mean_prec_class0)\n",
    "print(\"Precision for Class 1: \", mean_prec_class1)\n",
    "\n",
    "print(\"Recall for Class 0: \", mean_recall_class0)\n",
    "print(\"Recall for Class 1: \", mean_recall_class1)\n",
    "\n",
    "print(\"------------------------------------\")\n",
    "print(\"Computing Precision Recall whole data set (not per fold)\")\n",
    "\n",
    "print(\"Precision for class 0: \", prec[0])\n",
    "print(\"Precision for class 1: \", prec[1])\n",
    "\n",
    "print(\"Recall for class 0: \", rec[0])\n",
    "print(\"Recall for class 1: \", rec[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "4f29bb70",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "conf_matrix = confusion_matrix(all_folds_y_true, all_folds_y_pred, labels = [0, 1])\n",
    "\n",
    "disp = ConfusionMatrixDisplay(\n",
    "    confusion_matrix = conf_matrix,\n",
    "    display_labels = [\"Not liked\", \"Liked\"]\n",
    ")\n",
    "\n",
    "disp.plot(cmap=\"Blues\", values_format = \".0f\")\n",
    "plt.title(\"Confusion Matrix (Version - Pooled Across Participants)\")\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"Ground Truth\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57ae1eb1",
   "metadata": {},
   "source": [
    "## Metrics for Linear SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e8ac48ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_linear = SVC(\n",
    "    kernel = \"linear\",\n",
    "    C = 1,\n",
    "    probability = True,\n",
    "    class_weight = \"balanced\",\n",
    "    random_state = 42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "15ebf02f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metrics from aggregation across folds\n",
      "Acurracy across all folds:  0.595\n",
      "Precision for Class 0:  0.45083333333333336\n",
      "Precision for Class 1:  0.4608531746031746\n",
      "Recall for Class 0:  0.5059523809523812\n",
      "Recall for Class 1:  0.5356150793650795\n",
      "------------------------------------\n",
      "Computing Precision Recall whole data set (not per fold)\n",
      "Precision for class 0:  0.5543478260869565\n",
      "Precision for class 1:  0.6296296296296297\n",
      "Recall for class 0:  0.5604395604395604\n",
      "Recall for class 1:  0.6238532110091743\n"
     ]
    }
   ],
   "source": [
    "accuracies = []\n",
    "precisions = []\n",
    "recalls = []\n",
    "\n",
    "# collecting across all folds bc my prec recall kinda weird with some classes having none per fold\n",
    "all_folds_y_true = []\n",
    "all_folds_y_pred = []\n",
    "\n",
    "for train_idx, test_idx in logo.split(X_scaled, y_all, participant_ids):\n",
    "    X_train, X_test = X_scaled[train_idx], X_scaled[test_idx]\n",
    "    y_train, y_test = y_all[train_idx], y_all[test_idx]\n",
    "\n",
    "    svm_linear.fit(X_train, y_train)\n",
    "    y_pred = svm_linear.predict(X_test)\n",
    "    y_prob = svm_linear.predict_proba(X_test)[:, 1]\n",
    "\n",
    "    accuracies.append(accuracy_score(y_test, y_pred))\n",
    "\n",
    "    all_folds_y_true.extend(y_test)\n",
    "    all_folds_y_pred.extend(y_pred)\n",
    "\n",
    "    \n",
    "    # precisions.append(precision_score(y_test, y_pred, pos_label=1, zero_division = 0))\n",
    "    # recalls.append(recall_score(y_test, y_pred, pos_label = 1, zero_division=0))\n",
    "\n",
    "    prec, rec, f1, support = precision_recall_fscore_support(\n",
    "        y_test,\n",
    "        y_pred,\n",
    "        labels = [0,1],\n",
    "        average = None,\n",
    "        zero_division = 0\n",
    "    )\n",
    "\n",
    "    precisions.append(prec)\n",
    "    recalls.append(rec)\n",
    "\n",
    "all_folds_y_true = np.array(all_folds_y_true)\n",
    "all_folds_y_pred = np.array(all_folds_y_pred)\n",
    "\n",
    "prec, rec, f1, support = precision_recall_fscore_support(\n",
    "    all_folds_y_true,\n",
    "    all_folds_y_pred,\n",
    "    labels = [0,1],\n",
    "    average = None,\n",
    "    zero_division = 0\n",
    ")\n",
    "\n",
    "# print(\"Precision across all folds: \", np.mean(precisions))\n",
    "# print(\"Recall across all folds: \", np.mean(recalls))\n",
    "\n",
    "precisions = np.array(precisions)\n",
    "recalls = np.array(recalls)\n",
    "\n",
    "mean_prec_class0, mean_prec_class1 = precisions.mean(axis = 0)\n",
    "mean_recall_class0, mean_recall_class1 = recalls.mean(axis = 0)\n",
    "\n",
    "print(\"Metrics from aggregation across folds\")\n",
    "print(\"Acurracy across all folds: \", np.mean(accuracies))\n",
    "\n",
    "print(\"Precision for Class 0: \", mean_prec_class0)\n",
    "print(\"Precision for Class 1: \", mean_prec_class1)\n",
    "\n",
    "print(\"Recall for Class 0: \", mean_recall_class0)\n",
    "print(\"Recall for Class 1: \", mean_recall_class1)\n",
    "\n",
    "print(\"------------------------------------\")\n",
    "print(\"Computing Precision Recall whole data set (not per fold)\")\n",
    "\n",
    "print(\"Precision for class 0: \", prec[0])\n",
    "print(\"Precision for class 1: \", prec[1])\n",
    "\n",
    "print(\"Recall for class 0: \", rec[0])\n",
    "print(\"Recall for class 1: \", rec[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3763cdb0",
   "metadata": {},
   "source": [
    "# Messing with Target"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9cbd16c",
   "metadata": {},
   "source": [
    "## Like >= 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0a34fad6",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_X = []\n",
    "all_y = []\n",
    "participant_ids = []\n",
    "song_ids = []\n",
    "\n",
    "# they had weird mappings in the study, these numbers correspond to file names\n",
    "# looping through each song\n",
    "for song in range(21, 31):\n",
    "    # loads in the file\n",
    "    load_mat_file = loadmat(f\"../data/cleaned_eeg/song{song}_Imputed.mat\")\n",
    "    # gets the current song\n",
    "    data = load_mat_file[f\"data{song}\"]\n",
    "\n",
    "    fs = float(load_mat_file[\"fs\"].squeeze())\n",
    "\n",
    "    channels, time, participants = data.shape\n",
    "\n",
    "    # converting werid mappings to start at 1\n",
    "    song_idx = song - 20\n",
    "    song_label = like_df[f\"song_{song_idx}\"].to_numpy()\n",
    "\n",
    "    # loop through each participant\n",
    "    for participant in range(participants):\n",
    "        # get eeg data for given participant for the given song\n",
    "        eeg_participant = data[:, :, participant]\n",
    "\n",
    "        # this chunk just gets the band power for each participant for given song\n",
    "        feature_list = []\n",
    "        for channel in range(channels):\n",
    "            f, psd = welch(eeg_participant[channel, :], fs = fs, nperseg = 2 * int(fs))\n",
    "            for fmin, fmax in BANDS.values():\n",
    "                band_power = calculate_bandpower(f, psd, fmin, fmax)\n",
    "                feature_list.append(band_power)\n",
    "\n",
    "        feature_vec = np.log10(np.array(feature_list) + .000000000000000000001)\n",
    "        all_X.append(feature_vec)\n",
    "\n",
    "        # basically liked if score above 6, no like below 6\n",
    "        all_y.append(1 if song_label[participant] >= 5 else 0)\n",
    "        participant_ids.append(participant)\n",
    "        song_ids.append(song_idx)\n",
    "\n",
    "# Convert to usable format\n",
    "X_all = np.vstack(all_X)\n",
    "y_all = np.array(all_y)\n",
    "\n",
    "participant_ids = np.array(participant_ids)\n",
    "song_ids = np.array(song_ids)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2dc489bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LeaveOneGroupOut()"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaler = StandardScaler()\n",
    "logo = LeaveOneGroupOut()\n",
    "X_scaled = scaler.fit_transform(X_all)\n",
    "logo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3d99c458",
   "metadata": {},
   "outputs": [],
   "source": [
    "logistic_reg_initial = LogisticRegression(max_iter = 2000, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fee75f29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metrics from aggregation across folds\n",
      "Acurracy across all folds:  0.615\n",
      "Precision for Class 0:  0.1894642857142857\n",
      "Precision for Class 1:  0.746388888888889\n",
      "Recall for Class 0:  0.3441666666666667\n",
      "Recall for Class 1:  0.6798015873015872\n",
      "------------------------------------\n",
      "Computing Precision Recall whole data set (not per fold)\n",
      "Precision for class 0:  0.296875\n",
      "Precision for class 1:  0.7647058823529411\n",
      "Recall for class 0:  0.37254901960784315\n",
      "Recall for class 1:  0.697986577181208\n"
     ]
    }
   ],
   "source": [
    "accuracies = []\n",
    "precisions = []\n",
    "recalls = []\n",
    "\n",
    "# collecting across all folds bc my prec recall kinda weird with some classes having none per fold\n",
    "all_folds_y_true = []\n",
    "all_folds_y_pred = []\n",
    "\n",
    "for train_idx, test_idx in logo.split(X_scaled, y_all, participant_ids):\n",
    "    X_train, X_test = X_scaled[train_idx], X_scaled[test_idx]\n",
    "    y_train, y_test = y_all[train_idx], y_all[test_idx]\n",
    "\n",
    "    logistic_reg_initial.fit(X_train, y_train)\n",
    "    y_pred = logistic_reg_initial.predict(X_test)\n",
    "    y_prob = logistic_reg_initial.predict_proba(X_test)[:, 1]\n",
    "\n",
    "    accuracies.append(accuracy_score(y_test, y_pred))\n",
    "\n",
    "    all_folds_y_true.extend(y_test)\n",
    "    all_folds_y_pred.extend(y_pred)\n",
    "\n",
    "    \n",
    "    # precisions.append(precision_score(y_test, y_pred, pos_label=1, zero_division = 0))\n",
    "    # recalls.append(recall_score(y_test, y_pred, pos_label = 1, zero_division=0))\n",
    "\n",
    "    prec, rec, f1, support = precision_recall_fscore_support(\n",
    "        y_test,\n",
    "        y_pred,\n",
    "        labels = [0,1],\n",
    "        average = None,\n",
    "        zero_division = 0\n",
    "    )\n",
    "\n",
    "    precisions.append(prec)\n",
    "    recalls.append(rec)\n",
    "\n",
    "all_folds_y_true = np.array(all_folds_y_true)\n",
    "all_folds_y_pred = np.array(all_folds_y_pred)\n",
    "\n",
    "prec, rec, f1, support = precision_recall_fscore_support(\n",
    "    all_folds_y_true,\n",
    "    all_folds_y_pred,\n",
    "    labels = [0,1],\n",
    "    average = None,\n",
    "    zero_division = 0\n",
    ")\n",
    "\n",
    "# print(\"Precision across all folds: \", np.mean(precisions))\n",
    "# print(\"Recall across all folds: \", np.mean(recalls))\n",
    "\n",
    "precisions = np.array(precisions)\n",
    "recalls = np.array(recalls)\n",
    "\n",
    "mean_prec_class0, mean_prec_class1 = precisions.mean(axis = 0)\n",
    "mean_recall_class0, mean_recall_class1 = recalls.mean(axis = 0)\n",
    "\n",
    "print(\"Metrics from aggregation across folds\")\n",
    "print(\"Acurracy across all folds: \", np.mean(accuracies))\n",
    "\n",
    "print(\"Precision for Class 0: \", mean_prec_class0)\n",
    "print(\"Precision for Class 1: \", mean_prec_class1)\n",
    "\n",
    "print(\"Recall for Class 0: \", mean_recall_class0)\n",
    "print(\"Recall for Class 1: \", mean_recall_class1)\n",
    "\n",
    "print(\"------------------------------------\")\n",
    "print(\"Computing Precision Recall whole data set (not per fold)\")\n",
    "\n",
    "print(\"Precision for class 0: \", prec[0])\n",
    "print(\"Precision for class 1: \", prec[1])\n",
    "\n",
    "print(\"Recall for class 0: \", rec[0])\n",
    "print(\"Recall for class 1: \", rec[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f54a00fe",
   "metadata": {},
   "source": [
    "## Like => score >= 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "01d5232d",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_X = []\n",
    "all_y = []\n",
    "participant_ids = []\n",
    "song_ids = []\n",
    "\n",
    "# they had weird mappings in the study, these numbers correspond to file names\n",
    "# looping through each song\n",
    "for song in range(21, 31):\n",
    "    # loads in the file\n",
    "    load_mat_file = loadmat(f\"../data/cleaned_eeg/song{song}_Imputed.mat\")\n",
    "    # gets the current song\n",
    "    data = load_mat_file[f\"data{song}\"]\n",
    "\n",
    "    fs = float(load_mat_file[\"fs\"].squeeze())\n",
    "\n",
    "    channels, time, participants = data.shape\n",
    "\n",
    "    # converting werid mappings to start at 1\n",
    "    song_idx = song - 20\n",
    "    song_label = like_df[f\"song_{song_idx}\"].to_numpy()\n",
    "\n",
    "    # loop through each participant\n",
    "    for participant in range(participants):\n",
    "        # get eeg data for given participant for the given song\n",
    "        eeg_participant = data[:, :, participant]\n",
    "\n",
    "        # this chunk just gets the band power for each participant for given song\n",
    "        feature_list = []\n",
    "        for channel in range(channels):\n",
    "            f, psd = welch(eeg_participant[channel, :], fs = fs, nperseg = 2 * int(fs))\n",
    "            for fmin, fmax in BANDS.values():\n",
    "                band_power = calculate_bandpower(f, psd, fmin, fmax)\n",
    "                feature_list.append(band_power)\n",
    "\n",
    "        feature_vec = np.log10(np.array(feature_list) + .000000000000000000001)\n",
    "        all_X.append(feature_vec)\n",
    "\n",
    "        # basically liked if score above 6, no like below 6\n",
    "        all_y.append(1 if song_label[participant] >= 7 else 0)\n",
    "        participant_ids.append(participant)\n",
    "        song_ids.append(song_idx)\n",
    "\n",
    "# Convert to usable format\n",
    "X_all = np.vstack(all_X)\n",
    "y_all = np.array(all_y)\n",
    "\n",
    "participant_ids = np.array(participant_ids)\n",
    "song_ids = np.array(song_ids)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1419dbe3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LeaveOneGroupOut()"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaler = StandardScaler()\n",
    "logo = LeaveOneGroupOut()\n",
    "X_scaled = scaler.fit_transform(X_all)\n",
    "logo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "19d167b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "logistic_reg_initial = LogisticRegression(max_iter = 2000, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a2406bd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metrics from aggregation across folds\n",
      "Acurracy across all folds:  0.5900000000000001\n",
      "Precision for Class 0:  0.5782738095238095\n",
      "Precision for Class 1:  0.3293452380952381\n",
      "Recall for Class 0:  0.6497222222222222\n",
      "Recall for Class 1:  0.41146825396825387\n",
      "------------------------------------\n",
      "Computing Precision Recall whole data set (not per fold)\n",
      "Precision for class 0:  0.680327868852459\n",
      "Precision for class 1:  0.44871794871794873\n",
      "Recall for class 0:  0.6587301587301587\n",
      "Recall for class 1:  0.47297297297297297\n"
     ]
    }
   ],
   "source": [
    "accuracies = []\n",
    "precisions = []\n",
    "recalls = []\n",
    "\n",
    "# collecting across all folds bc my prec recall kinda weird with some classes having none per fold\n",
    "all_folds_y_true = []\n",
    "all_folds_y_pred = []\n",
    "\n",
    "for train_idx, test_idx in logo.split(X_scaled, y_all, participant_ids):\n",
    "    X_train, X_test = X_scaled[train_idx], X_scaled[test_idx]\n",
    "    y_train, y_test = y_all[train_idx], y_all[test_idx]\n",
    "\n",
    "    logistic_reg_initial.fit(X_train, y_train)\n",
    "    y_pred = logistic_reg_initial.predict(X_test)\n",
    "    y_prob = logistic_reg_initial.predict_proba(X_test)[:, 1]\n",
    "\n",
    "    accuracies.append(accuracy_score(y_test, y_pred))\n",
    "\n",
    "    all_folds_y_true.extend(y_test)\n",
    "    all_folds_y_pred.extend(y_pred)\n",
    "\n",
    "    \n",
    "    # precisions.append(precision_score(y_test, y_pred, pos_label=1, zero_division = 0))\n",
    "    # recalls.append(recall_score(y_test, y_pred, pos_label = 1, zero_division=0))\n",
    "\n",
    "    prec, rec, f1, support = precision_recall_fscore_support(\n",
    "        y_test,\n",
    "        y_pred,\n",
    "        labels = [0,1],\n",
    "        average = None,\n",
    "        zero_division = 0\n",
    "    )\n",
    "\n",
    "    precisions.append(prec)\n",
    "    recalls.append(rec)\n",
    "\n",
    "all_folds_y_true = np.array(all_folds_y_true)\n",
    "all_folds_y_pred = np.array(all_folds_y_pred)\n",
    "\n",
    "prec, rec, f1, support = precision_recall_fscore_support(\n",
    "    all_folds_y_true,\n",
    "    all_folds_y_pred,\n",
    "    labels = [0,1],\n",
    "    average = None,\n",
    "    zero_division = 0\n",
    ")\n",
    "\n",
    "# print(\"Precision across all folds: \", np.mean(precisions))\n",
    "# print(\"Recall across all folds: \", np.mean(recalls))\n",
    "\n",
    "precisions = np.array(precisions)\n",
    "recalls = np.array(recalls)\n",
    "\n",
    "mean_prec_class0, mean_prec_class1 = precisions.mean(axis = 0)\n",
    "mean_recall_class0, mean_recall_class1 = recalls.mean(axis = 0)\n",
    "\n",
    "print(\"Metrics from aggregation across folds\")\n",
    "print(\"Acurracy across all folds: \", np.mean(accuracies))\n",
    "\n",
    "print(\"Precision for Class 0: \", mean_prec_class0)\n",
    "print(\"Precision for Class 1: \", mean_prec_class1)\n",
    "\n",
    "print(\"Recall for Class 0: \", mean_recall_class0)\n",
    "print(\"Recall for Class 1: \", mean_recall_class1)\n",
    "\n",
    "print(\"------------------------------------\")\n",
    "print(\"Computing Precision Recall whole data set (not per fold)\")\n",
    "\n",
    "print(\"Precision for class 0: \", prec[0])\n",
    "print(\"Precision for class 1: \", prec[1])\n",
    "\n",
    "print(\"Recall for class 0: \", rec[0])\n",
    "print(\"Recall for class 1: \", rec[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "322d85da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        , 0.5       ],\n",
       "       [0.6       , 0.        ],\n",
       "       [0.25      , 1.        ],\n",
       "       [0.5       , 0.75      ],\n",
       "       [0.        , 0.6       ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.9       , 0.        ],\n",
       "       [0.7       , 0.        ],\n",
       "       [0.83333333, 0.        ],\n",
       "       [0.85714286, 0.33333333],\n",
       "       [0.66666667, 0.75      ],\n",
       "       [1.        , 0.5       ],\n",
       "       [0.33333333, 0.42857143],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 0.1       ],\n",
       "       [1.        , 0.125     ],\n",
       "       [0.8       , 0.        ],\n",
       "       [0.125     , 1.        ],\n",
       "       [0.5       , 0.5       ],\n",
       "       [0.5       , 0.        ]])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "precisions"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
