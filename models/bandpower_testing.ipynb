{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5e5edeff",
   "metadata": {},
   "source": [
    "Explored more bandpower features. \n",
    "\n",
    "Tried combining log bandpower, absolute bandpower and relative bandpower. Thought this could boost performance. Wanted to test windowing, but that is really computationally expensive. Spoiler, it didnt really.\n",
    "\n",
    "I knew this would blow up the number of features, but still thought something like a random forest could handle this extra features. it did not. Moving onto scalograms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5d7d3763",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import scipy.io\n",
    "import h5py\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.io import loadmat\n",
    "from scipy.signal import welch\n",
    "import seaborn as sns\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import LeaveOneGroupOut\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, precision_score, recall_score, precision_recall_fscore_support, confusion_matrix, ConfusionMatrixDisplay\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d24cbe25",
   "metadata": {},
   "source": [
    "# Creating the Like DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c6d40cd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HDF5 keys: ['behavioralRatings']\n"
     ]
    }
   ],
   "source": [
    "path = \"../data/behavioralRatings.mat\"\n",
    "with h5py.File(path, \"r\") as f:\n",
    "    print(\"HDF5 keys:\", list(f.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "db10a8bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type: <class 'h5py._hl.dataset.Dataset'>\n",
      "Shape: (2, 10, 20)\n",
      "Dtype: float64\n"
     ]
    }
   ],
   "source": [
    "with h5py.File(path, \"r\") as f:\n",
    "    br = f[\"behavioralRatings\"]\n",
    "    print(\"Type:\", type(br))\n",
    "    print(\"Shape:\", br.shape)\n",
    "    print(\"Dtype:\", br.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "740f42ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "with h5py.File(path, \"r\") as f:\n",
    "    br = f[\"behavioralRatings\"][:]   # turn into NumPy array, shape (2, 10, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d7bb3b9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20, 10, 2)\n"
     ]
    }
   ],
   "source": [
    "# (2, 10, 20) to (20, 10, 2) in order to turn into same structure as matlab \n",
    "ratings = np.transpose(br, (2, 1, 0))\n",
    "\n",
    "print(ratings.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9e46e242",
   "metadata": {},
   "outputs": [],
   "source": [
    "enjoyment = ratings[:, :, 1] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a68f3616",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subject</th>\n",
       "      <th>song_1</th>\n",
       "      <th>song_2</th>\n",
       "      <th>song_3</th>\n",
       "      <th>song_4</th>\n",
       "      <th>song_5</th>\n",
       "      <th>song_6</th>\n",
       "      <th>song_7</th>\n",
       "      <th>song_8</th>\n",
       "      <th>song_9</th>\n",
       "      <th>song_10</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>8.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>8.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>8.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>8.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>7.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>6.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>6.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>5.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>7.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>11</td>\n",
       "      <td>7.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>12</td>\n",
       "      <td>9.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>13</td>\n",
       "      <td>6.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>14</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>15</td>\n",
       "      <td>7.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>16</td>\n",
       "      <td>6.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>17</td>\n",
       "      <td>5.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>18</td>\n",
       "      <td>9.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>19</td>\n",
       "      <td>7.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>20</td>\n",
       "      <td>8.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    subject  song_1  song_2  song_3  song_4  song_5  song_6  song_7  song_8  \\\n",
       "0         1     8.0     8.0     5.0     5.0     9.0     7.0     6.0     7.0   \n",
       "1         2     8.0     8.0     7.0     5.0     3.0     7.0     5.0     5.0   \n",
       "2         3     8.0     7.0     8.0     6.0     8.0     7.0     7.0     8.0   \n",
       "3         4     8.0     7.0     2.0     7.0     9.0     6.0     7.0     7.0   \n",
       "4         5     6.0     8.0     6.0     5.0     7.0     7.0     8.0     7.0   \n",
       "5         6     4.0     5.0     4.0     5.0     5.0     5.0     4.0     4.0   \n",
       "6         7     6.0     2.0     2.0     5.0     3.0     7.0     3.0     5.0   \n",
       "7         8     5.0     8.0     3.0     7.0     5.0     6.0     6.0     6.0   \n",
       "8         9     7.0     5.0     6.0     5.0     6.0     5.0     4.0     6.0   \n",
       "9        10     5.0     4.0     2.0     7.0     5.0     6.0     4.0     7.0   \n",
       "10       11     7.0     5.0     2.0     4.0     7.0     9.0     7.0     8.0   \n",
       "11       12     9.0     6.0     6.0     8.0     6.0     7.0     6.0     8.0   \n",
       "12       13     6.0     7.0     5.0     9.0     7.0     7.0     6.0     8.0   \n",
       "13       14     3.0     5.0     5.0     6.0     5.0     6.0     4.0     6.0   \n",
       "14       15     7.0     4.0     4.0     3.0     5.0     5.0     3.0     6.0   \n",
       "15       16     6.0     5.0     4.0     6.0     5.0     5.0     7.0     6.0   \n",
       "16       17     5.0     6.0     4.0     5.0     7.0     7.0     4.0     6.0   \n",
       "17       18     9.0     7.0     8.0     9.0     9.0     9.0     9.0     9.0   \n",
       "18       19     7.0     6.0     6.0     8.0     5.0     8.0     5.0     7.0   \n",
       "19       20     8.0     8.0     2.0     6.0     8.0     9.0     4.0     6.0   \n",
       "\n",
       "    song_9  song_10  \n",
       "0      5.0      5.0  \n",
       "1      4.0      4.0  \n",
       "2      7.0      6.0  \n",
       "3      5.0      7.0  \n",
       "4      7.0      3.0  \n",
       "5      5.0      1.0  \n",
       "6      3.0      2.0  \n",
       "7      7.0      4.0  \n",
       "8      3.0      2.0  \n",
       "9      3.0      1.0  \n",
       "10     6.0      1.0  \n",
       "11     5.0      6.0  \n",
       "12     3.0      4.0  \n",
       "13     3.0      2.0  \n",
       "14     3.0      2.0  \n",
       "15     5.0      2.0  \n",
       "16     5.0      1.0  \n",
       "17     9.0      3.0  \n",
       "18     8.0      4.0  \n",
       "19     8.0      1.0  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "participants = np.arange(1, 21)\n",
    "songs = np.arange(1, 11)\n",
    "\n",
    "like_df = pd.DataFrame(enjoyment,\n",
    "                      index=participants,\n",
    "                      columns=[f\"song_{s}\" for s in songs]).rename_axis(\"subject\").reset_index()\n",
    "\n",
    "like_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81aad60a",
   "metadata": {},
   "source": [
    "# Feature Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "606b22b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basically mapping freq to names/categories\n",
    "BANDS = {\n",
    "    \"delta\": (1, 4),\n",
    "    \"theta\": (4, 8),\n",
    "    \"alpha\": (8, 12),\n",
    "    \"beta\": (12, 30),\n",
    "    \"gamma_low\": (30, 45),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "92da5641",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "755b0198",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_bandpower(f, psd, fmin, fmax):\n",
    "    idx = (f >= fmin) & (f <= fmax)\n",
    "    return np.trapezoid(psd[idx], f[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cc69ea7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_all shape: (200, 1875)\n",
      "y_all shape: (200,)\n"
     ]
    }
   ],
   "source": [
    "all_X = []\n",
    "all_y = []\n",
    "participant_ids = []\n",
    "song_ids = []\n",
    "\n",
    "for song in range(21, 31):\n",
    "\n",
    "    load_mat_file = loadmat(f\"../data/cleaned_eeg/song{song}_Imputed.mat\")\n",
    "    data = load_mat_file[f\"data{song}\"] # (channels=125, time, participants)\n",
    "    fs = float(load_mat_file[\"fs\"].squeeze())\n",
    "\n",
    "    channels, time, n_participants = data.shape\n",
    "\n",
    "    song_idx = song - 20\n",
    "    song_label = like_df[f\"song_{song_idx}\"].to_numpy()\n",
    "\n",
    "    for participant in range(n_participants):\n",
    "        eeg_participant = data[:, :, participant]  # (channels, time)\n",
    "\n",
    "        # ABSOLUTE BANDPOWER!!\n",
    "        full_abs_bandpower = []\n",
    "\n",
    "        for ch in range(channels):\n",
    "            sig = eeg_participant[ch, :]\n",
    "            f_full, psd_full = welch(\n",
    "                sig,\n",
    "                fs=fs,\n",
    "                nperseg=2 * int(fs)\n",
    "            )\n",
    "\n",
    "            for fmin, fmax in BANDS.values():\n",
    "                bp = calculate_bandpower(f_full, psd_full, fmin, fmax)\n",
    "                full_abs_bandpower.append(bp)\n",
    "\n",
    "        full_abs_bandpower = np.array(full_abs_bandpower)  # (channels * n_bands,)\n",
    "\n",
    "        # RELATIVE BANDPOWER (per channel)\n",
    "        full_rel_bandpower = []\n",
    "        n_bands = len(BANDS)\n",
    "\n",
    "        full_abs_matrix = full_abs_bandpower.reshape(channels, n_bands)\n",
    "\n",
    "        for ch in range(channels):\n",
    "            total_power = full_abs_matrix[ch].sum() + .00000000000000001\n",
    "            rel_powers = full_abs_matrix[ch] / total_power\n",
    "            full_rel_bandpower.extend(rel_powers)\n",
    "\n",
    "        full_rel_bandpower = np.array(full_rel_bandpower)  # (channels x n_bands,)\n",
    "\n",
    "        # LOG BANDPOWER\n",
    "        full_log_bandpower = np.log10(full_abs_bandpower + .0000000000000001)  # (channels x n_bands,)\n",
    "\n",
    "        # combine features -> [absolute, relative, log-absolute]\n",
    "        feature_vec = np.concatenate([\n",
    "            full_abs_bandpower,\n",
    "            full_rel_bandpower,\n",
    "            full_log_bandpower\n",
    "        ])\n",
    "\n",
    "        all_X.append(feature_vec)\n",
    "        all_y.append(1 if song_label[participant] >= 6 else 0)\n",
    "        participant_ids.append(participant)\n",
    "        song_ids.append(song_idx)\n",
    "\n",
    "# Convert lists to arrays\n",
    "X_all = np.vstack(all_X)\n",
    "y_all = np.array(all_y)\n",
    "participant_ids = np.array(participant_ids)\n",
    "song_ids = np.array(song_ids)\n",
    "\n",
    "print(\"X_all shape:\", X_all.shape)\n",
    "print(\"y_all shape:\", y_all.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "87058781",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>1865</th>\n",
       "      <th>1866</th>\n",
       "      <th>1867</th>\n",
       "      <th>1868</th>\n",
       "      <th>1869</th>\n",
       "      <th>1870</th>\n",
       "      <th>1871</th>\n",
       "      <th>1872</th>\n",
       "      <th>1873</th>\n",
       "      <th>1874</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.437154</td>\n",
       "      <td>3.295769</td>\n",
       "      <td>2.291528</td>\n",
       "      <td>3.901327</td>\n",
       "      <td>1.134805</td>\n",
       "      <td>4.739319</td>\n",
       "      <td>2.946570</td>\n",
       "      <td>2.193457</td>\n",
       "      <td>7.466447</td>\n",
       "      <td>5.888619</td>\n",
       "      <td>...</td>\n",
       "      <td>0.533776</td>\n",
       "      <td>0.590708</td>\n",
       "      <td>0.351696</td>\n",
       "      <td>0.670366</td>\n",
       "      <td>-0.092422</td>\n",
       "      <td>0.621949</td>\n",
       "      <td>0.751444</td>\n",
       "      <td>0.606980</td>\n",
       "      <td>0.855188</td>\n",
       "      <td>-0.121695</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6.802102</td>\n",
       "      <td>2.209015</td>\n",
       "      <td>6.177883</td>\n",
       "      <td>2.973930</td>\n",
       "      <td>0.719417</td>\n",
       "      <td>7.997379</td>\n",
       "      <td>2.289363</td>\n",
       "      <td>7.396735</td>\n",
       "      <td>3.644470</td>\n",
       "      <td>0.949761</td>\n",
       "      <td>...</td>\n",
       "      <td>0.833698</td>\n",
       "      <td>0.342209</td>\n",
       "      <td>0.798735</td>\n",
       "      <td>0.474534</td>\n",
       "      <td>-0.358329</td>\n",
       "      <td>0.764603</td>\n",
       "      <td>0.306948</td>\n",
       "      <td>0.732067</td>\n",
       "      <td>0.456729</td>\n",
       "      <td>-0.532869</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7.803644</td>\n",
       "      <td>3.607513</td>\n",
       "      <td>5.169091</td>\n",
       "      <td>2.381346</td>\n",
       "      <td>0.767014</td>\n",
       "      <td>32.320292</td>\n",
       "      <td>6.439324</td>\n",
       "      <td>6.739752</td>\n",
       "      <td>6.392207</td>\n",
       "      <td>3.306073</td>\n",
       "      <td>...</td>\n",
       "      <td>0.756495</td>\n",
       "      <td>0.444941</td>\n",
       "      <td>0.606594</td>\n",
       "      <td>0.282850</td>\n",
       "      <td>-0.382810</td>\n",
       "      <td>0.581887</td>\n",
       "      <td>0.701730</td>\n",
       "      <td>0.814113</td>\n",
       "      <td>0.309131</td>\n",
       "      <td>-0.595847</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>12.926713</td>\n",
       "      <td>5.191892</td>\n",
       "      <td>2.780866</td>\n",
       "      <td>3.294347</td>\n",
       "      <td>1.304612</td>\n",
       "      <td>13.052549</td>\n",
       "      <td>8.681723</td>\n",
       "      <td>3.477780</td>\n",
       "      <td>4.630973</td>\n",
       "      <td>1.674099</td>\n",
       "      <td>...</td>\n",
       "      <td>0.917731</td>\n",
       "      <td>0.762848</td>\n",
       "      <td>0.297982</td>\n",
       "      <td>0.375505</td>\n",
       "      <td>-0.152520</td>\n",
       "      <td>0.981974</td>\n",
       "      <td>0.834027</td>\n",
       "      <td>0.653438</td>\n",
       "      <td>0.462594</td>\n",
       "      <td>-0.248389</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>11.682139</td>\n",
       "      <td>4.208124</td>\n",
       "      <td>4.340531</td>\n",
       "      <td>5.057867</td>\n",
       "      <td>1.501342</td>\n",
       "      <td>26.018133</td>\n",
       "      <td>7.611409</td>\n",
       "      <td>7.664269</td>\n",
       "      <td>9.744701</td>\n",
       "      <td>3.542585</td>\n",
       "      <td>...</td>\n",
       "      <td>1.078836</td>\n",
       "      <td>0.685927</td>\n",
       "      <td>0.544265</td>\n",
       "      <td>0.818167</td>\n",
       "      <td>0.302669</td>\n",
       "      <td>0.959196</td>\n",
       "      <td>0.666225</td>\n",
       "      <td>0.569900</td>\n",
       "      <td>0.610659</td>\n",
       "      <td>-0.207420</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>195</th>\n",
       "      <td>9.161990</td>\n",
       "      <td>3.246197</td>\n",
       "      <td>1.170821</td>\n",
       "      <td>1.983636</td>\n",
       "      <td>0.785976</td>\n",
       "      <td>6.466018</td>\n",
       "      <td>2.356428</td>\n",
       "      <td>0.908777</td>\n",
       "      <td>1.341314</td>\n",
       "      <td>0.556842</td>\n",
       "      <td>...</td>\n",
       "      <td>0.722118</td>\n",
       "      <td>0.373444</td>\n",
       "      <td>-0.052891</td>\n",
       "      <td>0.131516</td>\n",
       "      <td>-0.337811</td>\n",
       "      <td>0.684117</td>\n",
       "      <td>0.551660</td>\n",
       "      <td>0.033654</td>\n",
       "      <td>0.125793</td>\n",
       "      <td>-0.488601</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196</th>\n",
       "      <td>11.361508</td>\n",
       "      <td>5.616728</td>\n",
       "      <td>9.461826</td>\n",
       "      <td>2.736033</td>\n",
       "      <td>0.907838</td>\n",
       "      <td>8.636768</td>\n",
       "      <td>5.996460</td>\n",
       "      <td>12.358313</td>\n",
       "      <td>3.628967</td>\n",
       "      <td>1.128384</td>\n",
       "      <td>...</td>\n",
       "      <td>0.798728</td>\n",
       "      <td>0.835067</td>\n",
       "      <td>1.071288</td>\n",
       "      <td>0.579723</td>\n",
       "      <td>-0.062425</td>\n",
       "      <td>0.890615</td>\n",
       "      <td>1.048405</td>\n",
       "      <td>1.262889</td>\n",
       "      <td>0.672654</td>\n",
       "      <td>-0.202523</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>197</th>\n",
       "      <td>12.891483</td>\n",
       "      <td>3.990714</td>\n",
       "      <td>2.202423</td>\n",
       "      <td>3.169409</td>\n",
       "      <td>1.981254</td>\n",
       "      <td>8.765260</td>\n",
       "      <td>2.735044</td>\n",
       "      <td>1.756600</td>\n",
       "      <td>6.674329</td>\n",
       "      <td>4.085634</td>\n",
       "      <td>...</td>\n",
       "      <td>0.642207</td>\n",
       "      <td>0.274625</td>\n",
       "      <td>0.051561</td>\n",
       "      <td>0.318407</td>\n",
       "      <td>0.073765</td>\n",
       "      <td>0.691856</td>\n",
       "      <td>0.321364</td>\n",
       "      <td>0.111347</td>\n",
       "      <td>0.213866</td>\n",
       "      <td>-0.034468</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>198</th>\n",
       "      <td>12.343607</td>\n",
       "      <td>6.736856</td>\n",
       "      <td>7.796902</td>\n",
       "      <td>7.058343</td>\n",
       "      <td>2.654313</td>\n",
       "      <td>12.927676</td>\n",
       "      <td>5.909995</td>\n",
       "      <td>7.705911</td>\n",
       "      <td>7.626168</td>\n",
       "      <td>1.666947</td>\n",
       "      <td>...</td>\n",
       "      <td>0.641523</td>\n",
       "      <td>0.536660</td>\n",
       "      <td>0.789501</td>\n",
       "      <td>0.678763</td>\n",
       "      <td>-0.214273</td>\n",
       "      <td>0.651204</td>\n",
       "      <td>0.613554</td>\n",
       "      <td>0.759610</td>\n",
       "      <td>0.493334</td>\n",
       "      <td>-0.325397</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199</th>\n",
       "      <td>6.698374</td>\n",
       "      <td>1.993127</td>\n",
       "      <td>1.090463</td>\n",
       "      <td>3.524258</td>\n",
       "      <td>1.970634</td>\n",
       "      <td>5.493545</td>\n",
       "      <td>1.625094</td>\n",
       "      <td>0.914087</td>\n",
       "      <td>2.978165</td>\n",
       "      <td>1.654989</td>\n",
       "      <td>...</td>\n",
       "      <td>0.625190</td>\n",
       "      <td>0.034581</td>\n",
       "      <td>-0.185647</td>\n",
       "      <td>0.266156</td>\n",
       "      <td>0.016930</td>\n",
       "      <td>0.660981</td>\n",
       "      <td>0.365677</td>\n",
       "      <td>-0.040350</td>\n",
       "      <td>0.229787</td>\n",
       "      <td>-0.271086</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>200 rows Ã— 1875 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          0         1         2         3         4          5         6     \\\n",
       "0     5.437154  3.295769  2.291528  3.901327  1.134805   4.739319  2.946570   \n",
       "1     6.802102  2.209015  6.177883  2.973930  0.719417   7.997379  2.289363   \n",
       "2     7.803644  3.607513  5.169091  2.381346  0.767014  32.320292  6.439324   \n",
       "3    12.926713  5.191892  2.780866  3.294347  1.304612  13.052549  8.681723   \n",
       "4    11.682139  4.208124  4.340531  5.057867  1.501342  26.018133  7.611409   \n",
       "..         ...       ...       ...       ...       ...        ...       ...   \n",
       "195   9.161990  3.246197  1.170821  1.983636  0.785976   6.466018  2.356428   \n",
       "196  11.361508  5.616728  9.461826  2.736033  0.907838   8.636768  5.996460   \n",
       "197  12.891483  3.990714  2.202423  3.169409  1.981254   8.765260  2.735044   \n",
       "198  12.343607  6.736856  7.796902  7.058343  2.654313  12.927676  5.909995   \n",
       "199   6.698374  1.993127  1.090463  3.524258  1.970634   5.493545  1.625094   \n",
       "\n",
       "          7         8         9     ...      1865      1866      1867  \\\n",
       "0     2.193457  7.466447  5.888619  ...  0.533776  0.590708  0.351696   \n",
       "1     7.396735  3.644470  0.949761  ...  0.833698  0.342209  0.798735   \n",
       "2     6.739752  6.392207  3.306073  ...  0.756495  0.444941  0.606594   \n",
       "3     3.477780  4.630973  1.674099  ...  0.917731  0.762848  0.297982   \n",
       "4     7.664269  9.744701  3.542585  ...  1.078836  0.685927  0.544265   \n",
       "..         ...       ...       ...  ...       ...       ...       ...   \n",
       "195   0.908777  1.341314  0.556842  ...  0.722118  0.373444 -0.052891   \n",
       "196  12.358313  3.628967  1.128384  ...  0.798728  0.835067  1.071288   \n",
       "197   1.756600  6.674329  4.085634  ...  0.642207  0.274625  0.051561   \n",
       "198   7.705911  7.626168  1.666947  ...  0.641523  0.536660  0.789501   \n",
       "199   0.914087  2.978165  1.654989  ...  0.625190  0.034581 -0.185647   \n",
       "\n",
       "         1868      1869      1870      1871      1872      1873      1874  \n",
       "0    0.670366 -0.092422  0.621949  0.751444  0.606980  0.855188 -0.121695  \n",
       "1    0.474534 -0.358329  0.764603  0.306948  0.732067  0.456729 -0.532869  \n",
       "2    0.282850 -0.382810  0.581887  0.701730  0.814113  0.309131 -0.595847  \n",
       "3    0.375505 -0.152520  0.981974  0.834027  0.653438  0.462594 -0.248389  \n",
       "4    0.818167  0.302669  0.959196  0.666225  0.569900  0.610659 -0.207420  \n",
       "..        ...       ...       ...       ...       ...       ...       ...  \n",
       "195  0.131516 -0.337811  0.684117  0.551660  0.033654  0.125793 -0.488601  \n",
       "196  0.579723 -0.062425  0.890615  1.048405  1.262889  0.672654 -0.202523  \n",
       "197  0.318407  0.073765  0.691856  0.321364  0.111347  0.213866 -0.034468  \n",
       "198  0.678763 -0.214273  0.651204  0.613554  0.759610  0.493334 -0.325397  \n",
       "199  0.266156  0.016930  0.660981  0.365677 -0.040350  0.229787 -0.271086  \n",
       "\n",
       "[200 rows x 1875 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(X_all)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c6fd705",
   "metadata": {},
   "source": [
    "# Loading in Cleaned EEG data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b2f5172e",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f228f882",
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_X = []\n",
    "# all_y = []\n",
    "# participant_ids = []\n",
    "# song_ids = []\n",
    "\n",
    "# # they had weird mappings in the study, these numbers correspond to file names\n",
    "# # looping through each song\n",
    "# for song in range(21, 31):\n",
    "#     # loads in the file\n",
    "#     load_mat_file = loadmat(f\"../data/cleaned_eeg/song{song}_Imputed.mat\")\n",
    "#     # gets the current song\n",
    "#     data = load_mat_file[f\"data{song}\"]\n",
    "\n",
    "#     fs = float(load_mat_file[\"fs\"].squeeze())\n",
    "\n",
    "#     channels, time, participants = data.shape\n",
    "\n",
    "#     # converting werid mappings to start at 1\n",
    "#     song_idx = song - 20\n",
    "#     song_label = like_df[f\"song_{song_idx}\"].to_numpy()\n",
    "\n",
    "#     # loop through each participant\n",
    "#     for participant in range(participants):\n",
    "#         # get eeg data for given participant for the given song\n",
    "#         eeg_participant = data[:, :, participant]\n",
    "\n",
    "#         # this chunk just gets the band power for each participant for given song\n",
    "#         feature_list = []\n",
    "#         for channel in range(channels):\n",
    "#             f, psd = welch(eeg_participant[channel, :], fs = fs, nperseg = 2 * int(fs))\n",
    "#             for fmin, fmax in BANDS.values():\n",
    "#                 band_power = calculate_bandpower(f, psd, fmin, fmax)\n",
    "#                 feature_list.append(band_power)\n",
    "\n",
    "#         feature_vec = np.log10(np.array(feature_list) + .000000000000000000001)\n",
    "#         all_X.append(feature_vec)\n",
    "\n",
    "#         # basically liked if score above 6, no like below 6\n",
    "#         all_y.append(1 if song_label[participant] >= 6 else 0)\n",
    "#         participant_ids.append(participant)\n",
    "#         song_ids.append(song_idx)\n",
    "\n",
    "# # Convert to usable format\n",
    "# X_all = np.vstack(all_X)\n",
    "# y_all = np.array(all_y)\n",
    "\n",
    "# participant_ids = np.array(participant_ids)\n",
    "# song_ids = np.array(song_ids)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "28c285c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LeaveOneGroupOut()"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaler = StandardScaler()\n",
    "logo = LeaveOneGroupOut()\n",
    "X_scaled = scaler.fit_transform(X_all)\n",
    "logo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "adb43aa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "logistic_reg_initial = LogisticRegression(max_iter = 2000, random_state = 42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4e56c10",
   "metadata": {},
   "source": [
    "## Metrics for Logistic Reg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "24becbb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metrics from aggregation across folds\n",
      "Acurracy across all folds:  0.51\n",
      "Precision for Class 0:  0.3148015873015873\n",
      "Precision for Class 1:  0.43154761904761907\n",
      "Recall for Class 0:  0.4370238095238094\n",
      "Recall for Class 1:  0.5068055555555556\n",
      "------------------------------------\n",
      "Computing Precision Recall whole data set (not per fold)\n",
      "Precision for class 0:  0.4631578947368421\n",
      "Precision for class 1:  0.5523809523809524\n",
      "Recall for class 0:  0.4835164835164835\n",
      "Recall for class 1:  0.5321100917431193\n"
     ]
    }
   ],
   "source": [
    "accuracies = []\n",
    "precisions = []\n",
    "recalls = []\n",
    "# conf_matrix_folds = []\n",
    "\n",
    "# collecting across all folds bc my prec recall kinda weird with some classes having none per fold\n",
    "all_folds_y_true = []\n",
    "all_folds_y_pred = []\n",
    "\n",
    "for train_idx, test_idx in logo.split(X_scaled, y_all, participant_ids):\n",
    "    X_train, X_test = X_scaled[train_idx], X_scaled[test_idx]\n",
    "    y_train, y_test = y_all[train_idx], y_all[test_idx]\n",
    "\n",
    "    logistic_reg_initial.fit(X_train, y_train)\n",
    "    y_pred = logistic_reg_initial.predict(X_test)\n",
    "    y_prob = logistic_reg_initial.predict_proba(X_test)[:, 1]\n",
    "\n",
    "    accuracies.append(accuracy_score(y_test, y_pred))\n",
    "\n",
    "    all_folds_y_true.extend(y_test)\n",
    "    all_folds_y_pred.extend(y_pred)\n",
    "\n",
    "    # cm = confusion_matrix(y_test, y_pred, labels = [0,1])\n",
    "    # conf_matrix_folds.append(cm)\n",
    "    \n",
    "    # precisions.append(precision_score(y_test, y_pred, pos_label=1, zero_division = 0))\n",
    "    # recalls.append(recall_score(y_test, y_pred, pos_label = 1, zero_division=0))\n",
    "\n",
    "    prec, rec, f1, support = precision_recall_fscore_support(\n",
    "        y_test,\n",
    "        y_pred,\n",
    "        labels = [0,1],\n",
    "        average = None,\n",
    "        zero_division = 0\n",
    "    )\n",
    "\n",
    "    precisions.append(prec)\n",
    "    recalls.append(rec)\n",
    "\n",
    "all_folds_y_true = np.array(all_folds_y_true)\n",
    "all_folds_y_pred = np.array(all_folds_y_pred)\n",
    "\n",
    "prec, rec, f1, support = precision_recall_fscore_support(\n",
    "    all_folds_y_true,\n",
    "    all_folds_y_pred,\n",
    "    labels = [0,1],\n",
    "    average = None,\n",
    "    zero_division = 0\n",
    ")\n",
    "\n",
    "# print(\"Precision across all folds: \", np.mean(precisions))\n",
    "# print(\"Recall across all folds: \", np.mean(recalls))\n",
    "\n",
    "precisions = np.array(precisions)\n",
    "recalls = np.array(recalls)\n",
    "\n",
    "mean_prec_class0, mean_prec_class1 = precisions.mean(axis = 0)\n",
    "mean_recall_class0, mean_recall_class1 = recalls.mean(axis = 0)\n",
    "\n",
    "print(\"Metrics from aggregation across folds\")\n",
    "print(\"Acurracy across all folds: \", np.mean(accuracies))\n",
    "\n",
    "print(\"Precision for Class 0: \", mean_prec_class0)\n",
    "print(\"Precision for Class 1: \", mean_prec_class1)\n",
    "\n",
    "print(\"Recall for Class 0: \", mean_recall_class0)\n",
    "print(\"Recall for Class 1: \", mean_recall_class1)\n",
    "\n",
    "print(\"------------------------------------\")\n",
    "print(\"Computing Precision Recall whole data set (not per fold)\")\n",
    "\n",
    "print(\"Precision for class 0: \", prec[0])\n",
    "print(\"Precision for class 1: \", prec[1])\n",
    "\n",
    "print(\"Recall for class 0: \", rec[0])\n",
    "print(\"Recall for class 1: \", rec[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a7215c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# did not tune, could be future works (kinda mannually tuned but not really efficient)\n",
    "rf = RandomForestClassifier(\n",
    "    n_estimators=300,\n",
    "    max_depth=10,\n",
    "    min_samples_leaf=10,\n",
    "    max_features=0.2,\n",
    "    class_weight=\"balanced\",\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7e5f8c42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metrics from aggregation across folds\n",
      "Acurracy across all folds:  0.5650000000000001\n",
      "Precision for Class 0:  0.265515873015873\n",
      "Precision for Class 1:  0.5849801587301587\n",
      "Recall for Class 0:  0.3270833333333333\n",
      "Recall for Class 1:  0.6279761904761905\n",
      "------------------------------------\n",
      "Computing Precision Recall whole data set (not per fold)\n",
      "Precision for class 0:  0.5294117647058824\n",
      "Precision for class 1:  0.5833333333333334\n",
      "Recall for class 0:  0.3956043956043956\n",
      "Recall for class 1:  0.7064220183486238\n"
     ]
    }
   ],
   "source": [
    "accuracies = []\n",
    "precisions = []\n",
    "recalls = []\n",
    "# conf_matrix_folds = []\n",
    "\n",
    "# collecting across all folds bc my prec recall kinda weird with some classes having none per fold\n",
    "all_folds_y_true = []\n",
    "all_folds_y_pred = []\n",
    "\n",
    "for train_idx, test_idx in logo.split(X_scaled, y_all, participant_ids):\n",
    "    X_train, X_test = X_scaled[train_idx], X_scaled[test_idx]\n",
    "    y_train, y_test = y_all[train_idx], y_all[test_idx]\n",
    "\n",
    "    rf.fit(X_train, y_train)\n",
    "    y_pred = rf.predict(X_test)\n",
    "    y_prob = rf.predict_proba(X_test)[:, 1]\n",
    "\n",
    "    accuracies.append(accuracy_score(y_test, y_pred))\n",
    "\n",
    "    all_folds_y_true.extend(y_test)\n",
    "    all_folds_y_pred.extend(y_pred)\n",
    "\n",
    "    # cm = confusion_matrix(y_test, y_pred, labels = [0,1])\n",
    "    # conf_matrix_folds.append(cm)\n",
    "    \n",
    "    # precisions.append(precision_score(y_test, y_pred, pos_label=1, zero_division = 0))\n",
    "    # recalls.append(recall_score(y_test, y_pred, pos_label = 1, zero_division=0))\n",
    "\n",
    "    prec, rec, f1, support = precision_recall_fscore_support(\n",
    "        y_test,\n",
    "        y_pred,\n",
    "        labels = [0,1],\n",
    "        average = None,\n",
    "        zero_division = 0\n",
    "    )\n",
    "\n",
    "    precisions.append(prec)\n",
    "    recalls.append(rec)\n",
    "\n",
    "all_folds_y_true = np.array(all_folds_y_true)\n",
    "all_folds_y_pred = np.array(all_folds_y_pred)\n",
    "\n",
    "prec, rec, f1, support = precision_recall_fscore_support(\n",
    "    all_folds_y_true,\n",
    "    all_folds_y_pred,\n",
    "    labels = [0,1],\n",
    "    average = None,\n",
    "    zero_division = 0\n",
    ")\n",
    "\n",
    "# print(\"Precision across all folds: \", np.mean(precisions))\n",
    "# print(\"Recall across all folds: \", np.mean(recalls))\n",
    "\n",
    "precisions = np.array(precisions)\n",
    "recalls = np.array(recalls)\n",
    "\n",
    "mean_prec_class0, mean_prec_class1 = precisions.mean(axis = 0)\n",
    "mean_recall_class0, mean_recall_class1 = recalls.mean(axis = 0)\n",
    "\n",
    "print(\"Metrics from aggregation across folds\")\n",
    "print(\"Acurracy across all folds: \", np.mean(accuracies))\n",
    "\n",
    "print(\"Precision for Class 0: \", mean_prec_class0)\n",
    "print(\"Precision for Class 1: \", mean_prec_class1)\n",
    "\n",
    "print(\"Recall for Class 0: \", mean_recall_class0)\n",
    "print(\"Recall for Class 1: \", mean_recall_class1)\n",
    "\n",
    "print(\"------------------------------------\")\n",
    "print(\"Computing Precision Recall whole data set (not per fold)\")\n",
    "\n",
    "print(\"Precision for class 0: \", prec[0])\n",
    "print(\"Precision for class 1: \", prec[1])\n",
    "\n",
    "print(\"Recall for class 0: \", rec[0])\n",
    "print(\"Recall for class 1: \", rec[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4f29bb70",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "conf_matrix = confusion_matrix(all_folds_y_true, all_folds_y_pred, labels = [0, 1])\n",
    "\n",
    "disp = ConfusionMatrixDisplay(\n",
    "    confusion_matrix = conf_matrix,\n",
    "    display_labels = [\"Not liked\", \"Liked\"]\n",
    ")\n",
    "\n",
    "disp.plot(cmap=\"Blues\", values_format = \".0f\")\n",
    "plt.title(\"Confusion Matrix (Version - Pooled Across Participants)\")\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"Ground Truth\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1e6b8b9",
   "metadata": {},
   "source": [
    "## More Power"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57ae1eb1",
   "metadata": {},
   "source": [
    "## Metrics for Linear SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8ac48ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# didnt tune :(\n",
    "svm_linear = SVC(\n",
    "    kernel = \"linear\",\n",
    "    C = 1,\n",
    "    probability = True,\n",
    "    class_weight = \"balanced\",\n",
    "    random_state = 42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "15ebf02f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metrics from aggregation across folds\n",
      "Acurracy across all folds:  0.5349999999999999\n",
      "Precision for Class 0:  0.3489682539682539\n",
      "Precision for Class 1:  0.4416666666666667\n",
      "Recall for Class 0:  0.4245238095238095\n",
      "Recall for Class 1:  0.5608333333333333\n",
      "------------------------------------\n",
      "Computing Precision Recall whole data set (not per fold)\n",
      "Precision for class 0:  0.48863636363636365\n",
      "Precision for class 1:  0.5714285714285714\n",
      "Recall for class 0:  0.4725274725274725\n",
      "Recall for class 1:  0.5871559633027523\n"
     ]
    }
   ],
   "source": [
    "accuracies = []\n",
    "precisions = []\n",
    "recalls = []\n",
    "\n",
    "# collecting across all folds bc my prec recall kinda weird with some classes having none per fold\n",
    "all_folds_y_true = []\n",
    "all_folds_y_pred = []\n",
    "\n",
    "for train_idx, test_idx in logo.split(X_scaled, y_all, participant_ids):\n",
    "    X_train, X_test = X_scaled[train_idx], X_scaled[test_idx]\n",
    "    y_train, y_test = y_all[train_idx], y_all[test_idx]\n",
    "\n",
    "    svm_linear.fit(X_train, y_train)\n",
    "    y_pred = svm_linear.predict(X_test)\n",
    "    y_prob = svm_linear.predict_proba(X_test)[:, 1]\n",
    "\n",
    "    accuracies.append(accuracy_score(y_test, y_pred))\n",
    "\n",
    "    all_folds_y_true.extend(y_test)\n",
    "    all_folds_y_pred.extend(y_pred)\n",
    "\n",
    "    \n",
    "    # precisions.append(precision_score(y_test, y_pred, pos_label=1, zero_division = 0))\n",
    "    # recalls.append(recall_score(y_test, y_pred, pos_label = 1, zero_division=0))\n",
    "\n",
    "    prec, rec, f1, support = precision_recall_fscore_support(\n",
    "        y_test,\n",
    "        y_pred,\n",
    "        labels = [0,1],\n",
    "        average = None,\n",
    "        zero_division = 0\n",
    "    )\n",
    "\n",
    "    precisions.append(prec)\n",
    "    recalls.append(rec)\n",
    "\n",
    "all_folds_y_true = np.array(all_folds_y_true)\n",
    "all_folds_y_pred = np.array(all_folds_y_pred)\n",
    "\n",
    "prec, rec, f1, support = precision_recall_fscore_support(\n",
    "    all_folds_y_true,\n",
    "    all_folds_y_pred,\n",
    "    labels = [0,1],\n",
    "    average = None,\n",
    "    zero_division = 0\n",
    ")\n",
    "\n",
    "# print(\"Precision across all folds: \", np.mean(precisions))\n",
    "# print(\"Recall across all folds: \", np.mean(recalls))\n",
    "\n",
    "precisions = np.array(precisions)\n",
    "recalls = np.array(recalls)\n",
    "\n",
    "mean_prec_class0, mean_prec_class1 = precisions.mean(axis = 0)\n",
    "mean_recall_class0, mean_recall_class1 = recalls.mean(axis = 0)\n",
    "\n",
    "print(\"Metrics from aggregation across folds\")\n",
    "print(\"Acurracy across all folds: \", np.mean(accuracies))\n",
    "\n",
    "print(\"Precision for Class 0: \", mean_prec_class0)\n",
    "print(\"Precision for Class 1: \", mean_prec_class1)\n",
    "\n",
    "print(\"Recall for Class 0: \", mean_recall_class0)\n",
    "print(\"Recall for Class 1: \", mean_recall_class1)\n",
    "\n",
    "print(\"------------------------------------\")\n",
    "print(\"Computing Precision Recall whole data set (not per fold)\")\n",
    "\n",
    "print(\"Precision for class 0: \", prec[0])\n",
    "print(\"Precision for class 1: \", prec[1])\n",
    "\n",
    "print(\"Recall for class 0: \", rec[0])\n",
    "print(\"Recall for class 1: \", rec[1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
