{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5d7d3763",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import scipy.io\n",
    "import h5py\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.io import loadmat\n",
    "from scipy.signal import welch\n",
    "import seaborn as sns\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import LeaveOneGroupOut\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, precision_score, recall_score, precision_recall_fscore_support, confusion_matrix, ConfusionMatrixDisplay\n",
    "from sklearn.svm import SVC\n",
    "from scipy.signal import decimate\n",
    "from skimage.transform import resize\n",
    "import pywt\n",
    "from scipy.io import loadmat\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import random\n",
    "from torch.utils.data import TensorDataset, DataLoader\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d24cbe25",
   "metadata": {},
   "source": [
    "# Creating the Like DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c6d40cd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HDF5 keys: ['behavioralRatings']\n"
     ]
    }
   ],
   "source": [
    "path = \"../data/behavioralRatings.mat\"\n",
    "with h5py.File(path, \"r\") as f:\n",
    "    print(\"HDF5 keys:\", list(f.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "db10a8bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type: <class 'h5py._hl.dataset.Dataset'>\n",
      "Shape: (2, 10, 20)\n",
      "Dtype: float64\n"
     ]
    }
   ],
   "source": [
    "with h5py.File(path, \"r\") as f:\n",
    "    br = f[\"behavioralRatings\"]\n",
    "    print(\"Type:\", type(br))\n",
    "    print(\"Shape:\", br.shape)\n",
    "    print(\"Dtype:\", br.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "740f42ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "with h5py.File(path, \"r\") as f:\n",
    "    br = f[\"behavioralRatings\"][:]   # turn into NumPy array, shape (2, 10, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d7bb3b9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20, 10, 2)\n"
     ]
    }
   ],
   "source": [
    "# (2, 10, 20) to (20, 10, 2) in order to turn into same structure as matlab \n",
    "ratings = np.transpose(br, (2, 1, 0))\n",
    "\n",
    "print(ratings.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9e46e242",
   "metadata": {},
   "outputs": [],
   "source": [
    "enjoyment = ratings[:, :, 1] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a68f3616",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subject</th>\n",
       "      <th>song_1</th>\n",
       "      <th>song_2</th>\n",
       "      <th>song_3</th>\n",
       "      <th>song_4</th>\n",
       "      <th>song_5</th>\n",
       "      <th>song_6</th>\n",
       "      <th>song_7</th>\n",
       "      <th>song_8</th>\n",
       "      <th>song_9</th>\n",
       "      <th>song_10</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>8.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>8.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>8.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>8.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>7.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>6.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>6.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>5.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>7.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>11</td>\n",
       "      <td>7.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>12</td>\n",
       "      <td>9.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>13</td>\n",
       "      <td>6.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>14</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>15</td>\n",
       "      <td>7.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>16</td>\n",
       "      <td>6.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>17</td>\n",
       "      <td>5.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>18</td>\n",
       "      <td>9.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>19</td>\n",
       "      <td>7.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>20</td>\n",
       "      <td>8.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    subject  song_1  song_2  song_3  song_4  song_5  song_6  song_7  song_8  \\\n",
       "0         1     8.0     8.0     5.0     5.0     9.0     7.0     6.0     7.0   \n",
       "1         2     8.0     8.0     7.0     5.0     3.0     7.0     5.0     5.0   \n",
       "2         3     8.0     7.0     8.0     6.0     8.0     7.0     7.0     8.0   \n",
       "3         4     8.0     7.0     2.0     7.0     9.0     6.0     7.0     7.0   \n",
       "4         5     6.0     8.0     6.0     5.0     7.0     7.0     8.0     7.0   \n",
       "5         6     4.0     5.0     4.0     5.0     5.0     5.0     4.0     4.0   \n",
       "6         7     6.0     2.0     2.0     5.0     3.0     7.0     3.0     5.0   \n",
       "7         8     5.0     8.0     3.0     7.0     5.0     6.0     6.0     6.0   \n",
       "8         9     7.0     5.0     6.0     5.0     6.0     5.0     4.0     6.0   \n",
       "9        10     5.0     4.0     2.0     7.0     5.0     6.0     4.0     7.0   \n",
       "10       11     7.0     5.0     2.0     4.0     7.0     9.0     7.0     8.0   \n",
       "11       12     9.0     6.0     6.0     8.0     6.0     7.0     6.0     8.0   \n",
       "12       13     6.0     7.0     5.0     9.0     7.0     7.0     6.0     8.0   \n",
       "13       14     3.0     5.0     5.0     6.0     5.0     6.0     4.0     6.0   \n",
       "14       15     7.0     4.0     4.0     3.0     5.0     5.0     3.0     6.0   \n",
       "15       16     6.0     5.0     4.0     6.0     5.0     5.0     7.0     6.0   \n",
       "16       17     5.0     6.0     4.0     5.0     7.0     7.0     4.0     6.0   \n",
       "17       18     9.0     7.0     8.0     9.0     9.0     9.0     9.0     9.0   \n",
       "18       19     7.0     6.0     6.0     8.0     5.0     8.0     5.0     7.0   \n",
       "19       20     8.0     8.0     2.0     6.0     8.0     9.0     4.0     6.0   \n",
       "\n",
       "    song_9  song_10  \n",
       "0      5.0      5.0  \n",
       "1      4.0      4.0  \n",
       "2      7.0      6.0  \n",
       "3      5.0      7.0  \n",
       "4      7.0      3.0  \n",
       "5      5.0      1.0  \n",
       "6      3.0      2.0  \n",
       "7      7.0      4.0  \n",
       "8      3.0      2.0  \n",
       "9      3.0      1.0  \n",
       "10     6.0      1.0  \n",
       "11     5.0      6.0  \n",
       "12     3.0      4.0  \n",
       "13     3.0      2.0  \n",
       "14     3.0      2.0  \n",
       "15     5.0      2.0  \n",
       "16     5.0      1.0  \n",
       "17     9.0      3.0  \n",
       "18     8.0      4.0  \n",
       "19     8.0      1.0  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "participants = np.arange(1, 21)\n",
    "songs = np.arange(1, 11)\n",
    "\n",
    "like_df = pd.DataFrame(enjoyment,\n",
    "                      index=participants,\n",
    "                      columns=[f\"song_{s}\" for s in songs]).rename_axis(\"subject\").reset_index()\n",
    "\n",
    "like_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81aad60a",
   "metadata": {},
   "source": [
    "# Feature Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "606b22b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basically mapping freq to names/categories\n",
    "BANDS = {\n",
    "    \"delta\": (1, 4),\n",
    "    \"theta\": (4, 8),\n",
    "    \"alpha\": (8, 12),\n",
    "    \"beta\": (12, 30),\n",
    "    \"gamma_low\": (30, 45),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "755b0198",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_bandpower(f, psd, fmin, fmax):\n",
    "    idx = (f >= fmin) & (f <= fmax)\n",
    "    return np.trapezoid(psd[idx], f[idx])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c6fd705",
   "metadata": {},
   "source": [
    "# Loading in Cleaned EEG data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b2f5172e",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f228f882",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 30\u001b[39m\n\u001b[32m     28\u001b[39m feature_list = []\n\u001b[32m     29\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m channel \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(channels):\n\u001b[32m---> \u001b[39m\u001b[32m30\u001b[39m     f, psd = \u001b[43mwelch\u001b[49m\u001b[43m(\u001b[49m\u001b[43meeg_participant\u001b[49m\u001b[43m[\u001b[49m\u001b[43mchannel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfs\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mfs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnperseg\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mfs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     31\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m fmin, fmax \u001b[38;5;129;01min\u001b[39;00m BANDS.values():\n\u001b[32m     32\u001b[39m         band_power = calculate_bandpower(f, psd, fmin, fmax)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/EEG-Music-Prediction/myenv/lib/python3.12/site-packages/scipy/signal/_spectral_py.py:664\u001b[39m, in \u001b[36mwelch\u001b[39m\u001b[34m(x, fs, window, nperseg, noverlap, nfft, detrend, return_onesided, scaling, axis, average)\u001b[39m\n\u001b[32m    497\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mwelch\u001b[39m(x, fs=\u001b[32m1.0\u001b[39m, window=\u001b[33m'\u001b[39m\u001b[33mhann\u001b[39m\u001b[33m'\u001b[39m, nperseg=\u001b[38;5;28;01mNone\u001b[39;00m, noverlap=\u001b[38;5;28;01mNone\u001b[39;00m, nfft=\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m    498\u001b[39m           detrend=\u001b[33m'\u001b[39m\u001b[33mconstant\u001b[39m\u001b[33m'\u001b[39m, return_onesided=\u001b[38;5;28;01mTrue\u001b[39;00m, scaling=\u001b[33m'\u001b[39m\u001b[33mdensity\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m    499\u001b[39m           axis=-\u001b[32m1\u001b[39m, average=\u001b[33m'\u001b[39m\u001b[33mmean\u001b[39m\u001b[33m'\u001b[39m):\n\u001b[32m    500\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33mr\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    501\u001b[39m \u001b[33;03m    Estimate power spectral density using Welch's method.\u001b[39;00m\n\u001b[32m    502\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m    662\u001b[39m \n\u001b[32m    663\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m664\u001b[39m     freqs, Pxx = \u001b[43mcsd\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwindow\u001b[49m\u001b[43m=\u001b[49m\u001b[43mwindow\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnperseg\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnperseg\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    665\u001b[39m \u001b[43m                     \u001b[49m\u001b[43mnoverlap\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnoverlap\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnfft\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnfft\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdetrend\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdetrend\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    666\u001b[39m \u001b[43m                     \u001b[49m\u001b[43mreturn_onesided\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_onesided\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscaling\u001b[49m\u001b[43m=\u001b[49m\u001b[43mscaling\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    667\u001b[39m \u001b[43m                     \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m=\u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maverage\u001b[49m\u001b[43m=\u001b[49m\u001b[43maverage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    669\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m freqs, Pxx.real\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/EEG-Music-Prediction/myenv/lib/python3.12/site-packages/scipy/signal/_spectral_py.py:917\u001b[39m, in \u001b[36mcsd\u001b[39m\u001b[34m(x, y, fs, window, nperseg, noverlap, nfft, detrend, return_onesided, scaling, axis, average)\u001b[39m\n\u001b[32m    914\u001b[39m SFT = ShortTimeFFT(win, nperseg - noverlap, fs, fft_mode=fft_mode, mfft=nfft,\n\u001b[32m    915\u001b[39m                    scale_to=scales[scaling], phase_shift=\u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m    916\u001b[39m \u001b[38;5;66;03m# csd() calculates X.conj()*Y instead of X*Y.conj():\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m917\u001b[39m Pxy = \u001b[43mSFT\u001b[49m\u001b[43m.\u001b[49m\u001b[43mspectrogram\u001b[49m\u001b[43m(\u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdetr\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdetrend\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdetrend\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    918\u001b[39m \u001b[43m                      \u001b[49m\u001b[43mp0\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mp1\u001b[49m\u001b[43m=\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn\u001b[49m\u001b[43m \u001b[49m\u001b[43m-\u001b[49m\u001b[43m \u001b[49m\u001b[43mnoverlap\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m/\u001b[49m\u001b[43m/\u001b[49m\u001b[43m \u001b[49m\u001b[43mSFT\u001b[49m\u001b[43m.\u001b[49m\u001b[43mhop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk_offset\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnperseg\u001b[49m\u001b[43m \u001b[49m\u001b[43m/\u001b[49m\u001b[43m/\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    919\u001b[39m \u001b[43m                      \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m=\u001b[49m\u001b[43maxis\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    921\u001b[39m \u001b[38;5;66;03m# Note:\u001b[39;00m\n\u001b[32m    922\u001b[39m \u001b[38;5;66;03m# 'onesided2X' scaling of ShortTimeFFT conflicts with the\u001b[39;00m\n\u001b[32m    923\u001b[39m \u001b[38;5;66;03m# scaling='spectrum' parameter, since it doubles the squared magnitude,\u001b[39;00m\n\u001b[32m    924\u001b[39m \u001b[38;5;66;03m# which in the view of the ShortTimeFFT implementation does not make sense.\u001b[39;00m\n\u001b[32m    925\u001b[39m \u001b[38;5;66;03m# Hence, the doubling of the square is implemented here:\u001b[39;00m\n\u001b[32m    926\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m return_onesided:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/EEG-Music-Prediction/myenv/lib/python3.12/site-packages/scipy/signal/_short_time_fft.py:1386\u001b[39m, in \u001b[36mShortTimeFFT.spectrogram\u001b[39m\u001b[34m(self, x, y, detr, p0, p1, k_offset, padding, axis)\u001b[39m\n\u001b[32m   1250\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mspectrogram\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: np.ndarray, y: np.ndarray | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   1251\u001b[39m                 detr: Callable[[np.ndarray], np.ndarray] | Literal[\u001b[33m'\u001b[39m\u001b[33mlinear\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mconstant\u001b[39m\u001b[33m'\u001b[39m] | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m,  \u001b[38;5;66;03m# noqa: E501\u001b[39;00m\n\u001b[32m   1252\u001b[39m                 *,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1255\u001b[39m                 axis: \u001b[38;5;28mint\u001b[39m = -\u001b[32m1\u001b[39m) \\\n\u001b[32m   1256\u001b[39m         -> np.ndarray:\n\u001b[32m   1257\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33mr\u001b[39m\u001b[33;03m\"\"\"Calculate spectrogram or cross-spectrogram.\u001b[39;00m\n\u001b[32m   1258\u001b[39m \n\u001b[32m   1259\u001b[39m \u001b[33;03m    The spectrogram is the absolute square of the STFT, i.e., it is\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   1384\u001b[39m \u001b[33;03m    :class:`scipy.signal.ShortTimeFFT`: Class this method belongs to.\u001b[39;00m\n\u001b[32m   1385\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1386\u001b[39m     Sx = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mstft_detrend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdetr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mp0\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mp1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk_offset\u001b[49m\u001b[43m=\u001b[49m\u001b[43mk_offset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1387\u001b[39m \u001b[43m                           \u001b[49m\u001b[43mpadding\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m=\u001b[49m\u001b[43maxis\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1388\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m y \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m y \u001b[38;5;129;01mis\u001b[39;00m x:  \u001b[38;5;66;03m# do spectrogram:\u001b[39;00m\n\u001b[32m   1389\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m Sx.real**\u001b[32m2\u001b[39m + Sx.imag**\u001b[32m2\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/EEG-Music-Prediction/myenv/lib/python3.12/site-packages/scipy/signal/_short_time_fft.py:1244\u001b[39m, in \u001b[36mShortTimeFFT.stft_detrend\u001b[39m\u001b[34m(self, x, detr, p0, p1, k_offset, padding, axis)\u001b[39m\n\u001b[32m   1242\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m p_, x_ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mself\u001b[39m._x_slices(x, k_offset, p0, p1, padding)):\n\u001b[32m   1243\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m detr \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1244\u001b[39m         x_ = \u001b[43mdetr\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1245\u001b[39m     S[..., :, p_] = \u001b[38;5;28mself\u001b[39m._fft_func(x_ * \u001b[38;5;28mself\u001b[39m.win.conj())\n\u001b[32m   1246\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m x.ndim > \u001b[32m1\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/EEG-Music-Prediction/myenv/lib/python3.12/site-packages/scipy/signal/_signaltools.py:4215\u001b[39m, in \u001b[36mdetrend\u001b[39m\u001b[34m(data, axis, type, bp, overwrite_data)\u001b[39m\n\u001b[32m   4213\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m \u001b[38;5;129;01min\u001b[39;00m [\u001b[33m'\u001b[39m\u001b[33mconstant\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mc\u001b[39m\u001b[33m'\u001b[39m]:\n\u001b[32m   4214\u001b[39m     ret = data - np.mean(data, axis, keepdims=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m-> \u001b[39m\u001b[32m4215\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mxp\u001b[49m\u001b[43m.\u001b[49m\u001b[43masarray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mret\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4216\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   4217\u001b[39m     dshape = data.shape\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/EEG-Music-Prediction/myenv/lib/python3.12/site-packages/scipy/_lib/array_api_compat/numpy/_aliases.py:104\u001b[39m, in \u001b[36masarray\u001b[39m\u001b[34m(obj, dtype, device, copy, **kwargs)\u001b[39m\n\u001b[32m     89\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34masarray\u001b[39m(\n\u001b[32m     90\u001b[39m     obj: Array | \u001b[38;5;28mcomplex\u001b[39m | NestedSequence[\u001b[38;5;28mcomplex\u001b[39m] | SupportsBufferProtocol,\n\u001b[32m     91\u001b[39m     /,\n\u001b[32m   (...)\u001b[39m\u001b[32m     96\u001b[39m     **kwargs: Any,\n\u001b[32m     97\u001b[39m ) -> Array:\n\u001b[32m     98\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     99\u001b[39m \u001b[33;03m    Array API compatibility wrapper for asarray().\u001b[39;00m\n\u001b[32m    100\u001b[39m \n\u001b[32m    101\u001b[39m \u001b[33;03m    See the corresponding documentation in the array library and/or the array API\u001b[39;00m\n\u001b[32m    102\u001b[39m \u001b[33;03m    specification for more details.\u001b[39;00m\n\u001b[32m    103\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m104\u001b[39m     \u001b[43m_helpers\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_check_device\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    106\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m copy \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    107\u001b[39m         copy = np._CopyMode.IF_NEEDED\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/EEG-Music-Prediction/myenv/lib/python3.12/site-packages/scipy/_lib/array_api_compat/common/_helpers.py:673\u001b[39m, in \u001b[36m_check_device\u001b[39m\u001b[34m(bare_xp, device)\u001b[39m\n\u001b[32m    660\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_check_device\u001b[39m(bare_xp: Namespace, device: Device) -> \u001b[38;5;28;01mNone\u001b[39;00m:  \u001b[38;5;66;03m# pyright: ignore[reportUnusedFunction]\u001b[39;00m\n\u001b[32m    661\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    662\u001b[39m \u001b[33;03m    Validate dummy device on device-less array backends.\u001b[39;00m\n\u001b[32m    663\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m    671\u001b[39m \u001b[33;03m    https://github.com/data-apis/array-api-compat/pull/293\u001b[39;00m\n\u001b[32m    672\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m673\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m bare_xp \u001b[38;5;129;01mis\u001b[39;00m \u001b[43msys\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmodules\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mnumpy\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m:\n\u001b[32m    674\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m device \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m (\u001b[33m\"\u001b[39m\u001b[33mcpu\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[32m    675\u001b[39m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mUnsupported device for NumPy: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdevice\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "all_X = []\n",
    "all_y = []\n",
    "participant_ids = []\n",
    "song_ids = []\n",
    "\n",
    "# they had weird mappings in the study, these numbers correspond to file names\n",
    "# looping through each song\n",
    "for song in range(21, 31):\n",
    "    # loads in the file\n",
    "    load_mat_file = loadmat(f\"../data/cleaned_eeg/song{song}_Imputed.mat\")\n",
    "    # gets the current song\n",
    "    data = load_mat_file[f\"data{song}\"]\n",
    "\n",
    "    fs = float(load_mat_file[\"fs\"].squeeze())\n",
    "\n",
    "    channels, time, participants = data.shape\n",
    "\n",
    "    # converting werid mappings to start at 1\n",
    "    song_idx = song - 20\n",
    "    song_label = like_df[f\"song_{song_idx}\"].to_numpy()\n",
    "\n",
    "    # loop through each participant\n",
    "    for participant in range(participants):\n",
    "        # get eeg data for given participant for the given song\n",
    "        eeg_participant = data[:, :, participant]\n",
    "\n",
    "        # this chunk just gets the band power for each participant for given song\n",
    "        feature_list = []\n",
    "        for channel in range(channels):\n",
    "            f, psd = welch(eeg_participant[channel, :], fs = fs, nperseg = 2 * int(fs))\n",
    "            for fmin, fmax in BANDS.values():\n",
    "                band_power = calculate_bandpower(f, psd, fmin, fmax)\n",
    "                feature_list.append(band_power)\n",
    "\n",
    "        feature_vec = np.log10(np.array(feature_list) + .000000000000000000001)\n",
    "        all_X.append(feature_vec)\n",
    "\n",
    "        # basically liked if score above 6, no like below 6\n",
    "        all_y.append(1 if song_label[participant] >= 6 else 0)\n",
    "        participant_ids.append(participant)\n",
    "        song_ids.append(song_idx)\n",
    "\n",
    "# Convert to usable format\n",
    "X_all = np.vstack(all_X)\n",
    "y_all = np.array(all_y)\n",
    "\n",
    "participant_ids = np.array(participant_ids)\n",
    "song_ids = np.array(song_ids)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6c34c05",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>615</th>\n",
       "      <th>616</th>\n",
       "      <th>617</th>\n",
       "      <th>618</th>\n",
       "      <th>619</th>\n",
       "      <th>620</th>\n",
       "      <th>621</th>\n",
       "      <th>622</th>\n",
       "      <th>623</th>\n",
       "      <th>624</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.735372</td>\n",
       "      <td>0.517957</td>\n",
       "      <td>0.360125</td>\n",
       "      <td>0.591212</td>\n",
       "      <td>0.054921</td>\n",
       "      <td>0.675716</td>\n",
       "      <td>0.469317</td>\n",
       "      <td>0.341129</td>\n",
       "      <td>0.873114</td>\n",
       "      <td>0.770013</td>\n",
       "      <td>...</td>\n",
       "      <td>0.533776</td>\n",
       "      <td>0.590708</td>\n",
       "      <td>0.351696</td>\n",
       "      <td>0.670366</td>\n",
       "      <td>-0.092422</td>\n",
       "      <td>0.621949</td>\n",
       "      <td>0.751444</td>\n",
       "      <td>0.606980</td>\n",
       "      <td>0.855188</td>\n",
       "      <td>-0.121695</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.832643</td>\n",
       "      <td>0.344199</td>\n",
       "      <td>0.790840</td>\n",
       "      <td>0.473331</td>\n",
       "      <td>-0.143020</td>\n",
       "      <td>0.902948</td>\n",
       "      <td>0.359715</td>\n",
       "      <td>0.869040</td>\n",
       "      <td>0.561634</td>\n",
       "      <td>-0.022386</td>\n",
       "      <td>...</td>\n",
       "      <td>0.833698</td>\n",
       "      <td>0.342209</td>\n",
       "      <td>0.798735</td>\n",
       "      <td>0.474534</td>\n",
       "      <td>-0.358329</td>\n",
       "      <td>0.764603</td>\n",
       "      <td>0.306948</td>\n",
       "      <td>0.732067</td>\n",
       "      <td>0.456729</td>\n",
       "      <td>-0.532869</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.892297</td>\n",
       "      <td>0.557208</td>\n",
       "      <td>0.713414</td>\n",
       "      <td>0.376822</td>\n",
       "      <td>-0.115197</td>\n",
       "      <td>1.509475</td>\n",
       "      <td>0.808840</td>\n",
       "      <td>0.828644</td>\n",
       "      <td>0.805651</td>\n",
       "      <td>0.519312</td>\n",
       "      <td>...</td>\n",
       "      <td>0.756495</td>\n",
       "      <td>0.444941</td>\n",
       "      <td>0.606594</td>\n",
       "      <td>0.282850</td>\n",
       "      <td>-0.382810</td>\n",
       "      <td>0.581887</td>\n",
       "      <td>0.701730</td>\n",
       "      <td>0.814113</td>\n",
       "      <td>0.309131</td>\n",
       "      <td>-0.595847</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.111488</td>\n",
       "      <td>0.715326</td>\n",
       "      <td>0.444180</td>\n",
       "      <td>0.517769</td>\n",
       "      <td>0.115481</td>\n",
       "      <td>1.115695</td>\n",
       "      <td>0.938606</td>\n",
       "      <td>0.541302</td>\n",
       "      <td>0.665672</td>\n",
       "      <td>0.223781</td>\n",
       "      <td>...</td>\n",
       "      <td>0.917731</td>\n",
       "      <td>0.762848</td>\n",
       "      <td>0.297982</td>\n",
       "      <td>0.375505</td>\n",
       "      <td>-0.152520</td>\n",
       "      <td>0.981974</td>\n",
       "      <td>0.834027</td>\n",
       "      <td>0.653438</td>\n",
       "      <td>0.462594</td>\n",
       "      <td>-0.248389</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.067522</td>\n",
       "      <td>0.624089</td>\n",
       "      <td>0.637543</td>\n",
       "      <td>0.703967</td>\n",
       "      <td>0.176480</td>\n",
       "      <td>1.415276</td>\n",
       "      <td>0.881465</td>\n",
       "      <td>0.884471</td>\n",
       "      <td>0.988769</td>\n",
       "      <td>0.549320</td>\n",
       "      <td>...</td>\n",
       "      <td>1.078836</td>\n",
       "      <td>0.685927</td>\n",
       "      <td>0.544265</td>\n",
       "      <td>0.818167</td>\n",
       "      <td>0.302669</td>\n",
       "      <td>0.959196</td>\n",
       "      <td>0.666225</td>\n",
       "      <td>0.569900</td>\n",
       "      <td>0.610659</td>\n",
       "      <td>-0.207420</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>195</th>\n",
       "      <td>0.961990</td>\n",
       "      <td>0.511375</td>\n",
       "      <td>0.068491</td>\n",
       "      <td>0.297462</td>\n",
       "      <td>-0.104591</td>\n",
       "      <td>0.810637</td>\n",
       "      <td>0.372254</td>\n",
       "      <td>-0.041543</td>\n",
       "      <td>0.127531</td>\n",
       "      <td>-0.254268</td>\n",
       "      <td>...</td>\n",
       "      <td>0.722118</td>\n",
       "      <td>0.373444</td>\n",
       "      <td>-0.052891</td>\n",
       "      <td>0.131516</td>\n",
       "      <td>-0.337811</td>\n",
       "      <td>0.684117</td>\n",
       "      <td>0.551660</td>\n",
       "      <td>0.033654</td>\n",
       "      <td>0.125793</td>\n",
       "      <td>-0.488601</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196</th>\n",
       "      <td>1.055436</td>\n",
       "      <td>0.749483</td>\n",
       "      <td>0.975975</td>\n",
       "      <td>0.437121</td>\n",
       "      <td>-0.041992</td>\n",
       "      <td>0.936351</td>\n",
       "      <td>0.777895</td>\n",
       "      <td>1.091959</td>\n",
       "      <td>0.559783</td>\n",
       "      <td>0.052457</td>\n",
       "      <td>...</td>\n",
       "      <td>0.798728</td>\n",
       "      <td>0.835067</td>\n",
       "      <td>1.071288</td>\n",
       "      <td>0.579723</td>\n",
       "      <td>-0.062425</td>\n",
       "      <td>0.890615</td>\n",
       "      <td>1.048405</td>\n",
       "      <td>1.262889</td>\n",
       "      <td>0.672654</td>\n",
       "      <td>-0.202523</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>197</th>\n",
       "      <td>1.110303</td>\n",
       "      <td>0.601051</td>\n",
       "      <td>0.342901</td>\n",
       "      <td>0.500978</td>\n",
       "      <td>0.296940</td>\n",
       "      <td>0.942765</td>\n",
       "      <td>0.436964</td>\n",
       "      <td>0.244673</td>\n",
       "      <td>0.824408</td>\n",
       "      <td>0.611259</td>\n",
       "      <td>...</td>\n",
       "      <td>0.642207</td>\n",
       "      <td>0.274625</td>\n",
       "      <td>0.051561</td>\n",
       "      <td>0.318407</td>\n",
       "      <td>0.073765</td>\n",
       "      <td>0.691856</td>\n",
       "      <td>0.321364</td>\n",
       "      <td>0.111347</td>\n",
       "      <td>0.213866</td>\n",
       "      <td>-0.034468</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>198</th>\n",
       "      <td>1.091442</td>\n",
       "      <td>0.828457</td>\n",
       "      <td>0.891922</td>\n",
       "      <td>0.848703</td>\n",
       "      <td>0.423952</td>\n",
       "      <td>1.111520</td>\n",
       "      <td>0.771587</td>\n",
       "      <td>0.886824</td>\n",
       "      <td>0.882306</td>\n",
       "      <td>0.221922</td>\n",
       "      <td>...</td>\n",
       "      <td>0.641523</td>\n",
       "      <td>0.536660</td>\n",
       "      <td>0.789501</td>\n",
       "      <td>0.678763</td>\n",
       "      <td>-0.214273</td>\n",
       "      <td>0.651204</td>\n",
       "      <td>0.613554</td>\n",
       "      <td>0.759610</td>\n",
       "      <td>0.493334</td>\n",
       "      <td>-0.325397</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199</th>\n",
       "      <td>0.825969</td>\n",
       "      <td>0.299535</td>\n",
       "      <td>0.037611</td>\n",
       "      <td>0.547068</td>\n",
       "      <td>0.294606</td>\n",
       "      <td>0.739853</td>\n",
       "      <td>0.210879</td>\n",
       "      <td>-0.039012</td>\n",
       "      <td>0.473949</td>\n",
       "      <td>0.218795</td>\n",
       "      <td>...</td>\n",
       "      <td>0.625190</td>\n",
       "      <td>0.034581</td>\n",
       "      <td>-0.185647</td>\n",
       "      <td>0.266156</td>\n",
       "      <td>0.016930</td>\n",
       "      <td>0.660981</td>\n",
       "      <td>0.365677</td>\n",
       "      <td>-0.040350</td>\n",
       "      <td>0.229787</td>\n",
       "      <td>-0.271086</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>200 rows Ã— 625 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          0         1         2         3         4         5         6    \\\n",
       "0    0.735372  0.517957  0.360125  0.591212  0.054921  0.675716  0.469317   \n",
       "1    0.832643  0.344199  0.790840  0.473331 -0.143020  0.902948  0.359715   \n",
       "2    0.892297  0.557208  0.713414  0.376822 -0.115197  1.509475  0.808840   \n",
       "3    1.111488  0.715326  0.444180  0.517769  0.115481  1.115695  0.938606   \n",
       "4    1.067522  0.624089  0.637543  0.703967  0.176480  1.415276  0.881465   \n",
       "..        ...       ...       ...       ...       ...       ...       ...   \n",
       "195  0.961990  0.511375  0.068491  0.297462 -0.104591  0.810637  0.372254   \n",
       "196  1.055436  0.749483  0.975975  0.437121 -0.041992  0.936351  0.777895   \n",
       "197  1.110303  0.601051  0.342901  0.500978  0.296940  0.942765  0.436964   \n",
       "198  1.091442  0.828457  0.891922  0.848703  0.423952  1.111520  0.771587   \n",
       "199  0.825969  0.299535  0.037611  0.547068  0.294606  0.739853  0.210879   \n",
       "\n",
       "          7         8         9    ...       615       616       617  \\\n",
       "0    0.341129  0.873114  0.770013  ...  0.533776  0.590708  0.351696   \n",
       "1    0.869040  0.561634 -0.022386  ...  0.833698  0.342209  0.798735   \n",
       "2    0.828644  0.805651  0.519312  ...  0.756495  0.444941  0.606594   \n",
       "3    0.541302  0.665672  0.223781  ...  0.917731  0.762848  0.297982   \n",
       "4    0.884471  0.988769  0.549320  ...  1.078836  0.685927  0.544265   \n",
       "..        ...       ...       ...  ...       ...       ...       ...   \n",
       "195 -0.041543  0.127531 -0.254268  ...  0.722118  0.373444 -0.052891   \n",
       "196  1.091959  0.559783  0.052457  ...  0.798728  0.835067  1.071288   \n",
       "197  0.244673  0.824408  0.611259  ...  0.642207  0.274625  0.051561   \n",
       "198  0.886824  0.882306  0.221922  ...  0.641523  0.536660  0.789501   \n",
       "199 -0.039012  0.473949  0.218795  ...  0.625190  0.034581 -0.185647   \n",
       "\n",
       "          618       619       620       621       622       623       624  \n",
       "0    0.670366 -0.092422  0.621949  0.751444  0.606980  0.855188 -0.121695  \n",
       "1    0.474534 -0.358329  0.764603  0.306948  0.732067  0.456729 -0.532869  \n",
       "2    0.282850 -0.382810  0.581887  0.701730  0.814113  0.309131 -0.595847  \n",
       "3    0.375505 -0.152520  0.981974  0.834027  0.653438  0.462594 -0.248389  \n",
       "4    0.818167  0.302669  0.959196  0.666225  0.569900  0.610659 -0.207420  \n",
       "..        ...       ...       ...       ...       ...       ...       ...  \n",
       "195  0.131516 -0.337811  0.684117  0.551660  0.033654  0.125793 -0.488601  \n",
       "196  0.579723 -0.062425  0.890615  1.048405  1.262889  0.672654 -0.202523  \n",
       "197  0.318407  0.073765  0.691856  0.321364  0.111347  0.213866 -0.034468  \n",
       "198  0.678763 -0.214273  0.651204  0.613554  0.759610  0.493334 -0.325397  \n",
       "199  0.266156  0.016930  0.660981  0.365677 -0.040350  0.229787 -0.271086  \n",
       "\n",
       "[200 rows x 625 columns]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(X_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28c285c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LeaveOneGroupOut()"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaler = StandardScaler()\n",
    "logo = LeaveOneGroupOut()\n",
    "X_scaled = scaler.fit_transform(X_all)\n",
    "logo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adb43aa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "logistic_reg_initial = LogisticRegression(max_iter = 2000, random_state = 42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4e56c10",
   "metadata": {},
   "source": [
    "## Metrics for Logistic Reg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24becbb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metrics from aggregation across folds\n",
      "Acurracy across all folds:  0.625\n",
      "Precision for Class 0:  0.4594841269841269\n",
      "Precision for Class 1:  0.5030158730158729\n",
      "Recall for Class 0:  0.545654761904762\n",
      "Recall for Class 1:  0.5473214285714285\n",
      "------------------------------------\n",
      "Computing Precision Recall whole data set (not per fold)\n",
      "Precision for class 0:  0.5816326530612245\n",
      "Precision for class 1:  0.6666666666666666\n",
      "Recall for class 0:  0.6263736263736264\n",
      "Recall for class 1:  0.6238532110091743\n"
     ]
    }
   ],
   "source": [
    "accuracies = []\n",
    "precisions = []\n",
    "recalls = []\n",
    "# conf_matrix_folds = []\n",
    "\n",
    "# collecting across all folds bc my prec recall kinda weird with some classes having none per fold\n",
    "all_folds_y_true = []\n",
    "all_folds_y_pred = []\n",
    "\n",
    "for train_idx, test_idx in logo.split(X_scaled, y_all, participant_ids):\n",
    "    X_train, X_test = X_scaled[train_idx], X_scaled[test_idx]\n",
    "    y_train, y_test = y_all[train_idx], y_all[test_idx]\n",
    "\n",
    "    logistic_reg_initial.fit(X_train, y_train)\n",
    "    y_pred = logistic_reg_initial.predict(X_test)\n",
    "    y_prob = logistic_reg_initial.predict_proba(X_test)[:, 1]\n",
    "\n",
    "    accuracies.append(accuracy_score(y_test, y_pred))\n",
    "\n",
    "    all_folds_y_true.extend(y_test)\n",
    "    all_folds_y_pred.extend(y_pred)\n",
    "\n",
    "    # cm = confusion_matrix(y_test, y_pred, labels = [0,1])\n",
    "    # conf_matrix_folds.append(cm)\n",
    "    \n",
    "    # precisions.append(precision_score(y_test, y_pred, pos_label=1, zero_division = 0))\n",
    "    # recalls.append(recall_score(y_test, y_pred, pos_label = 1, zero_division=0))\n",
    "\n",
    "    prec, rec, f1, support = precision_recall_fscore_support(\n",
    "        y_test,\n",
    "        y_pred,\n",
    "        labels = [0,1],\n",
    "        average = None,\n",
    "        zero_division = 0\n",
    "    )\n",
    "\n",
    "    precisions.append(prec)\n",
    "    recalls.append(rec)\n",
    "\n",
    "all_folds_y_true = np.array(all_folds_y_true)\n",
    "all_folds_y_pred = np.array(all_folds_y_pred)\n",
    "\n",
    "prec, rec, f1, support = precision_recall_fscore_support(\n",
    "    all_folds_y_true,\n",
    "    all_folds_y_pred,\n",
    "    labels = [0,1],\n",
    "    average = None,\n",
    "    zero_division = 0\n",
    ")\n",
    "\n",
    "# print(\"Precision across all folds: \", np.mean(precisions))\n",
    "# print(\"Recall across all folds: \", np.mean(recalls))\n",
    "\n",
    "precisions = np.array(precisions)\n",
    "recalls = np.array(recalls)\n",
    "\n",
    "mean_prec_class0, mean_prec_class1 = precisions.mean(axis = 0)\n",
    "mean_recall_class0, mean_recall_class1 = recalls.mean(axis = 0)\n",
    "\n",
    "print(\"Metrics from aggregation across folds\")\n",
    "print(\"Acurracy across all folds: \", np.mean(accuracies))\n",
    "\n",
    "print(\"Precision for Class 0: \", mean_prec_class0)\n",
    "print(\"Precision for Class 1: \", mean_prec_class1)\n",
    "\n",
    "print(\"Recall for Class 0: \", mean_recall_class0)\n",
    "print(\"Recall for Class 1: \", mean_recall_class1)\n",
    "\n",
    "print(\"------------------------------------\")\n",
    "print(\"Computing Precision Recall whole data set (not per fold)\")\n",
    "\n",
    "print(\"Precision for class 0: \", prec[0])\n",
    "print(\"Precision for class 1: \", prec[1])\n",
    "\n",
    "print(\"Recall for class 0: \", rec[0])\n",
    "print(\"Recall for class 1: \", rec[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "158485be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([4.34633263e-01, 2.22551533e-01, 2.22805628e-02, 1.35156094e-01,\n",
       "       3.00718109e-01, 5.36088260e-02, 1.73541434e-02, 7.29167800e-02,\n",
       "       3.06152166e-01, 4.15234751e-01, 1.85450063e-01, 1.87011204e-01,\n",
       "       6.25979904e-02, 2.39198690e-01, 4.83568962e-02, 2.78234207e-01,\n",
       "       1.20114324e-01, 7.83794065e-02, 2.08202274e-01, 4.08479323e-03,\n",
       "       1.30846588e-01, 3.75851433e-01, 1.76498510e-01, 8.25339997e-02,\n",
       "       1.65004131e-02, 4.76257757e-02, 1.50921468e-01, 9.17140204e-02,\n",
       "       1.43550429e-01, 1.17179159e-02, 3.77665564e-01, 1.35971500e-01,\n",
       "       2.35240232e-01, 5.58896755e-02, 1.20828953e-01, 3.34144597e-01,\n",
       "       2.89114756e-01, 2.94297367e-01, 2.26119731e-01, 1.50485434e-02,\n",
       "       2.25696326e-01, 1.64935916e-01, 2.12470648e-02, 4.20849793e-02,\n",
       "       1.13333780e-01, 4.17311240e-01, 1.09164607e-01, 6.91407038e-02,\n",
       "       2.72699535e-01, 2.75212255e-01, 7.78335972e-02, 4.62013634e-02,\n",
       "       4.19069552e-02, 2.70163968e-03, 1.65455554e-01, 6.11228640e-02,\n",
       "       2.22898206e-01, 4.07964126e-02, 2.68425617e-02, 1.26604441e-01,\n",
       "       2.07985414e-01, 2.78233040e-01, 1.40415796e-02, 1.17168509e-01,\n",
       "       1.21346269e-01, 1.09655187e-01, 2.20735784e-02, 9.72343127e-02,\n",
       "       5.17657547e-02, 1.38936793e-01, 1.41395431e-01, 1.77168933e-01,\n",
       "       6.13322599e-02, 4.82920263e-01, 4.34528386e-01, 3.49098332e-02,\n",
       "       3.08732023e-02, 1.73913427e-03, 1.58753057e-01, 1.03020769e-01,\n",
       "       4.65113133e-01, 2.51902760e-01, 2.80768064e-01, 3.32549074e-01,\n",
       "       2.67548967e-01, 1.99826841e-01, 1.31535480e-01, 5.81166449e-02,\n",
       "       1.51073831e-02, 7.75536832e-02, 3.99677802e-01, 1.72131745e-01,\n",
       "       3.16927176e-02, 1.13451271e-01, 1.37814136e-01, 1.67349468e-01,\n",
       "       2.91347377e-01, 5.48613555e-02, 9.37706218e-02, 1.51648320e-02,\n",
       "       3.01684295e-01, 1.19734713e-01, 9.12360147e-03, 1.14978881e-01,\n",
       "       1.27066853e-01, 1.46576315e-01, 2.67470355e-01, 1.18078766e-01,\n",
       "       1.98654092e-01, 1.20910508e-01, 2.54779287e-01, 2.60359137e-01,\n",
       "       1.16992150e-01, 2.47938711e-01, 3.47192518e-01, 2.65399010e-01,\n",
       "       6.73520686e-02, 1.63182518e-02, 1.45798271e-01, 1.29336507e-01,\n",
       "       8.39567641e-03, 1.85237674e-02, 9.67742686e-02, 1.55972236e-01,\n",
       "       2.39289848e-01, 1.01525098e+00, 2.54336363e-01, 7.91044492e-02,\n",
       "       2.33073355e-01, 2.18742618e-01, 7.30191723e-02, 1.62455087e-01,\n",
       "       8.74968703e-02, 1.84708152e-01, 1.16500470e-01, 1.27174186e-01,\n",
       "       8.21514255e-02, 7.09325273e-02, 7.79151473e-02, 1.07428646e-01,\n",
       "       3.47742910e-01, 3.71497993e-01, 7.54582374e-02, 1.35922240e-01,\n",
       "       5.03762828e-02, 1.28395990e-01, 1.16113267e-01, 3.62576358e-01,\n",
       "       8.77796517e-02, 2.63586487e-01, 3.44295704e-03, 1.43999451e-01,\n",
       "       3.34812841e-01, 1.10758294e-01, 9.05064117e-02, 4.32133094e-01,\n",
       "       1.69404992e-02, 1.34150735e-01, 1.26069397e-02, 6.91703691e-02,\n",
       "       1.79702225e-01, 3.29640118e-02, 1.53576364e-01, 1.50150021e-01,\n",
       "       4.48826169e-01, 2.92053532e-02, 2.13122690e-01, 4.81640552e-02,\n",
       "       1.99608942e-01, 3.93278044e-01, 2.93132314e-02, 4.35017392e-02,\n",
       "       9.63791251e-02, 1.69514239e-01, 1.52939765e-01, 2.58428683e-01,\n",
       "       3.07152921e-01, 5.95500867e-02, 4.38717187e-01, 6.48404259e-01,\n",
       "       1.24868203e-01, 3.04082111e-02, 1.43821738e-01, 1.62614927e-01,\n",
       "       2.55741570e-01, 3.79951418e-01, 8.85582492e-02, 2.98106862e-01,\n",
       "       1.95886851e-01, 2.45609525e-01, 1.69187031e-01, 1.22186537e-02,\n",
       "       2.05226238e-01, 3.72222929e-02, 1.05251470e-01, 1.04646150e-01,\n",
       "       3.38687437e-01, 5.95784421e-02, 1.43241500e-01, 8.47822624e-02,\n",
       "       1.21119276e-01, 7.54681276e-02, 2.14510852e-01, 5.27562452e-01,\n",
       "       5.30896680e-01, 2.39570182e-01, 9.25865128e-02, 6.38655580e-02,\n",
       "       2.54193166e-02, 2.14413131e-01, 2.55090188e-01, 9.57639124e-02,\n",
       "       8.32749516e-02, 3.18655675e-01, 1.13743139e-01, 3.87488850e-01,\n",
       "       5.44216913e-02, 2.14653349e-01, 4.00109838e-01, 2.06147872e-01,\n",
       "       2.86035510e-01, 1.94237212e-01, 2.08329835e-01, 2.91851739e-01,\n",
       "       4.33732763e-02, 3.57666923e-01, 3.46804504e-01, 1.33025191e-01,\n",
       "       9.36493840e-02, 2.04178910e-01, 1.02356459e-01, 6.74264786e-03,\n",
       "       2.32993711e-01, 1.23670335e-01, 9.04151159e-02, 1.59800738e-03,\n",
       "       6.93094073e-02, 1.57577651e-01, 1.75414816e-01, 3.06133276e-01,\n",
       "       7.04522548e-02, 2.26576030e-01, 1.85666773e-01, 1.67776298e-01,\n",
       "       1.37109066e-02, 3.30843065e-02, 2.71209777e-01, 1.65964722e-01,\n",
       "       4.88979954e-02, 1.14848504e-01, 1.42482721e-02, 3.45006145e-02,\n",
       "       2.10139105e-01, 1.11449435e-01, 5.82154764e-02, 4.81100640e-01,\n",
       "       2.23856214e-02, 1.95287404e-01, 1.06058914e-01, 2.70808070e-01,\n",
       "       4.87295328e-01, 2.54015605e-02, 1.83761783e-01, 7.47999343e-02,\n",
       "       2.75666180e-01, 7.05071861e-02, 1.43795947e-01, 6.11371466e-02,\n",
       "       2.47458015e-01, 1.17258413e-01, 6.59737169e-01, 8.61418691e-02,\n",
       "       4.24858347e-01, 2.99922858e-01, 3.17503662e-02, 3.17072696e-01,\n",
       "       3.30586261e-01, 5.80570583e-02, 9.20564595e-02, 9.82629099e-02,\n",
       "       6.97226321e-02, 1.04140460e-01, 3.54809579e-01, 4.32364598e-01,\n",
       "       3.21504301e-01, 1.53907513e-01, 2.04903655e-02, 1.68478164e-01,\n",
       "       1.98700526e-01, 2.23531983e-01, 2.48629657e-03, 1.68340126e-01,\n",
       "       7.64371526e-03, 1.99708032e-01, 2.28833153e-01, 1.96250399e-01,\n",
       "       1.06904415e-02, 1.57846301e-01, 1.98767551e-01, 1.95419023e-01,\n",
       "       1.25461598e-01, 1.82238163e-01, 8.19363715e-02, 3.34021746e-01,\n",
       "       1.69740836e-02, 1.99359544e-01, 2.44245130e-01, 3.75817081e-02,\n",
       "       3.44122576e-01, 1.34518080e-01, 5.33600433e-01, 2.10593489e-01,\n",
       "       2.73692558e-01, 1.56227662e-01, 1.85998222e-02, 1.30586154e-01,\n",
       "       4.54934332e-02, 8.23241611e-02, 1.23434615e-01, 5.97387740e-01,\n",
       "       2.60865911e-01, 1.92329690e-02, 7.67909733e-02, 1.73965616e-01,\n",
       "       2.76043331e-01, 1.70380062e-01, 6.33914377e-03, 1.10818887e-01,\n",
       "       3.40090454e-02, 3.02390517e-02, 1.34756543e-01, 4.88778769e-02,\n",
       "       8.23351800e-02, 2.84637991e-01, 5.93779336e-02, 1.82338742e-01,\n",
       "       3.60427340e-02, 2.19921439e-01, 8.84152433e-02, 2.33352241e-01,\n",
       "       2.34364111e-01, 4.26522984e-02, 1.49458406e-01, 1.08060886e-01,\n",
       "       3.20999530e-01, 2.75747737e-01, 5.88603598e-02, 1.14822879e-01,\n",
       "       2.43860615e-01, 8.38960270e-02, 2.00528134e-01, 1.04701741e-01,\n",
       "       1.34810187e-01, 1.07600173e-01, 3.52587158e-01, 1.02928236e-03,\n",
       "       1.16309590e-01, 6.73939266e-02, 9.53877586e-02, 3.09535564e-01,\n",
       "       5.07343438e-02, 7.94022525e-03, 2.70124440e-01, 1.81329254e-01,\n",
       "       9.50011508e-02, 2.48186684e-01, 1.70410666e-02, 3.81922923e-02,\n",
       "       6.57778317e-02, 8.62840773e-02, 5.62667914e-02, 1.22051429e-01,\n",
       "       1.89637169e-01, 2.88901585e-01, 2.06890298e-01, 2.89078819e-01,\n",
       "       3.38711934e-01, 2.24941244e-01, 2.86797131e-01, 1.01552418e-01,\n",
       "       1.04216840e-01, 9.45781704e-02, 3.82606892e-02, 8.61681540e-02,\n",
       "       1.97259158e-01, 1.47474838e-01, 1.39249086e-01, 2.26411702e-02,\n",
       "       8.01961616e-02, 1.30114633e-01, 2.13665012e-01, 4.71961932e-03,\n",
       "       7.90907292e-02, 2.65111047e-01, 1.87009109e-01, 5.75902070e-02,\n",
       "       2.87056767e-01, 2.01814095e-01, 1.18224513e-01, 1.72847344e-01,\n",
       "       2.68812866e-01, 3.59526696e-02, 4.33673159e-02, 2.48633375e-01,\n",
       "       4.26839650e-02, 2.67634360e-01, 1.80356790e-02, 2.20103745e-01,\n",
       "       6.19308313e-04, 3.29173467e-02, 9.52974161e-02, 5.32177100e-02,\n",
       "       8.80137706e-02, 2.22486129e-01, 3.37521598e-01, 1.52682777e-01,\n",
       "       1.54581223e-01, 1.88016316e-02, 1.51719636e-01, 1.45217493e-01,\n",
       "       1.24896628e-01, 3.53507524e-02, 4.29181483e-02, 1.19424976e-02,\n",
       "       2.91108504e-01, 5.28128264e-02, 3.99037821e-02, 1.55159614e-02,\n",
       "       1.15548715e-01, 3.06047088e-02, 5.56662905e-02, 2.08719902e-01,\n",
       "       1.05975741e-01, 1.00415987e-01, 2.59113522e-01, 5.29209656e-01,\n",
       "       1.99886079e-01, 1.20832555e-02, 3.27186808e-01, 1.73547756e-01,\n",
       "       3.22295125e-01, 4.21451320e-02, 2.93549208e-01, 7.24251281e-02,\n",
       "       1.91562773e-01, 9.80041685e-02, 1.07988806e-01, 2.11648212e-01,\n",
       "       3.23901788e-02, 4.24062104e-01, 2.90776417e-01, 1.00628098e-01,\n",
       "       1.50140729e-01, 1.64851050e-01, 1.06496302e-01, 4.83171962e-01,\n",
       "       1.82134531e-01, 5.37750137e-02, 2.42623300e-01, 2.90368539e-01,\n",
       "       1.62396186e-01, 2.70366425e-01, 1.67635629e-01, 4.92375003e-01,\n",
       "       2.36465181e-01, 3.78798591e-01, 9.77326720e-02, 2.14457735e-01,\n",
       "       3.72132094e-03, 3.25047522e-01, 2.89067002e-01, 4.56461840e-02,\n",
       "       2.94962985e-01, 1.89882806e-01, 1.55886133e-01, 1.97246835e-01,\n",
       "       1.61572481e-01, 2.17265295e-01, 2.32978942e-02, 5.78132813e-01,\n",
       "       2.82176834e-01, 2.58815066e-01, 1.04248824e-01, 2.98843630e-01,\n",
       "       3.27564594e-01, 2.14657037e-01, 2.35568384e-01, 2.98030274e-01,\n",
       "       3.98890576e-01, 1.22425732e-01, 2.24418322e-01, 8.58048116e-02,\n",
       "       2.10222639e-01, 6.67340760e-02, 4.27406336e-01, 2.80849310e-01,\n",
       "       5.87974298e-02, 1.85542947e-01, 3.40974417e-01, 2.80684831e-01,\n",
       "       1.25985397e-02, 2.74536957e-01, 1.94190309e-01, 1.49595379e-01,\n",
       "       2.72152626e-02, 8.28402523e-02, 6.67315374e-02, 2.27347959e-02,\n",
       "       3.35812914e-03, 2.93288874e-01, 2.93094878e-02, 1.95172483e-02,\n",
       "       2.60559416e-01, 1.21581355e-01, 5.61834830e-01, 1.45938759e-01,\n",
       "       2.21249479e-01, 5.18164725e-02, 1.18067956e-01, 2.33849583e-01,\n",
       "       7.14664298e-02, 6.83750731e-02, 2.59343955e-02, 9.05776730e-02,\n",
       "       1.87510005e-01, 2.42659255e-01, 1.25301736e-03, 1.90769657e-02,\n",
       "       4.53269672e-02, 3.56439384e-01, 4.66454848e-02, 1.66121404e-01,\n",
       "       1.82232598e-01, 2.46158412e-02, 2.48241622e-01, 1.75992951e-01,\n",
       "       2.16249405e-01, 2.81026295e-02, 4.94435338e-02, 3.60183111e-01,\n",
       "       1.73870518e-01, 9.79282370e-03, 8.62288080e-02, 1.24690096e-01,\n",
       "       3.87662119e-01, 2.67729714e-02, 2.86815718e-02, 9.97789434e-02,\n",
       "       2.48280938e-01, 2.64966699e-01, 3.48753702e-01, 9.62612048e-02,\n",
       "       2.51204906e-01, 1.32942650e-02, 1.77937197e-02, 2.40512969e-01,\n",
       "       1.09660695e-01, 1.84995428e-01, 8.00864641e-03, 7.97818936e-03,\n",
       "       2.64309315e-01, 2.00623746e-01, 3.41846968e-01, 1.49010680e-01,\n",
       "       5.04227352e-01, 1.39623587e-01, 6.35275990e-01, 4.31727960e-01,\n",
       "       2.42725183e-02, 2.31178802e-02, 2.79137540e-02, 7.90890821e-02,\n",
       "       4.54053975e-01, 3.19374802e-01, 1.33953786e-01, 1.08468862e-01,\n",
       "       7.68113328e-02, 2.40332802e-01, 2.32735218e-01, 2.21141251e-01,\n",
       "       1.59988330e-01, 2.17866436e-02, 2.25770747e-01, 2.17158432e-01,\n",
       "       8.49303975e-01, 4.24030491e-01, 1.78829844e-01, 2.75683490e-01,\n",
       "       2.85712728e-02, 3.33577861e-01, 1.83703336e-01, 3.54211994e-01,\n",
       "       2.67606015e-01, 6.18293857e-02, 1.36667393e-01, 6.99108203e-01,\n",
       "       2.31690325e-02, 1.82066154e-01, 1.63386537e-01, 1.58418072e-01,\n",
       "       5.88473237e-02, 4.60335137e-01, 2.20078822e-01, 3.97298575e-01,\n",
       "       1.51628460e-01, 8.36998160e-01, 5.02458960e-02, 2.17372773e-01,\n",
       "       1.49246389e-01, 4.43791892e-01, 9.19164991e-02, 2.60849719e-02,\n",
       "       1.34665668e-01, 6.66876601e-02, 8.33720731e-02, 6.53020557e-02,\n",
       "       1.56353815e-01, 1.53684189e-01, 1.15651071e-01, 1.28016504e-01,\n",
       "       1.78693722e-02, 8.27791556e-03, 2.78689003e-01, 2.38120258e-02,\n",
       "       4.87778595e-02])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coefs = logistic_reg_initial.coef_.ravel()\n",
    "feat_importance = np.abs(coefs)\n",
    "feat_importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b56f029e",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_channels = 125\n",
    "n_bands = 5\n",
    "\n",
    "channel_importance = feat_importance.reshape(n_channels, n_bands).sum(axis=1)\n",
    "# channel_importance[i] = total importance for channel i across all 5 bands\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b6d7083",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 16  25  35  40  54 112 116 121]\n"
     ]
    }
   ],
   "source": [
    "k = 8 # k kinda arbitrary here just wanted to limit bc computation time\n",
    "top_channel_indices = np.argsort(channel_importance)[-k:]\n",
    "top_channel_indices = np.sort(top_channel_indices)\n",
    "print(top_channel_indices)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32e86eb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_scalogram(\n",
    "    signal,\n",
    "    fs,\n",
    "    fmin=1.0,\n",
    "    fmax=45.0,\n",
    "    num_freqs=32, # gonna experiment with later\n",
    "    out_size=(64, 64),\n",
    "    target_fs=32.0, # for downsampling\n",
    "):\n",
    "\n",
    "    # downsampling here in order to save time\n",
    "    decim_factor = int(max(1, round(fs / target_fs)))\n",
    "    if decim_factor > 1:\n",
    "        signal_ds = decimate(signal, decim_factor, zero_phase=True)\n",
    "        fs_eff = fs / decim_factor\n",
    "    else:\n",
    "        signal_ds = signal\n",
    "        fs_eff = fs\n",
    "\n",
    "    # using morl wavelet here\n",
    "    freqs = np.linspace(fmin, fmax, num_freqs)\n",
    "    wavelet = 'morl'\n",
    "    central_freq = pywt.central_frequency(wavelet)\n",
    "    scales = central_freq * fs_eff / freqs\n",
    "\n",
    "    coef, _ = pywt.cwt(signal_ds, scales, wavelet, sampling_period=1.0/fs_eff)\n",
    "    power = np.abs(coef) ** 2  # (num_freqs, time_ds)\n",
    "\n",
    "    # apply log transformation then normalize\n",
    "    power = np.log10(power + 1e-12)\n",
    "    power = (power - power.mean()) / (power.std() + 1e-6)\n",
    "\n",
    "    # wanna have it 64,64\n",
    "    img = resize(power, out_size, mode=\"reflect\", anti_aliasing=True)\n",
    "\n",
    "    return img\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfad0b0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_cnn shape: (200, 8, 64, 64)\n"
     ]
    }
   ],
   "source": [
    "k = len(top_channel_indices)\n",
    "H, W = 64, 64\n",
    "\n",
    "X_cnn = []\n",
    "y_cnn = []\n",
    "participant_ids_cnn = []\n",
    "song_ids_cnn = []\n",
    "\n",
    "for song in range(21, 31):\n",
    "    load_mat_file = loadmat(f\"../data/cleaned_eeg/song{song}_Imputed.mat\")\n",
    "    data = load_mat_file[f\"data{song}\"] # (channels=125, time, participants)\n",
    "    fs = float(load_mat_file[\"fs\"].squeeze())\n",
    "\n",
    "    channels, time, n_participants = data.shape\n",
    "\n",
    "    song_idx = song - 20\n",
    "    song_label = like_df[f\"song_{song_idx}\"].to_numpy()  # should be length 20\n",
    "\n",
    "    for participant in range(n_participants):\n",
    "        eeg_participant_full = data[:, :, participant] # (125, time)\n",
    "        \n",
    "        eeg_participant = eeg_participant_full\n",
    "\n",
    "        # limit to top k channels\n",
    "        eeg_top = eeg_participant[top_channel_indices, :]\n",
    "\n",
    "        scalograms = []\n",
    "        for ch_idx in range(k):\n",
    "            sig = eeg_top[ch_idx, :]\n",
    "            img = compute_scalogram(\n",
    "                sig,\n",
    "                fs,\n",
    "                fmin=1.0,\n",
    "                fmax=45.0,\n",
    "                num_freqs=32,\n",
    "                out_size=(H, W),\n",
    "                target_fs=64,\n",
    "            )\n",
    "            scalograms.append(img)\n",
    "\n",
    "        # stack into (k, H, W) tensor for this sample\n",
    "        scalograms = np.stack(scalograms, axis=0)  # this is (k, H, W)\n",
    "        X_cnn.append(scalograms)\n",
    "\n",
    "        #define Like is enjoyment >= 6\n",
    "        label = 1 if song_label[participant] >= 6 else 0\n",
    "        y_cnn.append(label)\n",
    "        participant_ids_cnn.append(participant)\n",
    "        song_ids_cnn.append(song_idx)\n",
    "\n",
    "# convert to arrays\n",
    "X_cnn = np.stack(X_cnn, axis=0)  # this (N, k, H, W)\n",
    "y_cnn = np.array(y_cnn)\n",
    "participant_ids_cnn = np.array(participant_ids_cnn)\n",
    "song_ids_cnn = np.array(song_ids_cnn)\n",
    "\n",
    "print(\"X_cnn shape:\", X_cnn.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc03ce42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(200, 8, 64, 64)\n",
      "(200,)\n",
      "(200,)\n"
     ]
    }
   ],
   "source": [
    "print(X_cnn.shape)\n",
    "print(y_cnn.shape)\n",
    "print(participant_ids_cnn.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b688caa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZQAAAGGCAYAAABc23cwAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjcsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvTLEjVAAAAAlwSFlzAAAPYQAAD2EBqD+naQAAfNlJREFUeJztnQeYE0UbxyflGr036ViQjiio2BAEsXwgil1BUD9RVMSKBcQCgooNEP1UFCsqVlQEEcWCKCIWRARFkI7SuZ7s9/wHNpl5NztJ7i53yd3781m5ZHdnZ2c3++681WNZliUYhmEYpph4i9sAwzAMw7BAYRiGYUoMnqEwDMMwJQILFIZhGKZEYIHCMAzDlAgsUBiGYZgSgQUKwzAMUyKwQGEYhmFKBBYoDMMwTInAAoXw2WefCY/HI/9NRf766y/Z/+eff15UZCZOnChat24tgsGgSGVwHXE9cV1TGfu+fOihh0R5IBG/s6OPPlrccsstosIIFPvmdlu++eabxPWUYWJk9+7dYsKECeLWW28VXi+/M1EgZCFwW7RoITIzM0WHDh3Eq6++yvdXGY/3rbfeKqZMmSI2b96cstfCX5Sd7rnnHjk4lIMPPrgk+sQwxeK5554ThYWF4oILLuCRjMAdd9whHnjgAXHFFVeIo446Srz77rviwgsvlC+F559/Po9ZGY13v379RLVq1cTUqVPlMzYlseJg+vTpSCRpfffdd1Z5ZcGCBfIc8W8qsmbNGtl/XKvSJBAIWDk5OVYy0KFDB+viiy+2ygP2bw7XtSRYv369lZaWZl1zzTWh74LBoHX88cdbjRs3tgoLC61E3pcPPvigVR6I9Xe2Ps7xHj58uNWsWTO5TSqSEH3AmDFjpKph/vz52vdXXnmlSE9PFz/++KP8nJ+fL0aPHi26dOkiqlevLipXriyOP/54sWDBAlf9K6aELVu2FJUqVRK9e/cWf//9N4SiuPfee0Xjxo1FVlaWlPTbt2/X2mjevLk444wzxNy5c0WnTp3k1LNNmzbirbfeiumcFi9eLE499VTZTxz7xBNPFF999VVM+z7xxBOibdu2cr+aNWuKI488UrzyyivaNhs2bBBDhw4VjRo1EhkZGXIGOGzYMDlGAOdz0003ifbt24sqVarIN5m+ffuGxjIan376qRxbjHGNGjXkGK1YscKxHWxH6B/Gp1WrVuKpp54Sd999txx/FXwePny4ePnll+W5oc9z5syR63Cdjj32WFG7dm15PXB933zzTcex7DbeeOMNeS2w7THHHCN+/vlnuR7HxqwXfTnppJNisiOsWbNG/PTTT6JXr16Oda+99prsS9WqVeX4YSwfe+yx0PpYx9i2s73++uti7Nix4qCDDpJtnnPOOWLXrl0iLy9PjBgxQtSrV0+2c9lll8nv3MbvsMMOk+eIvi1cuFDEwkcffRS6njj26aefLpYvXx51P7wdFxQUiKuvvlrrC+619evXi0WLFomikJubK++TQw89VJ5Lw4YNxYABA8Qff/zh2Pbpp5+W9xbuGbyxf/fdd9p6XL/BgwfL3znaatCggRgyZIj4999/te3s+3L16tVye9zX+H1ivLOzsyOO9zvvvCPatWsnj4371r5n6W8Rx6tfv35oO8x6i8K7cY73KaecItauXSuWLVsmUpKivC198skn1rZt27Tln3/+CW2Xn59vde7cWUra3bt3y+/mzJkj97333ntD22G/hg0bWiNHjrSefPJJa+LEidZhhx0mJfoPP/zgeBvo1KmT1aZNG2vSpEnWnXfeaaWnp1tHH320dfvtt1vHHnus9fjjj1vXXXed5fF4rMsuu0zrO/py6KGHWjVq1LBuu+022Ub79u0tr9drzZ071zhDmT9/vjzWMcccYz388MPWI488It+C8d3ixYuNY/b000/L9s455xzrqaeesh577DFr6NChsp82GzZssBo1amRVqlTJGjFihDVt2jTrrrvusg4//HBrx44dchvMClu1aiX7jnbuuece66CDDrKqV68u9ze9Oc2bN8/y+/3y/DHGY8eOterUqWPVrFlTe/NdunSplZGRYTVv3tx64IEHrPvvv1/2q2PHjrJNFXxG/+rWrSvbmzJlSuia4c3r6quvtiZPnizHuWvXrnL72bNnO9rAODZp0kQeDwvOp2nTpnJfXGuMt32te/ToYUXjpZdeku3+9NNP2ve4xvi+Z8+esq9Y8DY4cODA0DaxjrF9j+B+xD2h3nfnn3++deGFF1p9+/aVx7jkkkvkthgjeu7t2rWT1wHHmTBhgrxHs7KyrJ9//tk4Q5kxY4Y81qmnnmo98cQTcl9cM9zb0WYyl19+uVW5cmXHG/Dq1avlcXAu8YK3bIwr9sf549qNHz/eOvnkk6133nlHuy/xXDj44INln3Ev4vxxv+CZYfPQQw/JN3iMC34/119/vRwX3Edqv8eMGRNqc8CAAdbUqVPl+eG7W265xTHeuI/xvMEz6NFHH7Vatmwpf3Pqs2vz5s2yP7gncXw8l/7zn//I/fG7j3eGcnmc440ZDb7HdU1FiiRQIi14EKngR4GHAAYUD0X8MI888kiroKBAuxHz8vK0/bBt/fr1rSFDhjguHh5eO3fuDH0/atSo0I2itnvBBRfIY+fm5oa+w48V286aNSv03a5du+QNhhvSTaDgRjjkkEOsPn36aDdFdna21aJFC+uUU04xjlm/fv2stm3bGre59NJLpWCLpEq0j4lzgVpJBeOCcceNb7rR8eCrV6+e9e+//4a++/HHH+UxcWybM888U/7A1IfnqlWrpDCKJFCw//Llyx19xtio4GGBhyceMLQN9F99COJBju8bNGgQehlRr3W0ByaED7bbs2eP9j0eStWqVTOqdGIdY/sewTmpD0Lcd3jQQ5ioQOjg/qPnjmXJkiWh79auXWtlZmZaZ511lqtAwXlBcFxxxRVae3gQQvDR7ymnn366fJBS9u3bJ48DYRovzz33nNwXLw9u9699X9auXdvavn17aP27774rv3///fdd7x/w6quvyu0WLlzoECjqswJg/HAcFWyHZwIe5OpvgD688bKHZ4IqZAAEJcbX7lusAuX0Iow3+jls2DCrwqi8oHaaN2+etmAKroJpJdQBzzzzjOjTp4/4559/xAsvvCD8/rAfgM/nkyow2xMCKgcYU6FyWbp0qeO4AwcOlFNam27dusl/L774Yq1dfA9VEaauKlAnnXXWWaHPUGlceuml4ocffnD1rMDUc9WqVdKIhik3zgPLvn37RM+ePaWKwuSaimk4prZ0Wm+DfTENP/PMM+V5U2xVE6betsdSIBCQfYE6BeqSSGNls2nTJnkOUAnUqlUr9D08TTC9/vDDD0NtfvLJJ6J///5ynGygcoLaJxJQ+0FVRYHqymbHjh1SDQT1TKR+YgyhjqTX9Oyzz5aqHPr9n3/+KUxgXHAvYGzodcA1w73qRrxjjHsnLS1N6yOeXVCXqOB7qGZxb6tAvQc1l03Tpk2lKvLjjz+Wx48E+r9z507pcGDfi1jwW8JxqLqYkpOTI8+TAtWSvT5eZs2aJerUqSOuvfZaxzqqKj3vvPOk2tcG9wW9rur9A1Uazg8utSDSdbjqqqu0z2gT1w7efipQg0LVpv4G8Aywj41rh3PBbxF/q+OLZxjuY9NvraTGG+ODY1YYL6+uXbtGfPhRbr75Zqm3/vbbb8W4ceMiPnwgZB5++GHx22+/SV2jTSQvMvzgVGzh0qRJk4jf42GmgocjvcGh8wXQz0NXS4EwAYMGDXI9T9xo6o+EugLiQY0xw/Fh94Fw6t69u1y/bds2eeNDAJuA4IG+Hx4gsBOoDxzYKtyAPhbgoUg5/PDD5cMLD1r0ATd3JE89N++9SNcIzJ49W9x3331SkKm2Azr2JXFNYwU6bNg8IBxh88B1OPfcc6VdrKhjHE/f0TbuE7WdQw45xNEm7kfo/3FfmO7Hk08+OeJ54gFpAg9ras+xH9z2+niBnQT3l/pS5wYdM/t3o15XvFjiZRTPjq1bt2rbYwzjaVMdD7qdva19bIw5hDVsPFgiQfsTjawijDeEWaTfSrkVKLECyW//AGxDq8pLL70k35zxVgzhAyMm3rTGjx8f0ZiHdZFw+74kqhvbs48HH3xQGvMjQd+G6UN75cqV8iELAyDegPDAgjMCfjSxAoF81113ybdfOCBgtoG3aRh/yyp4L9KP4YsvvhD/+c9/xAknnCDPE8ZZvMVPnz7d4YiQiGuKBzZmAnv27NFmOLi3IOAgQDGbxoI+YZaBl5qijHFZ3o8vvvhiRIET7aGO64FZDH1oYSYL1NlpIohlbCDov/76a/lMwG8Ovy+cN4R/PNeBjne07ey2ofFwe4HErCYeGhZhvCHUMONLRRImUHBxICzwhoAfJH6s8IKB54cNPH/gyQFPK3Ww4SWWCOANQi/s77//Lv9V1S4q9hQZ5xHJcygW4ImDqT4WqOIwBvfff78YNWqUqFu3rmz7l19+MbaBserRo4d49tln47r5mjVrJv+FUKNgVoh90T9MwbFgjCiRvnMDAhPt4MGtTvXx8C4NEB0PMMOgP36oV6HOwIL7E7MWeJJBiGAWVtQxLir2y5YK7kd4A+K+MN2PEJBFuR/xgIYaGh5+qsYAXoz2+nhBn7A/NAyqCrAoYLYA71C8bOGlyzRWJQ3GHC8hmJkW9bde3PGGmh7PCLyIpiIJCyOeNGmSfMvA1BFve3Ajhaucqhu03xjUNwkMdFFdF6OxceNG8fbbb4c+Q80zY8YMeVEjve0B6Ljxg4Er7N69ex3rMU02QV0d8VDDjYVzxg8Qb8CYob3//vtiyZIljv3tscFY0TcuuNtSO1GkNyScH97C8WC0gQCDC/Vpp50Wah8/IthzME6qMKH2MRNoBwJbVRdBnYh2SwPYJQAdS3odMO62wLFVEkUd46KC+1zVycPOAjdTqOPc3qahy8cLCF7QVBVxrPcjbDR46GP2aINznjZtmlQF4ncaL7B34Xc9efLkYs/KIj0TwKOPPioSDY6Nc8FLUaQXvGhjWxLj/f3338t/i3IdUnaGggcM3m4pGATMOCCN8daHGQreBu20LXiw2bpsgLgQzE5gKIcfPd4qMdB44EZ6eBcX6KcR6wEDOXzM4Vu+ZcsW49szHjx4w4DuHf7o8HHHjYCHDKay+HFDGLiBhwOEFWwmOCbGBj88nK+tksHDAQ93GLkRq4O3E0yJ8TD78ssvpUEZY4XoWRwf4wwVImIYMN7RgLoO/cfDFucPWwliY6Dbhy+/Df5GP9BXCH8IBfQV9p1Y/eJxXniZgHoCtiLonOHEgRkA4gsSDcYD/YXdSjWOX3755VI3D9sD4pVgW8IY4J603waLM8ZFAf2EgLjuuuvkbM5+6JhUobjfnnzySXHJJZeII444QkZa48163bp14oMPPpDXLtKD3QbnDo0B7gkIJMSBQNhDVYlzVQUZfrMYC/w+8Ft2A2pDvJiNHDlS2kthFIddDtcAv3c8VGMF5wd1KVKVoH/4reGexLOhNEBEO37XcHBAZDueRbhvIPhxPjS+LRqN4xhv2+kCtp7OnTuLlKSk3IZtFzq4ZR511FHSl1t18QWIwcB2M2fODLkUjhs3TrpUwjUT7ruIVRg0aJDmZukWZWu7b77xxhsR+6m64aI9uPB9/PHHMvYBx2vdurVjX7dIecRYwNcd7ojYF+2de+65MkbFBNxgTzjhhNB+iHO4+eabpcuyClxG4cIL12hsB1dDRNfabtVwab3xxhulSyN88rt3724tWrTIOvHEE+VCx4q6MyJ2CPtgX7jPwkX4119/dfQX54PrANdF9PWZZ56Rx4U7qwqOoUb/qjz77LPS1doeY/TFdvGM1ka81zoScF+tUqWK5n765ptvWr1795bu0zg3xLr897//tTZt2hTaJtYxjue+A/a5I+6KnjviZuyxwrjT+84tUh7bwZUdrqy4NrhWgwcP1tyQ3YBrtP27w1jArR39oMCdFsdGDFk0MNZ33HGHdKVHHBncvhF79ccff0SNlMf3GCM1FgOuv3CPxvkhVmjjxo2O7SKNq9uYud2vGAM8b1S2bNkit0Usin0uiLNBTExRMlIEYhxvbId7D67vqYoH/xMVANhI8EYI4zgTH1DJIQq7NPTYJQE8gTCrwFsuZmTJCNSC11xzjXE2UdbAOA51JWYdTOJ555135KweDklQVacinIqV0aB+8RAiiFVB6pNUAao8pAGHmiHV09eXFXjPRIoZuH8zpcOECRNkephUFSYJdxtmUg+82dt5lGBngL4ejgSpVqcB8T9YmKLPoOKNuWCKR6KckUoTFiiMBozpqNWAzAEwFMOQD6eBSEF4DMMwKhXGhsIwDMMkFrahMAzDMCUCCxSGYRimRGAbCgFeQYgUR9BhqiZoY5jyDLT0yNWGPFh2duhYQVJGu2hdvMA5xc4SzESGBQoBwoRmi2UYJvlAqhpEoscjTFo0qyI2b41cGiAayHiBiH0WKu6wQCHY6VA6/edO4Uvb/zbiz1ViGcikJW2vfnMG0sJvTB6Hu4P+RSAzvK23QF9nefUD+fLDfQj6PcZtCyqF27XIFfYoJTnS9+h99wb0PhQq/SusrL8Jemh/1T6R0A9t/OTOwvVcyBCJYHp4vS9PX+nN1z97lP6r+8l1pAuWkvEi2tgH08KfgyTFlnbepC16THpu3kJ1W31lflV6IOVvw8RZbTPSOATJuanQPli+8LYW2S1IckCmZYf3dVRZIuftUW87sm1AGWt6Pt4D926gIFf88P59WjbpWMDMBMJk7ffNRbWq8c1sdu8JimZd/pJtsEBxhwUKwVZzQZj4bYEScBcofr/+UPaYBApxqFO39YooAkUJ0FMfcJG2tZR26Q9f3dSfRgQK7bDSjtpmpP6qgoE+SLXxkxtE3m//gdwfMH7ywPPS8fRarmNkFCii6AKF9t8n4hAoynjTh3kwrYgChVxD+oD2KEKiOALFQ+4rnyJI1bHd/wXpo/JS4xA+RAB6lWtKu15UlXSVqh65xEPQNOhMCBYoDMNUKAJWUJDJeEz7MNFhgcIwTIUiKCy5xLsPEx0WKAzDVCiC8r/492GiwwKFYZgKRcCy5BLvPkx0WKAwDFOhYJVX4uBIeYZhGKZE4BkKwzAVboYSYKN8QmCB4gICAu0YjmDA4xqj4MkiOyqrqdqVBsFpMQJ0W+J07wmGPwcySJAhjWFQrmqQtqMG/0WLZ1H2pfEWtF01noDGX6h9p9BjOterxyTt0rtXiUuIGoei9reQrKMhIOplIuftiKMxHJNeJ/VmoSFA6jWkbTliQiz360LjUNTzpufpCbhv6wzS1VHbctxHdIiUxiwSS+I4b2XnwIH9imseZ5VX4mCVF8MwFdIoH+8SLwsXLhRnnnmmzDmGIEyU+DWBCpnYji6oTZQqpJRA2bBhg7j44otF7dq1RVZWlmjfvr1YsmSJljRu9OjRsoQm1vfq1Stl6qAzDFM6BIu4xMu+fftEx44dxZQpU+Lab+XKlWLTpk2hpV69eiJVSBmV144dO0T37t1Fjx49xEcffSTq1q0rhUXNmjVD20ycOFE8/vjj4oUXXhAtWrQQd911l+jTp4/49ddfOf8OwzCSQBFsKPFuD/r27SuXeIEAqVGjhkhFUkagTJgwQWYBnj59eug7CA11dvLoo4+KO++8U/Tr109+N2PGDFG/fn051Tz//PPjOh500XbeI1WvS3MPUV29CaqzVvXvai6k/cfXt/UY8kn56OuTx72/mg6d2i+Iolw9Dj0mxWRDoXYHh07d2K5qvxJmfbtmbyE5oWi+LnWMSDum5JuO5JB0fNX1DtsBaVdVoxA7k+PaKNua1lHoGOl51PRVjpyO6r3ieoQD65VzM91z0fJiOexXAec1DcRzA6UgnTp1Enl5eaJdu3bi7rvvli/SqULKqLzee+89ceSRR4qBAwdKCd65c2fxv//9L7QeaaWha4Say6Z69eqiW7duYtGiRWXUa4Zhkg34pRRlAbt379YWPPhLioYNG4pp06aJWbNmyQUv0CeddJJYunSpSBVSZoby559/iieffFKMHDlS3H777eK7774T1113nSx6M2jQoJDhCjMSFXw2GbVwQ6g3BW4ShmHKL0Wxidjb01pJY8aMkbOIkuCwww6Ti82xxx4r/vjjD/HII4+IF198UaQC/lSqpIgZyrhx4+RnzFB++eUXKdEhUIrK+PHjxdixY0uwpwzDJDNQuQXiTEdvq+lQ1KtatWqh7zMyMkQi6dq1q/jyyy9FqpAyKi9MB9u0aaN9d/jhh4t169aFqqmBLVu2aNvgs70uEqNGjRK7du0KLbhhGIYpvyAcqCgLgDBRl0QLlGXLlslnX6qQMjMUGKbgTqfy+++/i2bNmoUM9BAc8+fPl0YtW321ePFiMWzYMNd2cUNEuilgSLSNiUFT0JZSAIjatZ3GSWoEV9bRYkE04CuKUTxmI6dm5KbRdLEbSx3Fo9RzpUGa1IZsCJgzbUtfKqnxXK3u5yjcZKj6Sg34ziBOpT+GAlUUh+3YcD84gx51tHEiRnjt3qHX1Bt7kKajSIihEJqjv9q9TI7pdb93TAGn9LPdruPaxkmgCDOUeLcHe/fuFatXrxaqnRcColatWqJp06byZRahEHAeAnAqwnOsbdu2slzxM888Iz799FMxd+5ckSqkjEC54YYbpE4RKq9zzz1XfPvtt+Lpp5+WC0AA0IgRI8R9990nDjnkkJDbMIKK+vfvX9bdZximgrFkyRIZ5mAD+y+Aiv7555+XMSa2hgWgvPCNN94ohUylSpVEhw4dxCeffKK1keykjEA56qijxNtvvy2l+j333CMFBiT6RRddFNrmlltukcFEV155pdi5c6c47rjjxJw5czgGhWGYUp+hnHTSSbprOAFCRQXPLyypTMoIFHDGGWfIxQ3MUiBssDAMw0QiaHnkEg/xbl9RSSmBwjAMkyozlIoICxQXvIVCeL0RjIhkCuslhkxPIA5DphXZoBzJ8KgbnIkxusC9DzgPt/7SY9LPqkWXtkONqeqp0XU0C0C0DMM6VsRMyZHQxogcwjm+7pmeHVmC1WtBrzftg2r8p+1a7n3y5tNrSK6NMobOrNCWe9/JdVON9F7SIce2fvd7V73H6Lk47w135wk6Rt5Cum0EozztZ5wEhFcu8e3DxAILFIZhKhRWEVRe2IeJDgsUhmEqFKzyShwpE9jIMAzDJDc8Q2EYpkIRsLxyiW+fhHWnXMECxQVfXlD4g8GohlZ/trt12ksjkWn6cuWjM1Jav+F9OQHXCGdfvt4HtUQwNQT7c8Pb+pS/5ec8/bNHMZ46S+rSVPfu5WwdY2TIEOBwelDOhfbPYdBVxkHdj56L7G+a6nCgrwuQjAHUUGyCOkiYxkzdlvYvkEmucZ4V0QhPx17dbv/GXteMB44U/7QP6co1LTQ7Wvhz3LMUOEsNu29L7x1fvtMZwWMY41jzcgXjVM6gbDCTJAKloKBAZvzNzs6WhbGQeoBhGKYsYBtKCtpQ9uzZI9PNn3jiiTKJWvPmzWUyRwgU5N+64oorZAp6hmGYslB5xbsw0UnIKE2aNEkKEFRXRMErVExEUjQkc0SxK9QQKCwsFL179xannnoq131nGKbU2K/yin9hykjlhZnHwoULZdZMtxz/Q4YMkbVMIHS++OILmdAxWdFsHfGoUg02k7jbMkHbUePGDL+DqMenGZBj7Y6j9G08O0f5XNR26Go1gzCJWjNmwPUUvRQyRbUlUNuRqV1HO2pQaZR2jONium4l+Tw1tGW6X0OBjfxsr1gC5dVXX41pO6SNv+qqqxLRBYZhmIgEixApz0b52Ch1xSBqlEAFtmLFitI+NMMwDNtQUlmgoHbJ5MmT5d85OTmyjC++Q67/WbNmJfrwDMMwjhlKURYmOgkfJdhSjj/+ePk36pmgPgBqlTz++OOyGBbDMExpErA8RVqYJIhDQZ12O+4Exa7OPvtsWY3s9NNPFzfffLNIVgqzvMJK9zoDvhz3lR615TUEA5oMqzQgLZBBy9L63Q2vtFywctzCLPc6tDRjLO2vGiBZUIkGNpodDnR8sTsGkHNTMzZbdvrn0GdyFCUQzxG0R0KdC5Vz8/rNmZ7V8Q2k0/6STbUau2QdMdKnKe3SoMdCcv09yrVxjln4z6BfH2vndQv/HSCVr+k1DaaZSl+TfQPugYxqgOT+bT3u14Xcr2oQb/DATyCQ7ymDbMMc2JgUM5QmTZpIV2FUUoRAgasw2LFjB1dSZBiGKUckfIaCOu8o01ulShUZ0IiymLYqrH379ok+PMMwjEbQ8solHoLR3LGZ0hEoV199tYw7+fvvv8Upp5wivAfUFi1btmQbCsMwpQ6rvFI8lxc8u7CowIbCMAxT2sCMFK+RPUqMKpNIgTJy5Mi40rQkIzAs2sZFtVobnSkHiUFXtdrTEsDGaGdiIHcYQRVLLDWeer3EqOxxN5CqfaDrgn69g2r/bYNouA+xZxNw9EHLskzbdR8zmmU56G7rdzoqkNWmdk3XiRr7Hedq0IxQw7uljinJaGzqk6Mcr2qUJ89J7RjyOLGXqNbWe6JcU597u/TeUYeXpjSh11T1wwj9Hk3XPQaK4gbMbsNlKFB++OGHmLbzFDG1B8MwTOnWQ+E4lDITKAsWLEhEswzDMEwSU2pid/Xq1eLjjz+W0fIAAY4MwzClDWcbTmGB8u+//4qePXuKQw89VJx22mli06ZN8vuhQ4eKG2+8MdGHZxiG0eB6KCns5XXDDTeItLQ0sW7dOllgy+a8886TxvuHH35YJCWYQFmRjJVmo6y6rTMNunvUsjPtOI30VtaQ6GJ6HDX62FNoaocc0mdol3rFGLrr6E9h7Cne6blpxvUoY6SlcadjRD5rfXBMlt1tezSy2xQN77w3yOeAwdBuKGKulaSmTg5042DRU93r19+wjq6n2wZM520+N21bT+RyxKXjNsw2lKQQKHPnzpWqrsaNG2vfo/7J2rVrE314hmEYjaDlkUs8xLt9RSXhAgUpV5C7i7J9+3ZZD4VhGCb566HwDCUWEj5KyDQ8Y8YMzVU4GAyKiRMnih49eiT68AzDMBFTr8S7MEkwQ4HggFF+yZIlIj8/X9xyyy1i+fLlcoby1VdfiWTFn2cJ3wG9tppB2JHhlmQJ9uaHlcKegL6xR8neSnXYXtIOVX57C6yIf+/vg654DvrDB/Kn6dv6c8Pt+vKCRhuPV7Gp+DNpxli9t2qwGV3nzyHHUQPTyGnTcwsqmWmj6fHVa0FtENT24QmqEXN6OxYNVlXtDo7IQdIngy1By1pNxt9boJ+Mr7LXYB/S21UNJw5TlykrcJQ4MEsJmKX2IGoXw+/FJkhtZj6DnYnaYkif1PvVDpj0kHuESR4SLnbbtWsnfv/9d3HccceJfv36SRXYgAEDZPBjq1atEn14hmEYjYDwFGlhkmCGkpubK6pXry7uuOMOxzq4EDds2DDRXWAYhilmtmFWecVCwkfpiCOOEMuWLXN8j/K/KAPMMAxTmkDrF/8MhUkKgYL6J0cffbSYMGGC/AyV1+DBg8Ull1wibr/99kQfnmEYRoON8ims8po6dapMVX/55ZeL2bNnSzUXim19++230r6SrKTvDgh/WsBpyCQi2J+jv7t4c8OfLZq990BJ4Uh4qWE9X7dkqkZmX65u9fTmFLifCPEE8GeHj+PLMUeIqUZ6asClRu+gUuaVGlr9+9zf72hwnYcYp4MZPvdsvcSJwKeMPQ2CpEbvwipqfdsoAahKW8E0WobYkLE3aHYM8O9Vrluh3r+0TJ97wKwh6FFQxwrqGKIMkY+U0qWGd638MjEh+Ihh3J8d7r8jZIPc9tqYOYz9Htd7xx77QnIt44WTQ6Z4PZS+fftKQ/yTTz4p/H6/eP/995NamDAMwzBJqPL6448/xDHHHCNnJ4iYh9vwf/7zH/lvQYHhzZphGCYBWMITd4JI7MMkgUDp1KmTaNGihfjxxx9lCeD77rtPprd/6623ZGngWLn77rtlUKS6tG7dWvMmu+aaa0Tt2rWlSu3ss88WW7ZsSdBZMQyTqnByyBQWKLChvPbaa6JGjRqh74499lgZhwIPsHho27attMHYy5dffqkloYQq7Y033hCff/652Lhxo1SzMQzDRMrlFe/CJIFAgTdXJKpWrSqeffbZuNqC/aVBgwahpU6dOvL7Xbt2ybZQTvjkk08WXbp0EdOnTxdff/21+Oabb4rUbxgk7cUuBywXr0dbUBJWXeTM2F68+oKyruqiHcPv1ReyrbaQPlheL1mUdWrf5WJqR1+ET1nU84qwaGPip+dGFoxFaCHHNY0D3ZaOi3oMn5csZOyVxXnuwtxHw/hq90OaV1/ocZVF0IXeO4bzlob4Awuee/ri0RftXiYL2RfleO3FcR953Rfad3o/uP62IrWrXNNgWngpiWzD8S7xsnDhQnHmmWeKRo0aSY3KO++8E3Wfzz77TL5oI8/hwQcfLJ5//nkhKrpR/r333pOGeKStx99uYJAx4LGyatUqeXEyMzOlXWb8+PGiadOm4vvvv5f2mF69eoW2hToM6xYtWiTdlhmGYUoz2/C+fftEx44dxZAhQ2LSlqxZs0Z6xF511VXi5ZdfFvPnz5fesQj+7tOnT8UVKP379xebN28W9erVk3+bBEogEFvIULdu3aS0Puyww6S6a+zYsTLx5C+//CKPlZ6erqnVQP369eU6E3l5eXKx2b17d0z9YRiGMdG3b1+5xMq0adOkvdmuEYX6UVDrP/LIIxVboCCbcKS/i4N6YRBhDwHTrFkz8frrr4usrKwit4tZDoQTwzAVA6Sijzcdvb09feGEaqqkynAsWrRI07IACJIRI0aIVCFlE9RgNoKywqhVD3sKMhnv3LlT2wZeXlhnYtSoUdIGYy9///13gnvOMExZErA8RVpAkyZNZG5Ce8ELaUmxefNmqVVRwWcIsZycHJEKlEpgI3SBmLatWLEiNJWD1KXSOB727t0rY1xg9IcRHvYaHAfuwmDlypWy7DBsLSbc3jBya/mEL31/tLJXDZehEcN5XtcIZ2o8DGTon1W1rJr+W26b5h7F7M/Wo6i9hena5/yq4T7lV9H7l65E6/szzO8TatrxvGokBzmJcA7oXdAozDSUIXaUvhWuY0Yj8GEs1totVKPqhWsadFBQyf3cTdHvavR4pM9qNDzNqqD2T+6rjj/pb04tv3tZZ0P5XRrtXlDJ/Z4L0nIKQfexd2SIyNUPlKZkSqBjEnCUbXC/3kFyH/mU62TfY4F8b5nZUPDCWa1atdD3XCSwDNyGTz31VOnVdf3118sFF+S0004TU6ZMibmdm266SboD//XXX9J766yzzhI+n09ccMEF8k1h6NChskY9YlxgpL/sssukMGGDPMMwKlYRimthH4Bnl7qUpEBp0KCBI3YOn3Gc4qj1y9UMZdy4cXJ2Mnz48NB31113nejevbtch2DEWFi/fr0UHv/++6+oW7eurK8Cl2D8DXAMr9crZygwskP3CGHGMAyjUpT6JqVRD+WYY44RH374ofbdvHnzompZKpRAgV0DMxRK7969xa233hpzOwiONAFXYsx44pn1MAxT8YAGM36VV9HU8qtXr9bcglHKo1atWjKkAfbbDRs2hEqkw1148uTJMi0VXI0//fRT6XT0wQcfiFQh4QIFebvefvttcfPNN2vfv/vuu+KMM84QyUraPkv48w+UAFYzq1IbCilvm7YnbHAJHrDB2AQyaT1WpR2i4w+QzMSaDWWfniXYQzLVCosorRXS94S39dNsw7QZLdus+QdoCjZTM9HK/mr2AMtYJjeg2hloVmAE9bntS7INO8ZMVeyTbWm7an/VMYl0ndR7xfKbyxun7Q73yROgWZZJHwoNdid1V5plmdhtVFtIQLF7ONqRWX0j7yf7nkPOZa/i2UnaddgDlf6r9rRIfdJKXx/IREzHMVlZsmSJ6NGjR+gzVPJg0KBBMgQC4Q+w89rAZRjCA1k/HnvsMdG4cWPxzDPPpIzLcKkIlDZt2oj7779fRoDaUzeoqlBP/sYbbxSPP/64pgpjGIYpDxUbTzrpJGERAa8SKQoe+yAtVaqScIGClCg1a9YUv/76q1xUt1819QqCHFmgMAyTaOwMwvHuwySBQIHekGEYJllQ40ri2YdJkjgUhmGYiqbyqoiwQHGh+o/bhN+338fck69YJ4lx2tqnR7AG/t0eHtzKlbR1nsxM/bMvfJMG92UbtxWWYvTctUdfVZCvfc6qWTP8d53w35Kd4X2DNG8ZNfZmhn3sM2qTdqghO93dEcCzZ5/rOosYowU5F5HmHjHp8RODs1qwjfQvuFfvQ5oSnCaCxDLsJz+LQsV4np5u3lZNNUT6J/L0cwvsCGd2sAp0p4GqBzV0bddS+rO/IffSx56qVbTPltones3omGUq50qDNPfkul9j0q7x3iDnbWXo46v+9qys/fdjYSCce6/IKq94vbxY5RUTLHYZhmGYEoFnKAzDVMgSwPHuw0SHBQrDMBWK0qqHUhEpdZUXKiquXbu2tA/LMAwjiTePV1GM+BWVhM1Q3Co1oizm7NmzZRpoO5I+paAR4yQzbamgGOjjXp+o/hoCuGhwF2KOXPvj8Zb5NdX65wzQj/m8o6I6QdBrRselZMoKaVHq0XruUc4tqsrHW0LXjY6n+tm+LlGyNkSDZygpKFBQqRE/zEiRotdee23cFRsZhmFKAg5sTBwJex1E/hlUWUTRGFRttBeknEfZXvzNwoRhGKb8kDCB8tFHH4mePXuKI488Uqq4GIZhkgFb5RXvwkQnoQprZM2ELQVp6v/73/+K7Gw9eI9hGKa0YYGSwm7DnTp1kmmcIVzwtyn7ZjKxq2Nd4U/LdKRFpy8qtLRs+o5whHOBI309iexWPnrzSPpykhZdtaCm79Ijrmk689xq4WjjvOr6MdP21lb+JhHX5JBBJZI/j5akJWnmtbKvljl9vQm11K2j3SiletU+0THx5ei2urzK7re+el1kW+r1J/1xlAtW+k9T+tO06+m7lGh40t/dDTLdU76TdtRjUgorxZ6+nqKtp/d9TlX9s3L/Bkn6f1oOQDW0e2nQP+mTTznXgqz96wL5uULsryZeJNgon+JxKChfOW3aNDlbQYneOnXqlMZhGYZhHLBAKSeBjXARTjk3YYZhyhWY88QfKc+UqQ0FNeD/+eef0OcvvvhCXHTRReL4448XF198sVi0aFGiDs0wDMOUJ4Fy9tlny8qMdrlfVCJDjeXu3btL4/yJJ57I3l8Mw5Q6bJRPQZXX8uXLRdu2beXf48ePF+PGjZPeXjaTJ08Wo0ePTtq68vnVPCEDoedALev9H/TtCg8YCm0sT4Zrbe3CTP2zarykxn1HvW81qPpAWv3QOmLQza/q087DrV1HjXuDcTq3BqlxHzAYsol+II0afw3aBoeR1j3zucPY68tXB0nf1l9J739Bljfi2IIg+VWo50oN9kajPDHgU0cGtf/0GuZVd3fKoI4Lav/oudiG7PAG7vejGhkfzShfSKor+HPDAxP0RXFyUDxbvErFAXlM/dYW3vzwtoWVbKN88d6D2YaSgjMUv98v9uzZE6raiCBHFXxeuXJlog7PMAwTEZ6hpKBAgUrr1VdflX937txZfPbZZ9p6eHsddNBBiTo8wzBMRFigpKDK64EHHpAG+I0bN4rjjjtO3HHHHeK7774Thx9+uJyZzJw5U7oSMwzDlCaWhRyDcXp5caR82QoUCI7FixeLO++8U0ycOFHs27dPvPzyy1IVdtRRR4nXXntNJpBMVtL2WSGdvBbYSHTmakAXSN8TVgoXZunD6wm6B8X5cvR2fNSGolaWzdYNGN4CEjio7BokZWjTcizXdmiAnBpc6c8g9gBiQ1H15lSP788lX6iqeWI7oEF7XhrgqWD5SPBinrtzJw1s1OwOpA8Bckx1PbWLOOwDmg2F9IGcm9onahdJy/a59kG9Fxz9p0NA065rY0+31T+q9z21i/j1ytfafUUPSfdV+0BtKJ6Au13Mvq+8qq2sCHByyBSNQ2nVqpVUeyE6fuvWrTIhJIIa09IMllaGYRgmJSmVwEakqa9fv35pHIphGMYIe3klDi4BzDBMhYJtKImDBQrDMBUKnqEkDhYoLsBoa1c1VQ3QQWK5pAF+QjWuOsqZ0qyr7oZhapykQWfaOrKv3q77ttR47tw2tnXys2rspe06xsEwfiaDc7TSvYZtHQZowzFpAKLWX2pgptdN3dYxvpZhfA2lbx3Bi7GPJ+2D9jHK9VfHhVZmNjoGkChI6lahGu1NY0LX278Jxz0TJzxDSRwsUBiGqVBYRSiYxW7DSSRQdu7cKZ599lmxYsX+IgZIyTJkyBBRvXr10jg8wzAMk+oVGwGKa8F9+JFHHhHbt2+Xy6RJk+R3S5cuTfThGYZhNKBEg9YwroXHMDlmKKjUiBoo//vf/2RQIygsLBSXX365GDFihFi4cGGiu8AwDKMFNuK/eIi3fkpFxV8aMxRVmMiD+v3illtuEUceeaRIVmSm1QPR6l7FEEujgCm+Sn7XMr40g7CabdZbaN5WzdBKoX0qVDLpOkuqelxL1HpIFoCAko3W0XdD5lxHV0kHNUMrMXKrY73/uMq+xBhNsw1b1HKsrdQ/FijZh2kEu+MaK6vpmJlwRIgThYA3U4lEJ04YhSQTtB4xHntJ3UKS4UA9t2C4UnREY3dA2ddUFtmRBYJ2j9xXavdpueAA6ZPalr2umDZ5NsqnssqrWrVqYt26dY7v//77b1G1ql6XmmEYJtFwcsgUFijnnXeeGDp0qEwGCSGCBXm8oPK64IILEn14hmEYjbjtJwcWJgkEykMPPSQGDBggLr30UtG8eXO5DB48WJxzzjliwoQJxcpmjJQusMPY5ObmimuuuUbUrl1bVKlSRVaN3LJlSwmdCcMwDFOmAiU9PV089thjYseOHWLZsmVygacXvL4yMkh5thhBGvynnnpKdOjQweEA8P7774s33nhDfP755zJ1PoQZwzAMDWyMd2GSwCiPeBMIFNhL2rdvH/oe6eyvvfZa8dxzz8XVHurSX3TRRdLQf99994W+37Vrl4x1eeWVV8TJJ58sv5s+fbpMo4/a9kcffXRcxynMEsI6YAT0KiWAqcGWGhxVoyg14DoMpFrZXL3hYJrBMOwjBluSUl0tSxwgpVoLNeMvNZYLQ7lg0h3FuC/7pN5Jjt8ejWhXSt8SXYI61nTMqDGaGr19DiO41rLeruJwQNPMO/pvxVaamW5L26WfVUcMGjGO+8+9XDA9psc1DT69buqYOZwyaEmCDPexpqUYNOhvhG5qSKFPx1dty14XIL+5eOFI+RSeobzwwgsiJ4cUTxBCfjdjxoy424NK6/TTTxe9evXSvv/+++9FQUGB9n3r1q1F06ZNxaJFi1zby8vLE7t379YWhmHKL2yUT8EZCh7MqIOCBbXlMzPDr0qBQEB8+OGHol69enG1CWM+giGh8qJs3rxZqtdq1KihfY+0+Vjnxvjx48XYsWPj6gfDMKlLUYzsbJQv4xkKHuy1atWShvNDDz1U1KxZM7SgyBZUYZhtxAq8w66//npZ9VEVTsVl1KhRUl1mLzgOwzDlXaDEa0Mp2rGmTJkiHZHwzOrWrZv49ttvXbd9/vnn5fNSXUryWZfSM5QFCxbI2QnsGbNmzZLCxQYziWbNmolGjRrF3B5UWqj6eMQRR2gzHUTaT548WXz88cciPz9f5g1TZynw8mrQoIFru3AMiOQcUFDZI4IH9Pe+fGGwobgHeFEddYAcRg3qojpqi14ZNYkxCQYLplmOvkf6m0KDyqg9IKDYcWg7tHSr1l/Hj8/0ayR9IIF5anAlLVHsHAd33Ty1dalBfUEatGnIrKsG++3fWMRcLthR3lixZ9Hs0qodjPaXBhXqxyftVPK4BwqS+5GOWTCOgF61YRowabJnm+w2dAztdgNGW1ny2FBmzpwpRo4cKaZNmyaFyaOPPir69OkjVq5c6aqdQdwe1ttAqKQSCRMoJ554ovx3zZo10o5R3IHp2bOn+Pnnn7XvLrvsMmknufXWW0WTJk1kaeH58+dLd2GAC4OgymOOOaZYx2YYhomXSZMmiSuuuEI+pwAEywcffCAdkW677baI++A5aXoBFhXdywszkZIAXmLt2rXTvqtcubKMObG/RwAl3ggwG4KkhxcZhEm8Hl4Mw5Tz5JBF2AdQpx03DUd+fr7UqkClbuP1eqXTkMlJCF6seGYGg0GpjRk3bpzMzp4qJNzLqzRBbMsZZ5whZygnnHCClPRvvfVWWXeLYZhyEocCTQjKbtgLnHoi8c8//0iVPJyCYnUSOuyww+Ts5d133xUvvfSSFCrHHnusWL9+vUgVUrrA1meffaZ9hgELRjAsDMMwJT1FgdMOtB82RQ3OjgS0Kap6HsIEcXQI4r733ntFKpDSAiWRwADooZlPI2aidQ8Oo4Z1R8ZetRQqucGpQVddHyBBcNTgrBqOqYE0oAYO0jKzjj6oberrHM4JBqO8MQiOQLMPa8ZoYrimGXBVw7CxdLB0ODBkb/abjPJ0Y7Kt0keHYwWxIxYqballpiMattUSwMQZwehYQfvriS141nGNyfUOUgN+wHCvUL8AQ8nigKFP9v2oHqtIFCXy/cD2ECaqQHGjTp06wufzOVI/RXMSUoFNuHPnzmL16tUiVShXKi+GYZhkSA6Znp4uunTpIp2EbKDCwudYnYSgMoMjUsOGDVPmoiZkhgKpGqtXF1dtZBimPDJy5EgxaNAgWfepa9eu0m0YKadsry8kzD3ooINCdph77rlHOhAdfPDBMvzhwQcfFGvXrpWZ2Su0QOnfv7+WAXjq1KmiTZs2IcmM3FrLly8XV199dSIOzzAMU+ZxKOedd57Ytm2bGD16tDTEd+rUScyZMydkqEdIAzy/bJBAF27G2BYB4JjhfP311/LZWaEFypgxY0J/Q7ped911DqMStuGodIZhSh0IhyLaUOJl+PDhconFqQheqlhSmYQb5ZFKHmWAKRdffLGcCsabbbgs7jmn8VfZjhorFaOiRQzrDoO+ah/3RNlWMYLS/liOqHD3drR19LxM0eU0QJz2wT2JsdH468icG0+7tHywx1AemDoy+N2j8x0Vdv3uGYOpUVk9H8fYG7IhUL8FU7S+47qp+1JjueEX7mjHYJS3fNRjg0TyFxjGjx5HdTCI1idPhL+LGTzOubxS2CiflZUlvvrqK8f3+C7V8tQwDFOO3IbjXZiyn6GgouKwYcOk8R2GKbB48WI5M7nrrrsSfXiGYRgNroeSwgIFOWtatmwpi2wh+hMgWAfFr84999xEH55hGIYpT4GNEBwsPBiGSRpYhZW6AgU+1W+++ab4888/xU033SSTN0IFBvc5+GEnIzCY2kZTLajJimJoNaWkN6XdjmLA1SKcaRS9I6278jc1ZKvGaGJYdaQSNxijSdZ+vb/RjLLqpjQNvmHMHI4KJocDh7XXPdW9Lxj72DuM5YZxiOY8oZYP8JCGTAb9eNLBm8Y+6CeZEmgmAmW941xMx4niYKD1gXxWj0nHyD6G8XcUA6zyShwJFyg//fSTzLCJRGp//fWXdCOGQEHSRvhhF6UMMMMwTJmkG2bK1ssL0aKDBw8Wq1at0ry6TjvtNFkci2EYpnTxFHFhynyGgvrvyJZJgarLVOudYRgmIfAMJXUFCtI706I04Pfffxd169YVyQr0yx67tK5ia3DozE3693jsIlF030Y7hGFfky7eEaRHXsK0MrmkzLDjjU09jiHQUn5W4yVp5lhDxmM69uZAPLLO1G4cY++wSQn3wExTcKJcr44/sV+Ysg07szm7H8NxD6r2IMevn9gv/IZzIYGOahCsw8ZjsnVFC/70Oe/BILEZMhVI5fWf//xHJj0rKNhvfUXSSNhOULbXLtXLMAxTanBgY+oKlIcffliWtaxXr57IycmRteaRTRMlfe+///5EH55hGCZyXqV4l3JGQUGB8Pv94pdffkkdlRe8u+bNmydTrfz4449SuKBWMjy/GIZhShvO5RUu4NW0aVNZdyXlKjZ2795dLnZcCsMwTJnARvkQd9xxh7j99tvFiy++KMM5kl6gTJgwQTRv3lzWBgCImJ81a5Ysg/nhhx+Kjh07imQEBkvbKKllo43DKE8DG02ZX2npW2NAGumDl2bWNRmnVaNxFNumMUiP7KwZ2qNkTla3dTgGGA3XtIOx95eOmXpcL8k2TPtkzNBM0TIek3bJuXn8BocIEuDnVTeg103NREzPk2YJNgRpOjI9kz5o6+g9Z3JcoH1QIWPtCKCNEFzrdCZI3vT1yc7kyZNlieFGjRqJZs2aicqVKxerAGLCBcq0adPEyy+/LP+G6gvLRx99JF5//XVx8803i7lz5ya6CwzDMJq3oMNjMArxbp8qqMUQS4KECxTEmjRp0kT+PXv2bDlD6d27t5y1dOvWLdGHZxiGYWIohpgSXl4oZWlXZkT5S9sYD5VJSRqDGIZhYoLdhjVg037mmWfEqFGjxPbt20Oqrg0bNoikm6EMGDBAXHjhheKQQw4R//77r+jbt6/8/ocffpDuwwzDMKUK21Bccy2ipn1xci0mXKCgRjLUW5ilTJw4UVSpUkV+v2nTJnH11VeLpEVJ36MZJ2lGVocxVVlniBAOHcNeR42TPoOx2heHEZlmb1X7T437cZQSdjgYqOsDhmMC1SgvzEZ5NULfS6PzDQ4RjrGmY2TIpEyvhbEULrWma0ZvmgXaPeOxWr54/xekD+q50na12sdkP3qdDP2j2Q+0MaTtGjJK0zGKK8s2vV/Vktre6NmLY4K9vBy5FvFsRmygmmsRE4F48ZeGrzNS1lNuuOGGRB+aYRjGCQuUhOVaLJU4FGQaXrBggdi6dasIEt/J0aNHl0YXGIZh9sMCJWG5FhMuUP73v//JmvJ16tSRsSdqjAL+ZoHCMAxTNti5FhHGURK5FhMuUO677z6ZswsdZBiGKXPYKK/lWjznnHO0XItQdR1zzDFFyrWYcIGyY8cOMXDgQJFyqEZ5zQhoMIhGK9VqSiUepbytFg0dT8pvahBVjNyOkq+0HW8cZV3V9dH65zUY2gNxRKlTR4Ggob8OI7Lleg0dDgdKVJvTIGyIJo82DkFDKVzH/aAcx2PoHzlRU4B31HMxBYebHExM9xEJEjQZ9x2f7XaKaZTnwEZnrsUvv/xSenwVN9diwgUKhAmi4a+66qpEH4phGCY6bEMJkZubKyvpHnfccXIpLgkXKIg1ueuuu8Q333wj2rdvL72+VK677rpEd4FhGIaJQI0aNUTXrl2lqqtHjx5S1ZWVlSWSVqA8/fTTMvbk888/l4sKDEAsUBiGKXVtdry5vET55JNPPhELFy4Un332mYwZLCwsFEceeaQUMCeddJI45ZRTkkugrFmzJtGHYBiGYYqArepCCnsIEzsuBYGODzzwQNzpsUqtHkp+fr4ULq1atZJVwlJJzaq+nUQzngdNrzWGwOS4smNHMZBr0frUyKn+TdK209TnJqO8CZMBnPaJpsE3DYQzslu41zWnmQbiqT9vSvke7Zoa+0ePY9iWBuAbjN76MfQOGbtrcPxwEMXJQVvnK4aDiSF63z5msTPJs5eXI+YEMxR7ycvLE2eccYacocRLwp/s2dnZ4tprrxUvvPBCqPMtW7aU3yEa87bbbkt0FxiGYcKwUT4EnsFwF4bwwILwjg4dOjhqGiVNtmFksETpX0g+eBPYwC1t5syZMbfz5JNPyhOtVq2aXGA8Ql0V1VvhmmuuEbVr15Y2GwTlbNmypcTPh2GYFIezDYdANDxe+hF7ggXPTAiYopJwgfLOO+/IqmDQ06lSr23btuKPP/6IuZ3GjRtLnd73338vlixZIk4++WTRr18/sXz58lBusPfff1+88cYb0vi/ceNGmemYYRgmUhxKvEt5ZNmyZVKQQFMEVRdsKchqcuyxx8rywEmn8tq2bZuMwqTs27cvrmnVmWeeqX1GFCdmLXBHhrB59tlnxSuvvCIFDZg+fbo4/PDD5fqjjz66WIGNju9jDfCjq+hNacjm6wgGM6xzZMdVM7bSzLRBtVZv7Lpvh05fxI6pBLDjPE0Bc46AvthLAIt4bChxXNN47BDRsg8bj2kKglXbpGMiTPccsbfQ0s0ew81ryHgd1S6i3oN0nclWY/ehuE93Vnk5XIeRgqV79+5SkLz77rvi1VdfFYsXL447Wj7hMxS4oH3wwQeOmxYFXaC2KgrwPHjttdekUEIbmLUUFBRo0Z2tW7cWTZs2FYsWLSqBs2AYptzAKq8QqHuC0A2YE+rXry/zLiJaHilZ4q0nXyozlHHjxsmiWr/++qt0S3vsscfk319//bUjLiUaP//8sxQgsJfATvL222+LNm3ayGlbenq6lLQqGKBoKZgxzcNiEynzJsMwTHnkqquuEieccIK48sorZewJgs+LQ8IFCmwneODD/oHOIg0LcsVg5hBv5w877DDZ1q5du8Sbb74pBg0aFLdQoowfP16MHTu2WG0wDJM6cC6vMCgpUpKUSkAIYk+Qxr64YBZilw3u0qWLDMLBjOe8886TcS6ojazOUuCxgJT50bzQULVMnaE0adKk2H1lGCZJ4TgUhwkBzlMrVqyQn6H1gcOTz2cqtVmKAiUetRFcgIsKinVBXQXhghxh8+fPD+XwX7lypczrH81OgwIzWCgIbgsFuOXHboLWDOI082uEY9j4DGVy4y3Ha8p4rNldTYGMFEdkoLsBtzhxZw6jsqldj/vYOwzDhn2DpOxsPJZFatD3qoHFdMioIVsYrj89jhbYaDBKU0cXeq8o2aajOmV4DNumk1K9BWpQKXE+oPegKYO0iCG4ttiBjfEFpIb2KYesXr1alvvdsGGD1ADZWhu8VMP2jclAmRvlMUuoWbNmTEusYCaBnDN//fWXtKXgM2JbLrroIpmCeejQoXKmgcqQMNJfdtllUpgUycOLYZhyS2m6DU+ZMkU0b95cxuB169ZNfPvtt8btEfYAhyJsD5PAhx9+KBIJDPIQGn///bc0wmPBi3iLFi2KlGcxITMUPNRtIADg4zx48ODQbAH2E0TOQxLGo+u79NJLxaZNm6QAgVfCxx9/HEpehsRmXq9XzlAwa+nTp4+YOnVqAs6OYZiUppRmKDNnzpQvudOmTZPC5NFHH5XPJWhPIoVSwFHpggsukM9FpD5BGET//v3lQ75du3YiEcAGjdCKWrVqhb5DcDhs3nAjjheP5UimVLL07NlTXH755XKgVDBYyESMWUYyAXUdBFaz++8T3gOR/b4cd72LV5nqO6b7UZIOaSqvPHOxK0+BYR2JfSisEm43QFQT3sLwcfz7ougOlNUFVfSDeNRYAqqaoqEFNGeYMi/2knW06JeqovHQPHU0hiUQo/qOjKGjf3qFhbjw5sd2TIkypD6iVi1QriE9tyC5piq+XDJ+6cJ1PDX1bKSCa8aiXuS4+7yu7dKfgXoP0nOh6jJvrnqz7F8XzM0Va2+/UzrnxKMyt3/bLUePC/22YwXH/POe2+M6Zrdu3cRRRx0lA7tlG8GgVCUh7VSklFOwBSMUYvbs2aHvoGHp1KmTFEqJAIIEx0P8icpXX30lY/+2b9+eXHEomI0gFoWC76JN/xiGYUqcoqi74nztzs/Pl6p3NTYOGhR8douNw/e0UiJmNImMpcNMCC7DCGLE3AILZixwJ0awY7wkXKBAIkfy8EJgYzJ7U+EN114iRtDbCwmSUvezDfshAz+5S/HGGlq8dLG0RTumV1+CaWTxhRfaX9vBJeLkiWwb9FmhxdSObEs9ryjHUZtytEvHTD1Xeo1oPxxjqFxDet1UyHjSuDftmFGWuFCPSftrGLR4zpPeRzGPSRQwC1EXzHzsRbuvI9yDWjv0XqEDqt5XpvGJq/NFXA7MctRFjWFT+eeff6T3FGLhYo2Nw/fxbF8SPP7449JzFjMU2G2wQNWF7+BBm3Ruw7BtwK6BRI6YAgLMTFatWiVmzZqV6MMzDMOUmA2FvgSPGTNG3H333Sk3wsFgUDz44IPivffek7Mp2GoQ14dMJkhZZYdnJJ1AgUsahAcM5L/99pv8Dro5TKmSeYbCMEz5pDiBjfCGUm0okUIOABIsIo6DZjw3xcbh+3i2Lw7I0QVBCBUbSv7Cmwz2peeeey75AxuRvBEpWBiGYVIZu3xGLEHYXbp0kbFxePu3ZwX4PHz48Ij7wAsW60eMGBH6bt68eUXOeWhixowZ8iX/v//9b6gU8Omnny5NEbD1JLVAQQQ71Fxw/cWgqsAVmGEYprwxcuRIqUaCA1LXrl2l2zC8uBAjZz/7UODKDp+4/vrrZT4tJGbEwx0JcFGqA96wJQ1iTaA9ssFMBeoulP3ABCBpBQpqlCD4EBksIdnVFNn4OyUEiiGFOnXZVXGkZqfbaqm5zfuqSCO5ui29igZ3T90t05CuPEoUM3Xv1FQIhjT9jnaipIp3722kPhk2LkxM5LOxT3RAg+6lkLUIdrmyaJ2INp7qdbJMJX+Ja3i08staH6g7suImvH8DQzuGOgn2uRW/BHDpxKGcd955snzH6NGjpWEd7r9z5swJGd7xUFdnAzCMI5zizjvvlHVJDjnkEJkSJRExKEjUqxY8BMg2gqztxSHhAuXGG28UQ4YMkSqvSpUqJfpwDMMwSZMccvjw4a4qrkgxeAMHDpRLooF7MILNVRsQsrjDtl25cmUtvX1SCRTkiEEIPwsThmGShnKamytWoIqjXHzxxaK4JFygIDAHesCWLVsm+lAMwzDR4eSQAhVtE0HCBQqMSzfffLMsqoVkZ9DTqRQlGrPUMdg6jG86nnhK4RL9uqkkcJRsw/Sza5cc6TTc+xA1lUlRh8hU8pU2Rm1SprK+kYIXXT47kg/FofJ3ZBTWsvREKW9ruKZG44xhnRXHPRcV00V12PEs92zINKWLuj6KLsmKo0x4rHA9lMSRcIFyxRVXyH/vuecexzoY5RFNyjAMU2rwDCV1BQp1E2YYhmHKJ6USh8IwDJMssMorcSQ8OaSdcx/pVpAfBgvsJl988UVpHJphGKbEkkMyZTxDeemll2Rk6IABA0IVwJBrH3VSnn/+eXHhhReKZEdm7XWzIZqM5wajMTVOWj5z6VatrC9dR4Pi1MAyonFUg8xogCTFdExax0Qz9joKohgMq9SE5jOMEanBQuuYqONNg+scJYt9MQackj44aoYY7O40MNR5/ZUPpA8OJwjLEFSqjovpfgRaiWqyLh5zpl/vsJVuCk7UDxRMc++vw/gfdF6nosaElHZgY0Uk4QIFScgmTpwobrjhhtB3ECyTJk0S9957b0oIFIZhyg+s8kphldeff/4p1V0UqL3WrFmT6MMzDMPosMorYZRKgS1k0KQguyWnr2cYhik/lEouL6i4li1bFqpbDBsK7CdFqQjGMAxTLNiGkroCZdiwYbJADFIyv/766/I7VASbOXOm6Nevn0gFNANuIPbIZFMkN/3sWEf7oETSO7YlBmjNiGwKjY4ShKwZf6mRm2aQNZwLNXpr0eRRxkjLwku29dE+qO047mz3LAB0jBxJgpU+OJLuEocDLYNvtAzSysk7DPg007PariP63XLPkkAj2n0Ghw3qPGE4Fy8xygfVQaMJa72mlMf6KkdWY+V8PPmeEjGQsw0lxeNQzjrrLLkwDMOUOTxDSV2B8t1338loebuevM3ixYtliUwUn2EYhikteIaSwkb5a665RtZhjpTWHusYhmFKFfbySl2BgizDRxxxhOP7zp07y3UMwzBM+SDhKi9UBNuyZYujHsqmTZuE358aqcS01NyGiGv5Oc09stthgVbFuSltO41aN6QOp8dxOgLEYaT3Go5J2tGj/mkJWGqlD/8ZpGWIqYOBMp40oN2XZygXS/tLDM7GaGuaxj9dOTcSKe/YVr3mUdLiqwTTohjTleM60sObSkmnGZKzGq6hXK0ek3aPRsoHvO4p502ZBwqjbKvcD56CAytTpARwRSThM5TevXuLUaNGiV27doW+27lzp6yZfMoppyT68AzDMBqeIi5MdBI+RXjooYfECSecIJo1aybVXAAxKfXr1xcvvvhiog/PMAyjwzOU1BUoBx10kPjpp5/Eyy+/LH788UeRlZUlk0VecMEFjuqNDMMwiYa9vBJHqRgxKleuLK688kqRcspAb4SgQhqkZwhIc5SAFe764SDNNizcgwMdAXI0GMwRfRf5mA57isPeorQTrUSxavsgensrz92G4rD/0L5nhNuyPHo7NIhPCxSldgb6WdHdR8sKrGbWtdRMuZEC8dRBjJLF2GjrcPRf2dlkM6MXMd0QVUoCEJ3j4HE9T69Pb9ejfKaZs62g6RpHsTNGyMjtHPM44RlKatlQvvnmm5i3zc7OFsuXL09ENxiGYZhUFyiXXHKJ6NOnj3jjjTfEvn37Im4Dl2EY5lu1aiW+//77RHSDYRgmMlxcK3VUXhAWTz75pLjzzjtlvZNDDz1UNGrUSGRmZoodO3aI3377Tezdu1emY5k7d65o3759IrrBMAzjgG0oKSZQYGxHhmEsS5YsEV9++aVYu3atyMnJER07dpTFtnr06CFq1aqViMMzDMO4wzaU1DXKI1dXKubrglHXNhirRkCHndUOtrL3Uw2GtGStZTCQ0zK+9EBqQJ0jAJEEEiqGWBqIp25p+dyN5fvbNawzGE+9GXpKZivHZ8hMbDaw+jLDdX4DxAofyDRobOkqauRWro2W0VhuK9zHkwarkv4HDSWg6WetnDA1tDtKGCvjm66Pb1ANHCUGcB+5FqpRPqDWtpYdIpuqbdES0PSe8xn6Z/BbsAoNwb5oV3FWsNL3r7QCxTPK8wwlcaRGqDrDMExJwTOUhMEChWGYCgXPUFI49UpJMX78eHHUUUeJqlWrinr16on+/fuLlStXatvk5ubKDMa1a9cWVapUEWeffbbMI8YwDMMknpQRKJ9//rkUFohxmTdvnigoKJB5wlS3ZBj733//femujO03btwoBgwYUKb9ZhgmyeD09amr8vrzzz8dmYaLwpw5c7TPqEmPmQpiWJArDMknn332WfHKK6+Ik08+WW4zffp0WW4YQujoo4+O3yhvGxrVyG5iwA0qmWglalbgKEZZUwS2oLZULbLbbMj0p4cN2ZZmJdZt+7TvjlK9quHVYYTXP6rR8X5ilM2nRm+1LXouxIkgLS3cVjBNPxdLiaLf/4XaP2rkJtH7apZbei5kX7UP1IbsoRkEVEcLasCn3S3wRjT8S6jRW/nsV/oDChRDO41KT1fuBdkF1RlBOb7cl9yw2kfiJOAjkfJeb9C9LLKSiXj/ejVbA7X2k4h85bgB+7rQaxsvbENJ3RnKwQcfLF2EX3rpJamSKins7MW26zEEC2YtvXr1Cm3TunVr0bRpU7Fo0SLXdvLy8sTu3bu1hWGY8m9DiXdhkkCgLF26VHTo0EGMHDlSNGjQQPz3v/8V3377bbHaREnhESNGiO7du4t27drJ7zZv3izS09NFjRo1tG2R1RjrTLaZ6tWrh5YmTZoUq28MwyQ5rPJKXYHSqVMn8dhjj0l7xnPPPScLax133HFSEEyaNEls27Yt7jZhS/nll1/Ea6+9Vuz+2bVa7CVSuWKGYcoPHssq0sIkkVEe1RlhIIfBfMKECWL16tXipptukjOCSy+9VAqaWBg+fLiYPXu2WLBggWjcuHHoe8x+8vPzZfEuFXh5YZ2pomS1atW0hWEYhkniOBSkYMEMBbMKpLOHMBk6dKhYv369GDt2rOjXr59RFQaD4bXXXivefvtt8dlnn4kWLVpo67t06SJTvsyfP1+6CwO4Fa9bt04cc8wx8XfYrUwbLc1KDNuawZyWh/UYIrDpShKBrxrQ6THV1OEgKzOcl5y+V+V6wzVoCjJ0a7Q3nxpIDedNPvuUc1GN2CCfpGb3mMaI5FD3+8NtFZCS0QFiaFcH2EMcAxxR4Krxl5YsJv1Nz9AN26Y07oWFPlejvCNrgfKZRpc7jqMYvamhPaAYvWlUekaavm1BINy/QlpmgAbne9yN5Zm03QJfZAM96Z9jXBwOJsQor7QVsNdx+vqKK1Cg1oK3FR7up512mpgxY4b813vAuwOCAR5bzZs3j6rmggfXu+++K2NRbLsI7B4o2oV/IaBgq4GhHjMNCCAIk3g9vBiGKb9wYGMKCxRkHR4yZIgYPHiwaNiwYcRt4P4Ll99o7YCTTjpJ+x7CCm2DRx55RAoqzFDgvYUU+lOnTi2xc2EYphzAbsOpK1BWrVoVdRt4Zw0aNMi4DfWRjwTS40+ZMkUuDMMwkeAZSgoLFMwgkAZl4MCB2vcwzqNaYzRBUmZAT2vrag26ZEcJW00nTBT3ROfvVT6rAWeyXVKHNqhGxVG9MzlMpYx8EQsFGen6MWkgnmLfILFqDhuKX9Gpp/t1/Xp2mvt5UzMVtSSouvqCNN2+ElSDEwk+h31A3zYUJIc+0KA9sm9WumKTopeblsJVDhNQswDLBMde1z6otqJI+JUxo+Nb4A+PSwG5hmmGdgvIOocNxWCroH3weDIMQY8kGFj5aBl+E/Kz0pZte9NscOVkhrJ9+3apokemD1vLAu9YPDvdgLYGGUFUEJYxbdo0UW69vBDnUadOnYhqrnHjxiX68AzDMEnPRRddJEuhI60UvFgXLlworrzyyqj7XXHFFdJD1l4mTpwoyvUMBV5W1CMLNGvWTK5jGIapyCqvFStWyNRS3333Xah21BNPPCGdlx566CFZ7daNSpUqGcMiyt0MBTORn376yfH9jz/+KLMCMwzDpEqkPE3TBOef4rJo0SKZ4UMtRIgUUlB9LV682Ljvyy+/LDVACBRHkDbMCOV6hnLBBRfIUsBw9UUSRwC93/XXXy/OP//8RB+eYRimxGYcNDXTmDFjxN13312sEd68ebN88aaB4Ah/MKWNuvDCC6WmBzMYvLTfeuutMjzjrbfeEuVWoNx7773ir7/+Ej179pSDZOfiQnR8UttQFKO8arR1OJtRw6V2p5ISwNQ4rW5L1tEKwJY6mYwS2JWpGEy1Y5DP2ZmZ2roAyT6rZVm2Ys/I6ydGWZ/B0OooJUv6WyUj/AaYpwQNgsICv+u+aST4r1AJvAMBj881MJRm881KCxvl80kf0h2GbaXELglkpAF/Qb/XNRiUjkOGchyHUV4JVgyQ/vnIMS2fx/WYFMsKuDo1pPvMBn2tD44x8rsa4X2kT6ozQsGBv+n1iht0Nt5UKge2R2omNZsGMm24cdttt8msINHUXUVFtbG0b99ehmXgOfvHH3+IVq1aiXIpUOASPHPmTClYoOZCECJOHpKVYRgmlWwo8aRnuvHGG0Mxcm6gtAdsIFu3btW+LywslJ5f8dhHunXrJv9FWqtyK1BsDj30ULkwDMOUKaXkNly3bl25RAPZPJCDECU4kEIKfPrpp1KTYwuJWFi2bJn81y2AvFwIlEAgIFOrIMcWpDAGSQUDxzAMU1E5/PDDxamnnipdgBFDgrpOSIILG7Pt4bVhwwapzkLqqq5du0q1FlJRwRMMzk2woaBiLezUKBdSbgUKjO8QKKeffrr0RPDQCDmGYZhSBJVJaXXSWPZJJC+//LIUIhAadmDj448/HloPIQODu+3FBVPCJ598Ih599FFZBh3OAtjnzjvvFGVJwgUKsgu//vrrUpKmFDA625G5quGQRD87yvGq8pIoamm5WPWzl5jhFbvl/s+qgZfKZNKFdG/A1SirQg3XARKJrrVLjLL0XNJ87sekGXlVQzyNEKfG6Mpp4aj/nPRwpmSQl5fm2i419juU5sp6EsAu0kkm3Sx/2Ci/z6cfkxrIA0qkeoGX1hZ2zxJNnSfoGKrH8Xncxz6X7JdmuP4+R/901D4FqVFeucco9BqaroXHSx0X3HVL3nKcbbhWrVpyxuEGkueq6acgQGiUfDJQKkZ5lAFmGIZJBpItsLE8kfDARng6ICdNLMkdGYZhEo7tNhzvwpT9DOXLL7+U1RU/+ugj0bZtW1kES6Usg3AYhql48AwlhQUKUgqcddZZiT4MwzAMUxHS16ciSJEdSpetGAEdE19aslRNSV9Ihpdmh1cVs7T6LpliB02GSFo+WMFPy7EqbttpxPic59dnj5ZaujWKElk9F0d0vqHv1IBLU59n+goiRqyDPcSgrx6HlgOgzoXqZxrBrqarp0ZvCo0YL/D5YkptL7c1VBmg46ISMFxvisNBQmk3j6wrDHpdU9+rzgaR+qeemyeakV75TM8ySMoFq/d9yLmj2JHyyWeULy8k3IZiR33Cxe2pp54Se/bskd9t3LhR7N27tzQOzzAM41B5xbswSTBDWbt2rQzaQap6ZOY85ZRTZKJI5LjB57IsBsMwTAWkGLm8mDKeoSCwEWmZd+zYIfN42cCuguh5hmGY0oRnKCk8Q/niiy/E119/LeNRaKAO0gkwDMOUKmxDSV2BgtxdyOdFWb9+vVR9JSsw8HpjqF3trIGtGIZpvXmCZsh0KGmJQdfrda/9bjB65yupzaMZ7Gmtbksr/u1eb172VjHo0qhqem7UYG5CNeDS/lKjt7o+L2i+tY3p1g3GaprGnX42GeGp8V9dH+1O8xiM5+rYU2j/1GvjvC7u7fjIPRawSMYI1b/EYLCneGh6fce5caqmVCLhKq/evXvLfDM2yOUFYzwK06RcOhaGYVIeVnml8Azl4YcfFn369BFt2rQRubm5ssrYqlWrZNnKV199NdGHZxiG0cHMW519x0K821dQEi5QGjduLAtrIUkkUixjdjJ06FBx0UUXaUZ6hmGYUoFtKKldYAulfy+++GKRSkC/bOuYNT0uDWQkdge13KmVQfXtwvUztQdQbaRm3yB6cVpiN6ho3PMK9GDFDCVrrSk4zRHQme+uM6dlaL2FQbMeXwleo3r7QpLNOTeQ5mo78Bj0/A5bB9XFK8cx2RnA3vyMiOcJ8sjnAqUELx0jmiU4Hk9UddMCEvynjQvpey4NrtWOT86bjFGhchx6r2ST+0q9ps77ithqlOPSctFBR/Frpb8H+ue4lnGCveNODlmsI1YcEi5QUBDGBGrLMwzDlBoch5LaBbZUUCgGRWLgRlypUiUWKAzDMOWEhHt5IaBRXWBDQeWx4447jo3yDMOUOuzlleK5vCiHHHKIeOCBBxyzF4ZhmFIzyse7MMlhlI94YL9fJohMVrz+QMjAbimGVpo5VzXCA79mINez+RYq7dC2aLseUsRaXW+lkfKwGQWuRlpqlFWD9kj1VeGlWXWV2rg02yxFNTBTYy81yqrrA8TAHCBjlFOY5moQp6hlcrM9pDwwdXrQSgCbs9eqwaH03HLz9eNoh3BcU+qVoayj+xqGmwYZascg67JJ/9Rm0w3ZmkWU/uST+0o9bjSjvBasSh1KDPdZ6DdB9okXj2XJJd59mCQQKO+99572GZUbN23aJCZPniy6d++e6MMzDMPoQB7FK5OKmTG/opBwgdK/f3/tMyLl69atK04++WQZ9MgwDFOa8AwlxXN5MQzDJA0c2Fi+jPIMwzBM+SPhM5SRI0fGvO2kSZNEspCWFhS+tIBLFLu7IVMvF6sPLzXoqiVWo5n8VOM/7U8lYpRXjchqtHOkPmj9OXC+4eOE2831kshoGrXuic1ovH/b8PrCAp/RKLs3P909+t1QjpdeF7+POEhk+l0NwzSivdCQoTevQL/GlTPzXbMLU9SIb5rh2E8cJNTockfZYW94DAPk3sjODkf5y+Mo66tk5pFj6P1LU7al2QOok0YGuQdV6HXzK/eZo7yyL3op5EDA/VgxwYGNqStQfvjhB7kgoPGwww6T3/3+++/C5/OJI444QrOtMAzDJJqilPTlEsBJovI688wzxQknnCDrnyxdulQuf//9t+jRo4c444wzxIIFC+Ty6aefRm1r4cKFsr1GjRpJAfTOO+84PMhGjx4tGjZsKBNP9urVS2Y2ZhiGUR4URVuYshco8OQaP368qFmzZug7/H3ffffF7eW1b98+0bFjRzFlypSI6ydOnCgef/xxWad+8eLFonLlyjJ1PtLmMwzDAISzFGVhkkDltXv3brFt2zbH9/huz549cbXVt29fuUQCsxMU8rrzzjtFv379Qokp69evL2cy559/fhHPgGGYcgXbUFJXoJx11lnisssuk7ORrl27yu8we7j55pvFgAEDSuw4a9asEZs3b5ZqLpvq1auLbt26iUWLFsUtUDLTCoUvbb+FMF+J3qbGU2qAVo2pNDV3DonepoZXE2pEeUZaoWuEOI0oN6Vmp+dC21X7T43w1B1cNZ7SdlXnAwqNlLfIpFkde4rJwYCOCa6n1q5iTKfn7TAUG4zRBcSpQDt3OmZUa2JwZEgjY6heN7UEAchRzoWOdX4+cQxR+l8pzWzcVo9JVRkWsXlmKMc1RcZLlPuK/kacv6+g4772GJxkmHIuUKB+uummm2SlRhjm5UH9fllk68EHHyyx40CYAMxIVPDZXheJvLw8uagzKoZhyjEch5K6AgUp6qdOnSqFxx9//CG/a9WqlbRvJAOw74wdO7asu8EwTCnBkfLlILAR+buwINMwhAlsHiVJgwYN5L9btmzRvsdne10kRo0aJXbt2hVa4IHGMEw5hr28UneG8u+//4pzzz1XugbD1RduvC1btpQqL3h7lVQ+rxYtWkjBMX/+fNGpU6eQ+gr2mmHDhrnul5GRIRdKpr9A+NO8Dr05zdDqpwFpyueM9DyjfrhQsXVUTg8HxEUKJFOz2kYLHDSVbtXKr1J9dbq7bjpaAFrA0C4NoCswBF7SQEF1jOj40SBINaCzaoY+nlXItcjOCwdMViX9yyBBkDUzskN//+Opoq3LUdqh555jyERMSzfTc8vykwzSVnic0rzuNp6s9ALjfaRmvK7s18eIlljOUcr80naojSqg7EuvP7V15RlsklnErqO2tf3AvREt83VU8JOI1wzDXsPJMUO54YYbRFpamli3bp1Uf9mcd955Ys6cOXG1heJcy5Ytk4ttiMffaBvCasSIEdIdGRmOf/75Z1kNEjErNEElwzAVF1vlFe/CJMEMZe7cueLjjz8WjRs31r6H6mvt2rVxtbVkyRIZEEnTugwaNEg8//zz4pZbbpGxKldeeaXYuXOnrAoJoZWZmVlCZ8MwDMOUmUDBA16dmdhs3749oqrJxEknnWS0vWCWcs8998iFYRjG3csrzhkHT1CSQ+V1/PHHywBD9aGPGAZEtauzDYZhmFKBjfKpO0OB4OjZs6dUV+Xn50u11PLly+UM5auvvhLJCoyFtsFQNSpSgyM1VqtBXJXT8lwNq2C3YkSm7fhIroedPne1nWoQpYZ4R5lfbZ25VKvabiVi5KZGWtXITaEBdAXqGBK79e4o56b1gaxTjf1V0vT+ZiqZk2lAHTWAZxKjfCVlveN6G+4HanCm56Jm3aVjT4MXhVJyNy/gdx2HdGqUJ59zlePkB80lldX+OoJVyWePwbBOUQM86XmnE4cDfRyyRImArsdr1+dYyuSYobRr105mF4Y9AylRoAJDhDwyECMehWEYpjRho3yKChRExmN2snXrVnHHHXeI119/XXz44YfSEwsZgRmGYUqdJFR53X///eLYY4+V9uYaNWrEeBrJl109oQIF7sI//fRTIg/BMAyT8gIlPz9fDBw40BgzlwrZ1ROu8rr44ovFs88+m+jDMAzDpCxjx46VMXvt27ePaXuaXb1Dhw7S+Wnjxo2OOlHlyihfWFgonnvuOfHJJ5+ILl26OHJ4JVPZXzfUKHBqyKZG+qBhW2r83RHMco1SppHzlRTjKo3A9nr0fVUDOjUEq8ZVui7DkGW3Bok035Of4RrhTt/laET2HivDdUwKSJ9ySeS8CTXqnhrPHRHjJEKfZklw25e2Q7PlqvdDFeLIsCdXH7N0JcsxvVeoE0GeYpTflaM7aJiyYdNxUKPMt+3To/7pvmq7NSvlG7MJ+Lxe12tKUc9V/W1F6kMVf57DwcSQZLrCpK9fU8LZ1VNGoPzyyy+hUr8wzqtw2V+GYUqdYnh50WzkbqmbEk1Rs6unrED5888/ZX4t5PBiGIYpD9mGmzRpon0/ZswYcffdd0fc57bbbhMTJkwwtrtixQrRunVrUV5ImEBBahVkF65Xr14odxcMSFSiMgzDlCrFUHkhG3m1atVCX5tmJzfeeKMYPHiwsVkkyi1udnXVYxaf7eS45Uqg0BQpcBdG7RGGYZgyBZGVtIpkLPsIIYWJKlBM1K1bVy6JoKjZ1VPehpKqIDrejpD3KTcfNcrSz6qxMjdASv6S6HeaCt+EauylpYNpxLgaJb4rL9O1HTXde6RzUY2rlYhhfV9Buuu50ChqP4l+VtOvO6L1tU96qn5qsC0gpXtVaLsOZwrFmB7NgE+vowotH6xef5rtQE0HT1PA0/OmThomSDVeDVOZ313ZeuR59Uo5rttSIzzNArGvIPym7iVuGVXTdDfWoKUcl9yDJuzxKiR9KQ+sW7dOZg/Bv4FAIJRR/eCDDxZVqux3noBqDC/lKKuuZleHNggC5q677irz7OoJEyg4YWp0ZyM8wzBlThJ6eY0ePVq88MILoc+dO3eW/8IGjaS4YOXKlbIIoE0yZldPqMoL+kNbx4hgm6uuusrhNvzWW28lqgsMwzCRnk5FEBCJFSjPP/+8XIw9IH1OxuzqCRMoqFFCAxwZhmHKnCScoZQXEiZQpk+fLlIZ6NFtXbqqJ6e6bS/RAdfO3Bf6e8Pe6vq6rHApWVA9I9c1E3FOIbG/KPYDWt42Vwl6o334J9tZi8Ymn+yn2opA3ay9rvsGDIGYNLCN2iSqKPr3Xfm6Hp/adUz2ABqIuS/fPeMxDUCtkZXjGkSYXZhuvBYqVTPyXM+NnssukjFavf57SaCoapMAQcXKUruyfh/tynVXcajli2U7VlhDsIOU0qUZmtX7g2Y/rp6e6zpmNItxfXLfq9c8WkbmvYXhcci0bSg0E3O8SAN70YzyjBk2yjMMU7GwgvuXePdhyj6XF8MwDFMx4BkKwzAVC7ahJAwWKAzDVCzYhpIwWKC4UDMzR6Rl7jf6bs+t5GoAr0Ky8KoBdDRTrr9SwLUEbH6QlHWlWVgVHS7tQwHJnKsGElbLzHMNKqTQAL+qSqbXHBLcR8873Rs2lAaJgwFFDZIsJAbcPL/fdTxVZ4NIbBVVQ3/7PQHX/lHHgaokSI8GQaqOGAFybjRoTx8Hcg1Ju6oBn26rlp0G1dNzXMvkqgGU6YZrKM8lPdz/PVm64Z8a3tVSvnT8qvr1897mreKekZkEeKpj73ByoQGoynW09ysorlGeZygJgwUKwzAVC+nkFa/bcKI6U75ggcIwTMWCZygJg728GIZhmBKBZygMw1QsgrDpBIuwDxMNFigutK++QWRU2W+I/vqflq7Rz82rbNc+r8+u4ShZalPJEEFeUzG6gjxipP8nNxzhvGF3daMBV82Oe2j1rdq6HflhB4MdPj1KnRqns3x65LRKrSq6gTwnEI6U3pCt9y+TlMmtkRY+17rpejR+uq+m6xi1qPyvcYxMZYepUXm70m51pT+gYUY4AR/Y5A+fz6bsaq6GdUo0g7MaoZ9+wAHEJptE59NzV6mh3DvUGaHQ0u/Xupl7Xe+5bXl6SeAamTnu2Q58ea6OFjQ7M923fuae0N//5lU2ll+urNyDuwr2ZwTwkcwGccMqr4TBAoVhmIoFC5SEwQKFYZiKBcehJAwWKAzDVCgsKyiXePdhosMChWGYiqfyijd7MKevjwkWKC4cnLlFZGXuH55dNbIiGt1Bi6xt2ue9ShrvzBq68bBplm7Azw6Gt63p11N8FxBjqleJNt5VVU9Xbqf1tjm0StgQXyctbAAFf3rDNa7zA/rlr5+1W/vcPDNsCM7wFhj7tzU/bKzOI9Hv1Ui09pFV1oT+zg5mGNtVaZu1Xvu8J6g7FTRM3xVxvEAtn+5EsD6jVujvxun6dUnz6OMZULzracaA1pW3aJ93BcJ92uPPNKZ8V+8HWjaXjoPaxzRieFe3VftK7zFQ17/HtZ2vdh+ifVb7tFtJIy/Pxa8b9Jtk7RBuVCHX3+SoQA34rSqFf1//Fuw34OcFi2mUZxIGCxSGYSoWcrbBM5REwAKFYZiKBWJKyAw2KmxDiQkWKAzDVCx4hpIwWKC4UM+3S1T279dND671Vej7byu10LarQXTzadXCeulGabpeOdOj6343F9ZwbYdSyx9ef1S1tdq6QzI2a5/bpIdtHz/n19HW+ZSpfs003W5DdepHVvoz9HdHEgS3ICdsiwF5wbBtoWl1PQivhk8/zvlVw+MyN1u3SfxTGM4YDKr7wsftkvm3tm4PsQ80UvT6AaLRaJGmB+0FrI2hv/8N6uf2FykBvD6/dujvlpX+0db1qfqz9nlzIGxL2laoB0E28uv3Q2Vvnqs9qIFPt2dtDYT7X8+nB4NmKNdtY0Afvxpe/dy6ZITPbRc5b3p/blOuxZo8/XrXT9ODP5ulh8elZboeTEvtZOsKakW8vuBPcl8dW3lV6O+tB84tW7hnzI4FKxgUVpwzFPbyig0WKAzDVCx4hpIwODkkwzAMUyKUS4EyZcoU0bx5c5GZmSm6desmvv3227LuEsMwyQJiUIqyMBVPoMycOVOMHDlSjBkzRixdulR07NhR9OnTR2zdqut1GYapyLm8gnEuLFAqpECZNGmSuOKKK8Rll10m2rRpI6ZNmyYqVaoknnvuubLuGsMwSYAVtIq0MBVMoOTn54vvv/9e9OrVK/Sd1+uVnxctWhRxn7y8PLF7925tYRimHBP37OTAwlQsgfLPP/+IQCAg6tevr32Pz5s36661NuPHjxfVq1cPLU2aNCml3jIMUxbwDCVxlCuBUhRGjRoldu3aFVr+/luPdWAYhmEqYBxKnTp1hM/nE1u26An78LlBgwYR98nIyJCLjXXA+Ja9NzzFzfCG/87J0xMHpnv1IKvcwvD6bL++LkgCB3MCha7tUHILwtsGSDvZ+frnPelB13W5+eF28gr1QDba7j5lmr9baVO2m0PazVOqD/rIGPn0bXcr5Vf3ZZN2cvQ+ZSht7SVlWPeRz3v8QdfAxt1p+rYB5dz20HYL9c+5e8N9ygvo72B7STXK7EAg4vWV7ZJxEMp9lR0MGNvdp7RL1xUoQXrqdsCvHAPszleuKTnvbHIt1P7nKdeXrqP39j4y1vTccpTfiPp7Afnk+u9LCzjGNmdvQPutxkuhlRe3CqtQcELKmLDKGV27drWGDx8e+hwIBKyDDjrIGj9+fEz7//3333bmOF54DPgeSOJ7AL/VeMjJybEaNGhQ5ONhX7TBuFOuZigALsODBg0SRx55pOjatat49NFHxb59+6TXVyw0atRI/Prrr9JDDOqvatX09BnMfuC8AHsTj5E7PEaJGSPMTPbs2SN/q/GAuLQ1a9ZI552ikJ6eLttg3Cl3AuW8884T27ZtE6NHj5aG+E6dOok5c+Y4DPVuwCvsoIMOkn/jBmeBYobHKDo8RiU/RnCgKQoQCCwUEke5Eyhg+PDhcmEYhmFKjwrv5cUwDMOUDCxQIgCvL6RuUb2/GB6jeOH7iMeoouGBZb6sO8EwDMOkPjxDYRiGYUoEFigMwzBMicAChWEYhikRWKBEgAt0hRNnHnXUUaJq1aqiXr16on///mLlypXaWOXm5oprrrlG1K5dW1SpUkWcffbZjtQ3FYkHHnhAeDweMWLEiNB3PEZCbNiwQVx88cXyPsnKyhLt27cXS5YsCY0RTLmIHWvYsKFcjwzhq1aF68kzqQELFAIX6Arz+eefS2HxzTffiHnz5omCggLRu3dvmXnA5oYbbhDvv/++eOONN+T2GzduFAMGDBAVke+++0489dRTokOHDtr3FX2MduzYIbp37y7S0tLERx99JDNRPPzww6JmzZqhbSZOnCgef/xxWb9o8eLFonLlyrIwHoQxk0IY0rJUSJAL7JprrtFygTVq1CjmXGDlma1bt8qcRp9//rn8vHPnTistLc164403QtusWLFCbrNo0SKrIrFnzx7rkEMOsebNm2edeOKJ1vXXXy+/5zGyrFtvvdU67rjjXMcuGAzKPFkPPvhg6DuMW0ZGhvXqq6+WyvVjSgaeoRSzQFdFAun9Qa1ateS/GCvMWtTxat26tWjatGmFGy/M5E4//XRtLACPkRDvvfeezK03cOBAqTrt3Lmz+N///hcaI+TXQpokdeyQWqVbt24V7j5KdVigFLNAV0UhGAxKuwBUF+3atZPfYUyQMK9GjRoVerxee+01sXTpUmlzovAYCfHnn3+KJ598UhxyyCHi448/FsOGDRPXXXedeOGFF0JjBPh3l/qUy1xeTGLewH/55Rfx5Zdf8vAqIEvu9ddfL21MnHTQ/WUEM5Rx48bJz5ih4F6CvQSZwZnyA89QilmgqyKARJuzZ88WCxYsEI0bNw59jzGBmnDnzp0Vdryg0tq6das44ogjhN/vlwsM7zAw42+8dVf0MYLnFspBqBx++OFi3bp18m97HPh3l/qwQFGA+qZLly5i/vz52tsVPh9zzDGiogFXTgiTt99+W3z66aeiRYsW2nqMFTx31PGCWzEeFBVlvHr27Cl+/vlnsWzZstCCt/GLLroo9HdFHyOoSam7+e+//y6aNWsm/8Z9BaGijhHqpMDbq6KMUbmhhIz75YbXXntNepc8//zz1q+//mpdeeWVVo0aNazNmzdbFY1hw4ZZ1atXtz777DNr06ZNoSU7Ozu0zVVXXWU1bdrU+vTTT60lS5ZYxxxzjFwqMqqXF6joY/Ttt99afr/fuv/++61Vq1ZZL7/8slWpUiXrpZdeCm3zwAMPyN/Zu+++a/30009Wv379rBYtWnCFxBSDBUoEnnjiCfkASE9Pl27E33zzjVURcSuFOn369NA2KIl69dVXWzVr1pQPibPOOksKnYoMFSg8Rpb1/vvvW+3atZMva61bt7aefvpph+vwXXfdZdWvX19u07NnT2vlypVlcPWY4sDZhhmGYZgSgW0oDMMwTInAAoVhGIYpEVigMAzDMCUCCxSGYRimRGCBwjAMw5QILFAYhmGYEoEFCsMwDFMisEBhGIZhSgQWKKXA4MGDZfncsuKSSy4JZXotS1Aa95133hHJzGeffSb7SZM5ljdOOukkrUxxcbn77rtFp06dYtoWWYbPPPPMEjs2kzywQCkmePiYFvzQHnvsMfH888+LsuDHH38UH374oaw/wSSfYITgQmkAZOTNyMgQhx56qLxesdarLypvvfWWuPfee0VZMGTIEFk/5osvviiT4zOJg+uhFJNNmzZp9ehHjx6tZVatUqWKXMqKJ554QlbKK8s+MJFBWvtTTjlFVjF88803xUEHHSTWrl3rKFhmqldfVOyqm2WV1fvCCy+UKf6PP/74MusHU/LwDKWYIO22vaBsKd4g1e/wIKcqL6gbrr32WvmmWbNmTVkzAyVR9+3bJy677DJRtWpVcfDBB4uPPvpIOxaKEvXt21e2iX2gykKVSTdQfRIPKqpeyMvLE7feeqto0qSJfCvGsZ599tnQetTz6Nq1q1yHN+fbbrtNFBYWav3HjOeWW26RDyacJ2ZiKqtWrRInnHCCLDqFWhgoQBWpONW5554rH6Bop1+/fuKvv/4KrbfH7aGHHpL9qF27tnybR9lhm6lTp8pKgDgOxuScc87RSg+giiLSo2dlZYmOHTvK8VDBbACzAqzv0aOHdvxING/eXP571llnyWttfwaoStiqVSv5wDzssMPEiy++aGzrueeeE9u3b5ezHaR4R1snnnii7KfK3r17ZTp83CO4X6Jhj9vYsWNF3bp1RbVq1cRVV10lBVgklddvv/0mKlWqJF555ZXQ+tdff12Oya+//hqaSV1++eWh9k4++WQ5+zWpDnEPVa5cWV5fnB+EpQ3uSZQGzsnJiXo+TApRrNSSjAay8CLdO2XQoEEyHbeajbZq1arWvffea/3+++/yX5/PZ/Xt21dmYcV3SB1fu3Zta9++fXKfHTt2WHXr1rVGjRplrVixwlq6dKl1yimnWD169HC9CtgGl5im3j/33HOtJk2aWG+99Zb1xx9/WJ988olM2w/Wr18vswYjgzCO8/bbb1t16tSxxowZo/W/WrVq1t133y37+sILL1gej8eaO3euXB8IBGRmWWSMXbZsmfX5559bnTt3ln1BeyA/P986/PDDrSFDhsh05SgVcOGFF1qHHXaYlZeXFxo3HAfp39EXZKxF3+xMtd99950ct1deecX666+/5Pk+9thjoX7ed999MrPtnDlz5Hni+iCTLdLxg3Xr1snPI0eOtH777TeZTh3ZbtFPjHcktm7dGsq4jKzK+AwwlmlpadaUKVNkltyHH35Y9g0p693A9b7ooousK664wqpXr57Vtm1bmeK9sLBQ2+7SSy+1RowYETGTcSQwblWqVLHOO+8865dffrFmz54t753bb79du4ZqO+g37t21a9daf//9t8werY5lr169rDPPPFOOOa75jTfeKO/Pf//9V67H/dGxY0f5d0FBgWzrpptuslavXi2vLcpBoG0b3Nder9dasGCB8VyY1IIFShkJlOOOOy70GQ+QypUrW5dccknoOzys8OBatGiR/Ayh07t3b61d/PCxjVuabzy88VBDanAbbIt95s2bF3EfPHTwUFf3wcMGDygIikj9B0cddZR16623yr8//vhjWf9iw4YNofUfffSRJlBefPFFx3EgSLKysuT+9rg1a9ZMe8AOHDhQPijBrFmzpMDZvXu34zxyc3Ol8Pn666+174cOHWpdcMEF8m8I5zZt2mjrcQ4mgQLU87A59thjpWBQQV9PO+0013Zw/hBoEKqokwKhXqtWLSmobV599VUpnJECPx6BgnbslxHw5JNPOq4hbef000+3jj/+ePkigHvNvjZffPGFHGeMqUqrVq2sp556yiFQIGQwRrbgdgNCC4KGKT+wDaWMUHXhKDsMdU779u1D30F9A1BeFkC9gBK8kWwhf/zxh1TbUKBOgNoKqhkbVBHE8aBaicSKFStklTx1H6groHZZv369aNq0qaP/ACopu69oA+q0Ro0ahdbTyns4n9WrV0v1nkpubq48H5u2bdvK/qrHQYVEAPsDqv61bNlSnHrqqXKBKgrqG7SdnZ0tt1GB2gc1ze1+duvWTVtf1AqBaOvKK6/UvsO4wSHDDajkYD95+umn5TmiAuaGDRvEgw8+KMaMGVOsevVQm2Ec1PPCNUSbdqXESCo43Eder1csX748dA/gWmFf3KP0/lKvlQ3Ul1C79enTR45/r169pGoT104FKjVcI6b8wAKljEBZWBX8eNXv7B8zHjoAP2jonSdMmOBoi/5QberUqSN/sHiIQq9v/4gT1X+7r7GA88ED9OWXX3asg54+luNAGMFbCPr6uXPnSocI2HJgwEb74IMPPpDGbhUI2WQA1w3npwpM1FrfvHmzvGZqvXrVLrZw4UIxefJkaQtT9y0uEByw40GgwNnEvq8wlvgb40yJ5EAApk+fLu1sc+bMkc4qd955pxSMRx99dGgb2I/Ua82kPixQUgQ8VGbNmiUNt35/bJfNjguAYdX+G7MgPJBheMebIwUPNBwHmh1bqH311Vfy4d24ceOYjos28CasPpS++eYbx/ngQYM3dBh5iwrGAueBBW/1eMB9+umn8s0YggO1291mY+gnDMMqtJ+RgBDAg522hXEaNGhQ6Dt8hkOCG5jBwBCO64GHuF1rHWOGFwC7Xr0KnDZat24tnSpMwgTCATMI+wUC54XZLWaOkcDDHbOKO+64Q143OAFAWGN/XCsIOYy16oQQDcwEsYwaNUrOkHCutkDBzAazUXu2yJQP2MsrRYB3E370F1xwgXwDxw/y448/lg8Y+nCzwdsfHgZffvll6Ds8EPDQQywAvIvWrFkj3zzh1QOuvvpqKQzghQbvn3fffVc+qEeOHBl66EUDD3eoTnAcPNgQb4AHlQoeWJhBwbML6+1+4K0WqrVYmD17tnQ9hRoPHkQzZsyQD2d4WEEA3nTTTeKGG24QL7zwghwvPCDhRo3PAJ5P8Ea7+eabpas3HnixxAthDOfPny8fsjt27JDfoQ3sC08vtDlp0iQZ64E+uDFs2DB5TaHWgiDBbAoBqLjWAOfQrl07bYHXFFRP+NsEZjhDhw6VLxPwZMM1HD58uOs1xFhA2GAmgb7jnrL7jusJgQDPMcwE4Qn39ddfy2u6ZMkSR1u4lhAiixYtktcF+2BMIHRtcM2hqoRXHFOOKGsjTkU1ylODKIzPjzzyiNH4C+8a1GyvUaOGNF7DgwneP6phmzJ16lTr6KOP1r6DgfeGG26wGjZsaKWnp1sHH3yw9dxzz4XWw5gKIzvWNWjQQBqq4blj6j/OD+epGv9huEcbhx56qPS0oucDxwN4MMGLDMbpli1bSsP2rl27Io4bwHFxfNtYjL9h3MV4dOjQwZo5c2ZoW4zLo48+Ko3f8MCCp1OfPn2k15kNPMdw/jg+DNIYh2hG+ffee0/uA8cDXDd1rHEOOBbOecaMGVY04DTQrVu30PlH8vJSidUoj3EbPXq09MSCMR7jqhrV1XbgpQenENxfNosXL5bn8eGHH8rPcHy49tprrUaNGsnv4SUIDzV4ylGjPLwK+/fvH7q/MEboi+0QAGD0Hz9+fNTxYVILrilfzoHaA2/sUC8V1eDMpBZQXSFuJFnT3MDgjzgWzMoQu8WUH1jlVc6BDhyqIFMAJMOUJrDR4J5kYVL+YKN8BQBR0QyTLERyBmHKB6zyYhiGYUoEVnkxDMMwJQILFIZhGKZEYIHCMAzDlAgsUBiGYRgWKAzDMEzywDMUhmEYpkRggcIwDMOUCCxQGIZhmBKBBQrDMAwjSoL/A83QhjRd+7hWAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 400x400 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# first sample of first image for scalograms\n",
    "example_img = X_cnn[0, 0, :, :]\n",
    "\n",
    "plt.figure(figsize=(4, 4))\n",
    "plt.imshow(example_img, aspect=\"auto\", origin=\"lower\")\n",
    "plt.colorbar(label=\"Power\")\n",
    "plt.title(\"Example scalogram (sample 0, channel 0)\")\n",
    "plt.xlabel(\"Time (condensed to 64 pixels)\")\n",
    "plt.ylabel(\"Frequency (condensed to 64 pixels)\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9d570d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Never relly use this model again kinda bad\n",
    "class SmallEEGCNN(nn.Module):\n",
    "    def __init__(self, in_channels, img_size=64):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels, 16, kernel_size=3, padding=1)\n",
    "        self.pool1 = nn.MaxPool2d(2)  # 64 -> 32\n",
    "        self.conv2 = nn.Conv2d(16, 32, kernel_size=3, padding=1)\n",
    "        self.pool2 = nn.MaxPool2d(2)  # 32 -> 16\n",
    "\n",
    "        # feature map size after two poolings\n",
    "        self.feature_map_size = img_size // 4  # 64 -> 16\n",
    "\n",
    "        self.fc1 = nn.Linear(32 * self.feature_map_size * self.feature_map_size, 64)\n",
    "        self.fc_out = nn.Linear(64, 1)  # for binary classification\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x -> (batch, in_channels, H, W)\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = self.pool1(x)\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = self.pool2(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc_out(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75915af8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed=42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "set_seed(42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bc5234e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_cnn: (200, 8, 64, 64) y_cnn: (200,) groups: (200,)\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "X_cnn = np.array(X_cnn)\n",
    "y_cnn = np.array(y_cnn).astype(np.float32)\n",
    "groups = np.array(participant_ids_cnn)\n",
    "\n",
    "N, in_channels, H, W = X_cnn.shape\n",
    "print(\"X_cnn:\", X_cnn.shape, \"y_cnn:\", y_cnn.shape, \"groups:\", groups.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06013647",
   "metadata": {},
   "outputs": [],
   "source": [
    "logo = LeaveOneGroupOut()\n",
    "\n",
    "num_epochs = 15\n",
    "batch_size = 16\n",
    "lr = 1e-3\n",
    "\n",
    "all_acc = []\n",
    "all_prec = []\n",
    "all_rec = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a19d44b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Fold 1: leaving out participant 0\n",
      "\n",
      "Fold 2: leaving out participant 1\n",
      "\n",
      "Fold 3: leaving out participant 2\n",
      "\n",
      "Fold 4: leaving out participant 3\n",
      "\n",
      "Fold 5: leaving out participant 4\n",
      "\n",
      "Fold 6: leaving out participant 5\n",
      "\n",
      "Fold 7: leaving out participant 6\n",
      "\n",
      "Fold 8: leaving out participant 7\n",
      "\n",
      "Fold 9: leaving out participant 8\n",
      "\n",
      "Fold 10: leaving out participant 9\n",
      "\n",
      "Fold 11: leaving out participant 10\n",
      "\n",
      "Fold 12: leaving out participant 11\n",
      "\n",
      "Fold 13: leaving out participant 12\n",
      "\n",
      "Fold 14: leaving out participant 13\n",
      "\n",
      "Fold 15: leaving out participant 14\n",
      "\n",
      "Fold 16: leaving out participant 15\n",
      "\n",
      "Fold 17: leaving out participant 16\n",
      "\n",
      "Fold 18: leaving out participant 17\n",
      "\n",
      "Fold 19: leaving out participant 18\n",
      "\n",
      "Fold 20: leaving out participant 19\n"
     ]
    }
   ],
   "source": [
    "all_acc = []\n",
    "fold_precisions = [] # fold precisions\n",
    "fold_recalls = [] # fold recalls\n",
    "# used to compute not per fold metrics\n",
    "all_folds_y_true = []\n",
    "all_folds_y_pred = []\n",
    "\n",
    "for fold_idx, (train_idx, test_idx) in enumerate(logo.split(X_cnn, y_cnn, groups)):\n",
    "    print(f\"\\nFold {fold_idx+1}: leaving out participant {groups[test_idx][0]}\")\n",
    "\n",
    "    X_train = X_cnn[train_idx] # (n_train, k, H, W)\n",
    "    y_train = y_cnn[train_idx] # (n_train,)\n",
    "    X_test = X_cnn[test_idx] # (n_test, k, H, W)\n",
    "    y_test = y_cnn[test_idx] # (n_test,)\n",
    "\n",
    "    # convert to tensors cause u need to use tensors\n",
    "    X_train_t = torch.from_numpy(X_train).float()\n",
    "    y_train_t = torch.from_numpy(y_train).unsqueeze(1)\n",
    "    X_test_t = torch.from_numpy(X_test).float()\n",
    "    y_test_t = torch.from_numpy(y_test).unsqueeze(1)\n",
    "\n",
    "    train_ds = TensorDataset(X_train_t, y_train_t)\n",
    "    train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    # model each fold\n",
    "    model = SmallEEGCNN(in_channels=in_channels, img_size=H).to(device)\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    # actual training loop\n",
    "    model.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        epoch_loss = 0.0\n",
    "        for xb, yb in train_loader:\n",
    "            xb = xb.to(device)\n",
    "            yb = yb.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            logits = model(xb)\n",
    "            loss = criterion(logits, yb)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            epoch_loss += loss.item() * xb.size(0)\n",
    "\n",
    "        epoch_loss /= len(train_ds)\n",
    "\n",
    "    # Actutually evaluating the model\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        logits_test = model(X_test_t.to(device))\n",
    "        probs_test = torch.sigmoid(logits_test).cpu().numpy().ravel()\n",
    "        y_pred = (probs_test >= 0.5).astype(int)\n",
    "\n",
    "    y_true = y_test.astype(int)\n",
    "\n",
    "    # store accuracy for each fold\n",
    "    acc = accuracy_score(y_true, y_pred)\n",
    "    all_acc.append(acc)\n",
    "\n",
    "    # accumulate all folds predictions for \"global\" metrics\n",
    "    all_folds_y_true.extend(y_true)\n",
    "    all_folds_y_pred.extend(y_pred)\n",
    "\n",
    "    # computing precision recall for each fold (class 0 and class 1)\n",
    "    prec, rec, f1, support = precision_recall_fscore_support(\n",
    "        y_true,\n",
    "        y_pred,\n",
    "        labels=[0, 1],\n",
    "        average=None,\n",
    "        zero_division=0\n",
    "    )\n",
    "    fold_precisions.append(prec)\n",
    "    fold_recalls.append(rec)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74a2739c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Fold 1: leaving out participant 0\n",
      "\n",
      "Fold 2: leaving out participant 1\n",
      "\n",
      "Fold 3: leaving out participant 2\n",
      "\n",
      "Fold 4: leaving out participant 3\n",
      "\n",
      "Fold 5: leaving out participant 4\n",
      "\n",
      "Fold 6: leaving out participant 5\n",
      "\n",
      "Fold 7: leaving out participant 6\n",
      "\n",
      "Fold 8: leaving out participant 7\n",
      "\n",
      "Fold 9: leaving out participant 8\n",
      "\n",
      "Fold 10: leaving out participant 9\n",
      "\n",
      "Fold 11: leaving out participant 10\n",
      "\n",
      "Fold 12: leaving out participant 11\n",
      "\n",
      "Fold 13: leaving out participant 12\n",
      "\n",
      "Fold 14: leaving out participant 13\n",
      "\n",
      "Fold 15: leaving out participant 14\n",
      "\n",
      "Fold 16: leaving out participant 15\n",
      "\n",
      "Fold 17: leaving out participant 16\n",
      "\n",
      "Fold 18: leaving out participant 17\n",
      "\n",
      "Fold 19: leaving out participant 18\n",
      "\n",
      "Fold 20: leaving out participant 19\n"
     ]
    }
   ],
   "source": [
    "all_acc = []\n",
    "fold_precisions = [] # fold precisions\n",
    "fold_recalls = [] # fold recalls\n",
    "# used to compute not per fold metrics\n",
    "all_folds_y_true = []\n",
    "all_folds_y_pred = []\n",
    "\n",
    "for fold_idx, (train_idx, test_idx) in enumerate(logo.split(X_cnn, y_cnn, groups)):\n",
    "    print(f\"\\nFold {fold_idx+1}: leaving out participant {groups[test_idx][0]}\")\n",
    "\n",
    "    X_train = X_cnn[train_idx] # (n_train, k, H, W)\n",
    "    y_train = y_cnn[train_idx] # (n_train,)\n",
    "    X_test = X_cnn[test_idx] # (n_test, k, H, W)\n",
    "    y_test = y_cnn[test_idx] # (n_test,)\n",
    "\n",
    "    # convert to tensors cause u need to use tensors\n",
    "    X_train_t = torch.from_numpy(X_train).float()\n",
    "    y_train_t = torch.from_numpy(y_train).unsqueeze(1)\n",
    "    X_test_t = torch.from_numpy(X_test).float()\n",
    "    y_test_t = torch.from_numpy(y_test).unsqueeze(1)\n",
    "\n",
    "    train_ds = TensorDataset(X_train_t, y_train_t)\n",
    "    train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    # model each fold\n",
    "    model = SmallEEGCNN(in_channels=in_channels, img_size=H).to(device)\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    # actual training loop\n",
    "    model.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        epoch_loss = 0.0\n",
    "        for xb, yb in train_loader:\n",
    "            xb = xb.to(device)\n",
    "            yb = yb.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            logits = model(xb)\n",
    "            loss = criterion(logits, yb)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            epoch_loss += loss.item() * xb.size(0)\n",
    "\n",
    "        epoch_loss /= len(train_ds)\n",
    "\n",
    "    # Actutually evaluating the model\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        logits_test = model(X_test_t.to(device))\n",
    "        probs_test = torch.sigmoid(logits_test).cpu().numpy().ravel()\n",
    "        y_pred = (probs_test >= 0.5).astype(int)\n",
    "\n",
    "    y_true = y_test.astype(int)\n",
    "\n",
    "    # store accuracy for each fold\n",
    "    acc = accuracy_score(y_true, y_pred)\n",
    "    all_acc.append(acc)\n",
    "\n",
    "    # accumulate all folds predictions for \"global\" metrics\n",
    "    all_folds_y_true.extend(y_true)\n",
    "    all_folds_y_pred.extend(y_pred)\n",
    "\n",
    "    # computing precision recall for each fold (class 0 and class 1)\n",
    "    prec, rec, f1, support = precision_recall_fscore_support(\n",
    "        y_true,\n",
    "        y_pred,\n",
    "        labels=[0, 1],\n",
    "        average=None,\n",
    "        zero_division=0\n",
    "    )\n",
    "    fold_precisions.append(prec)\n",
    "    fold_recalls.append(rec)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c27fb215",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metrics from aggregation across folds\n",
      "Accuracy across all folds: 0.575\n",
      "Mean precision for class 0 across folds: 0.3616666666666667\n",
      "Mean precision for class 1 across folds: 0.5498412698412698\n",
      "Mean recall for class 0 across folds: 0.2373809523809524\n",
      "Mean recall for class 1 across folds: 0.7704960317460319\n",
      "------------------------------------\n",
      "Metrics computed on all folds combined (global)\n",
      "Global precision for class 0: 0.5681818181818182\n",
      "Global precision for class 1: 0.5769230769230769\n",
      "Global recall for class 0: 0.27472527472527475\n",
      "Global recall for class 1: 0.8256880733944955\n"
     ]
    }
   ],
   "source": [
    "fold_precisions = np.array(fold_precisions)\n",
    "fold_recalls = np.array(fold_recalls)\n",
    "\n",
    "mean_prec_class0, mean_prec_class1 = fold_precisions.mean(axis=0)\n",
    "mean_recall_class0, mean_recall_class1 = fold_recalls.mean(axis=0)\n",
    "\n",
    "all_folds_y_true = np.array(all_folds_y_true)\n",
    "all_folds_y_pred = np.array(all_folds_y_pred)\n",
    "\n",
    "# \"global\" metrics over all predictions\n",
    "prec_global, rec_global, f1_global, support_global = precision_recall_fscore_support(\n",
    "    all_folds_y_true,\n",
    "    all_folds_y_pred,\n",
    "    labels=[0, 1],\n",
    "    average=None,\n",
    "    zero_division=0\n",
    ")\n",
    "\n",
    "print(\"Metrics from aggregation across folds\")\n",
    "print(\"Accuracy across all folds:\", np.mean(all_acc))\n",
    "\n",
    "print(\"Mean precision for class 0 across folds:\", mean_prec_class0)\n",
    "print(\"Mean precision for class 1 across folds:\", mean_prec_class1)\n",
    "\n",
    "print(\"Mean recall for class 0 across folds:\", mean_recall_class0)\n",
    "print(\"Mean recall for class 1 across folds:\", mean_recall_class1)\n",
    "\n",
    "# Global metrics just be the ypred and ground truth for all preds combined together. It makes a bit more sense to use here bc test size just so low from LOGO.\n",
    "print(\"------------------------------------\")\n",
    "print(\"Metrics computed on all folds combined (global)\")\n",
    "\n",
    "print(\"Global precision for class 0:\", prec_global[0])\n",
    "print(\"Global precision for class 1:\", prec_global[1])\n",
    "\n",
    "print(\"Global recall for class 0:\", rec_global[0])\n",
    "print(\"Global recall for class 1:\", rec_global[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8b17565",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScalogramCNN(nn.Module):\n",
    "    def __init__(self, in_channels=1):\n",
    "        super().__init__()\n",
    "        # 64x64 -> 32x32\n",
    "        self.conv1 = nn.Conv2d(in_channels, 16, kernel_size=3, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(16)\n",
    "        self.pool1 = nn.MaxPool2d(2)\n",
    "\n",
    "        # 32x32 -> 16x16\n",
    "        self.conv2 = nn.Conv2d(16, 32, kernel_size=3, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(32)\n",
    "        self.pool2 = nn.MaxPool2d(2)\n",
    "\n",
    "        # 16x16 -> 8x8\n",
    "        self.conv3 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
    "        self.bn3 = nn.BatchNorm2d(64)\n",
    "        self.pool3 = nn.MaxPool2d(2)\n",
    "\n",
    "        # Global average pool where it go from 8x8 -> 1x1 (works for any input size)\n",
    "        self.global_pool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "\n",
    "        # Small classifier head\n",
    "        self.dropout = nn.Dropout(p=0.5)\n",
    "        self.fc_out = nn.Linear(64, 1)   # turned into binary logits\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x -> (batch, in_channels, H, W)\n",
    "        x = self.pool1(F.relu(self.bn1(self.conv1(x))))\n",
    "        x = self.pool2(F.relu(self.bn2(self.conv2(x))))\n",
    "        x = self.pool3(F.relu(self.bn3(self.conv3(x))))\n",
    "\n",
    "        x = self.global_pool(x) # (batch, 64, 1, 1)\n",
    "        x = x.view(x.size(0), -1) # (batch, 64)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc_out(x) # (batch, 1) logits\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54628107",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Fold 1: leaving out participant 0\n",
      "  acc=0.600, prec0=0.500, prec1=0.625, rec0=0.250, rec1=0.833\n",
      "\n",
      "Fold 2: leaving out participant 1\n",
      "  acc=0.600, prec0=0.600, prec1=0.000, rec0=1.000, rec1=0.000\n",
      "\n",
      "Fold 3: leaving out participant 2\n",
      "  acc=1.000, prec0=0.000, prec1=1.000, rec0=0.000, rec1=1.000\n",
      "\n",
      "Fold 4: leaving out participant 3\n",
      "  acc=0.600, prec0=0.000, prec1=0.750, rec0=0.000, rec1=0.750\n",
      "\n",
      "Fold 5: leaving out participant 4\n",
      "  acc=0.700, prec0=0.333, prec1=0.857, rec0=0.500, rec1=0.750\n",
      "\n",
      "Fold 6: leaving out participant 5\n",
      "  acc=1.000, prec0=1.000, prec1=0.000, rec0=1.000, rec1=0.000\n",
      "\n",
      "Fold 7: leaving out participant 6\n",
      "  acc=0.200, prec0=0.000, prec1=0.200, rec0=0.000, rec1=1.000\n",
      "\n",
      "Fold 8: leaving out participant 7\n",
      "  acc=0.600, prec0=0.000, prec1=0.600, rec0=0.000, rec1=1.000\n",
      "\n",
      "Fold 9: leaving out participant 8\n",
      "  acc=0.400, prec0=0.000, prec1=0.400, rec0=0.000, rec1=1.000\n",
      "\n",
      "Fold 10: leaving out participant 9\n",
      "  acc=0.700, prec0=1.000, prec1=0.500, rec0=0.571, rec1=1.000\n",
      "\n",
      "Fold 11: leaving out participant 10\n",
      "  acc=0.300, prec0=0.333, prec1=0.000, rec0=0.750, rec1=0.000\n",
      "\n",
      "Fold 12: leaving out participant 11\n",
      "  acc=0.800, prec0=0.000, prec1=0.889, rec0=0.000, rec1=0.889\n",
      "\n",
      "Fold 13: leaving out participant 12\n",
      "  acc=0.300, prec0=0.300, prec1=0.000, rec0=1.000, rec1=0.000\n",
      "\n",
      "Fold 14: leaving out participant 13\n",
      "  acc=0.600, prec0=0.714, prec1=0.333, rec0=0.714, rec1=0.333\n",
      "\n",
      "Fold 15: leaving out participant 14\n",
      "  acc=0.800, prec0=0.800, prec1=0.000, rec0=1.000, rec1=0.000\n",
      "\n",
      "Fold 16: leaving out participant 15\n",
      "  acc=0.600, prec0=0.625, prec1=0.500, rec0=0.833, rec1=0.250\n",
      "\n",
      "Fold 17: leaving out participant 16\n",
      "  acc=0.400, prec0=0.000, prec1=0.400, rec0=0.000, rec1=1.000\n",
      "\n",
      "Fold 18: leaving out participant 17\n",
      "  acc=0.900, prec0=0.000, prec1=0.900, rec0=0.000, rec1=1.000\n",
      "\n",
      "Fold 19: leaving out participant 18\n",
      "  acc=0.700, prec0=0.000, prec1=0.700, rec0=0.000, rec1=1.000\n",
      "\n",
      "Fold 20: leaving out participant 19\n",
      "  acc=0.600, prec0=0.000, prec1=0.667, rec0=0.000, rec1=0.857\n"
     ]
    }
   ],
   "source": [
    "all_acc = []\n",
    "fold_precisions = [] # fold precisions\n",
    "fold_recalls = [] # fold recalls\n",
    "# used to compute not per fold metrics\n",
    "all_folds_y_true = []\n",
    "all_folds_y_pred = []\n",
    "\n",
    "for fold_idx, (train_idx, test_idx) in enumerate(logo.split(X_cnn, y_cnn, groups)):\n",
    "    print(f\"\\nFold {fold_idx+1}: leaving out participant {groups[test_idx][0]}\")\n",
    "\n",
    "    X_train = X_cnn[train_idx] # (n_train, k, H, W)\n",
    "    y_train = y_cnn[train_idx] # (n_train,)\n",
    "    X_test = X_cnn[test_idx] # (n_test, k, H, W)\n",
    "    y_test = y_cnn[test_idx] # (n_test,)\n",
    "\n",
    "    # convert to tensors cause u need to use tensors\n",
    "    X_train_t = torch.from_numpy(X_train).float()\n",
    "    y_train_t = torch.from_numpy(y_train).unsqueeze(1)\n",
    "    X_test_t = torch.from_numpy(X_test).float()\n",
    "    y_test_t = torch.from_numpy(y_test).unsqueeze(1)\n",
    "\n",
    "    train_ds = TensorDataset(X_train_t, y_train_t)\n",
    "    train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    # model each fold\n",
    "    model = ScalogramCNN(in_channels=in_channels).to(device)\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=1e-4)\n",
    "\n",
    "    # actual training loop\n",
    "    model.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        epoch_loss = 0.0\n",
    "        for xb, yb in train_loader:\n",
    "            xb = xb.to(device)\n",
    "            yb = yb.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            logits = model(xb)\n",
    "            loss = criterion(logits, yb)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            epoch_loss += loss.item() * xb.size(0)\n",
    "\n",
    "        epoch_loss /= len(train_ds)\n",
    "\n",
    "    # Actutually evaluating the model\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        logits_test = model(X_test_t.to(device))\n",
    "        probs_test = torch.sigmoid(logits_test).cpu().numpy().ravel()\n",
    "        y_pred = (probs_test >= 0.5).astype(int)\n",
    "\n",
    "    y_true = y_test.astype(int)\n",
    "\n",
    "    # store accuracy for each fold\n",
    "    acc = accuracy_score(y_true, y_pred)\n",
    "    all_acc.append(acc)\n",
    "\n",
    "    # accumulate all folds predictions for \"global\" metrics\n",
    "    all_folds_y_true.extend(y_true)\n",
    "    all_folds_y_pred.extend(y_pred)\n",
    "\n",
    "    # computing precision recall for each fold (class 0 and class 1)\n",
    "    prec, rec, f1, support = precision_recall_fscore_support(\n",
    "        y_true,\n",
    "        y_pred,\n",
    "        labels=[0, 1],\n",
    "        average=None,\n",
    "        zero_division=0\n",
    "    )\n",
    "    fold_precisions.append(prec)\n",
    "    fold_recalls.append(rec)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d2943c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metrics from aggregation across folds\n",
      "Accuracy across all folds: 0.62\n",
      "Mean precision for class 0 across folds: 0.3102976190476191\n",
      "Mean precision for class 1 across folds: 0.4660515873015873\n",
      "Mean recall for class 0 across folds: 0.38095238095238093\n",
      "Mean recall for class 1 across folds: 0.6331349206349207\n",
      "------------------------------------\n",
      "Metrics computed on all folds combined (global)\n",
      "Global precision for class 0: 0.5974025974025974\n",
      "Global precision for class 1: 0.6341463414634146\n",
      "Global recall for class 0: 0.5054945054945055\n",
      "Global recall for class 1: 0.7155963302752294\n"
     ]
    }
   ],
   "source": [
    "fold_precisions = np.array(fold_precisions)\n",
    "fold_recalls = np.array(fold_recalls)\n",
    "\n",
    "mean_prec_class0, mean_prec_class1 = fold_precisions.mean(axis=0)\n",
    "mean_recall_class0, mean_recall_class1 = fold_recalls.mean(axis=0)\n",
    "\n",
    "all_folds_y_true = np.array(all_folds_y_true)\n",
    "all_folds_y_pred = np.array(all_folds_y_pred)\n",
    "\n",
    "# \"global\" metrics over all predictions\n",
    "prec_global, rec_global, f1_global, support_global = precision_recall_fscore_support(\n",
    "    all_folds_y_true,\n",
    "    all_folds_y_pred,\n",
    "    labels=[0, 1],\n",
    "    average=None,\n",
    "    zero_division=0\n",
    ")\n",
    "\n",
    "print(\"Metrics from aggregation across folds\")\n",
    "print(\"Accuracy across all folds:\", np.mean(all_acc))\n",
    "\n",
    "print(\"Mean precision for class 0 across folds:\", mean_prec_class0)\n",
    "print(\"Mean precision for class 1 across folds:\", mean_prec_class1)\n",
    "\n",
    "print(\"Mean recall for class 0 across folds:\", mean_recall_class0)\n",
    "print(\"Mean recall for class 1 across folds:\", mean_recall_class1)\n",
    "\n",
    "# Global metrics just be the ypred and ground truth for all preds combined together. It makes a bit more sense to use here bc test size just so low from LOGO.\n",
    "print(\"------------------------------------\")\n",
    "print(\"Metrics computed on all folds combined (global)\")\n",
    "\n",
    "print(\"Global precision for class 0:\", prec_global[0])\n",
    "print(\"Global precision for class 1:\", prec_global[1])\n",
    "\n",
    "print(\"Global recall for class 0:\", rec_global[0])\n",
    "print(\"Global recall for class 1:\", rec_global[1])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
